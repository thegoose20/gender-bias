{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671a945b-1821-4550-bdc2-1969ba00cc22",
   "metadata": {},
   "source": [
    "# Experiment 1, Model 1: Multilabel Classification of Linguistic Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315edf94-4468-4839-a216-a84a4d79c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For multilabel token classification\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For saving model\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8328cce-a6b2-45d7-9901-c811911c4781",
   "metadata": {},
   "source": [
    "### 1. Create Word Embeddings\n",
    "\n",
    "Train custom word embeddings on metadata descriptions from the University of Edinburgh Heritage Collections' Archives catalog.\n",
    "\n",
    "* Data file: `descriptions_by_fonds`\n",
    "* Date of harvesting: October 2020\n",
    "* Harvesting and transformation code: [annot-prep/PreparationForAnnotation.ipynb](https://github.com/thegoose20/annot-prep/blob/main/PreparationForAnnotation.ipynb)\n",
    "\n",
    "References:\n",
    "* https://radimrehurek.com/gensim/models/fasttext.html\n",
    "* https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dfe511c-129e-4f16-9a93-0aabc0c80bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1079\n"
     ]
    }
   ],
   "source": [
    "dir_path = config.inf_data_path+\"descriptions_by_fonds/\"\n",
    "file_list = os.listdir(dir_path)\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a4f4baa-4316-4186-a66a-a407ff476f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusIterator:\n",
    "    def __iter__(self):\n",
    "        file_list = os.listdir(dir_path)\n",
    "        for fonds_f in file_list:\n",
    "            assert \".txt\" in fonds_f, \"All files should be Plaintext.\" \n",
    "            file_path = dir_path+fonds_f\n",
    "            with utils.open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    # Lowercase the tokens\n",
    "                    yield list(tokenize(line.lower()))   #list(tokenize(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e40df-7f73-4cb6-a494-fb9b05fab75c",
   "metadata": {},
   "source": [
    "Define the hyperparameters for the unsupervised training of the fastText model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd541ad-e371-4bf0-9228-b41f01a0321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training architecture (default = \"cbow\" for Continuous Bag of Words)\n",
    "training_arch = \"cbow\"  #\"skipgram\n",
    "# Specify the learning rate (default = 0.025)\n",
    "alpha = 0.025\n",
    "# Specify the training objective (default = \"ns\")\n",
    "# losses = [\"ns\", \"hs\", \"softmax\"]\n",
    "# loss = losses[0]\n",
    "# Specify the number of negative words to sample for 'ns' training objective (default = 5)\n",
    "negative = 5\n",
    "# Specify the threshold for downsampling higher-frequency words (default = 0.001)\n",
    "sample = 0.001\n",
    "# Specify the word embeddings' dimensions\n",
    "vector_dimensions = 100 #50 #300\n",
    "# Specify the context window (default is 5) \n",
    "context_window = 5\n",
    "# Specify the number of epochs (default is 5)\n",
    "epochs = 5\n",
    "# Specify the threshold of word occurrences (ignore words that occur less than specified number of times; default = 5)\n",
    "min_count = 5\n",
    "# Specify the minimum and maximum length of character ngrams (defaults are 3 and 6)\n",
    "min_n = 2\n",
    "max_n = 6  # if 0, no character n-grams (subword vectors) will be used\n",
    "# Specify the number of buckets for hashing ngrams (default = 2000000) \n",
    "bucket = 2000000\n",
    "# Sort vocabulary by descending frequency (default = 1)\n",
    "sorted_vocab = 1\n",
    "# Specify the number of threads to use (default = 12)\n",
    "# threads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4aaee7d-7c31-406b-b34e-5e16e7f3895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = FastText(\n",
    "    alpha=alpha, negative=negative, sample=sample,\n",
    "    vector_size=vector_dimensions, window=context_window, \n",
    "    epochs=epochs, min_count=min_count, min_n=min_n, \n",
    "    max_n=max_n, bucket=bucket, sorted_vocab=sorted_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2979ddb7-0641-45dd-9151-c588d2aa542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.build_vocab(corpus_iterable=CorpusIterator())\n",
    "total_examples = embedding_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f67724-a502-4d0c-82ba-f39c01987eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7321545, 10119275)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.train(corpus_iterable=CorpusIterator(), total_examples=total_examples, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9645734-e845-4989-b61c-4bf9c0578eef",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e319e40-8164-4a0d-b31a-ba359a1c22a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext_cbow_100d.model\n"
     ]
    }
   ],
   "source": [
    "file_name = \"fasttext_{a}_{d}d.model\".format(a=training_arch, d=vector_dimensions)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6573211e-bd59-423b-a0b0-fcf6e1c142b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.save(\"models/\"+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d19f4f-8c6a-4daa-9f6a-df23e9cb11dc",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b09ea3e0-cc78-4d47-b419-55ad7d37a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_data = \"../data/token_clf_data/experiment_input/document_5fold.csv\"\n",
    "token_data = \"../data/token_clf_data/experiment_input/token_5fold.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4bf00252-5ed2-4d1e-9fd6-1700cd2a58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = \"token\"           # \"description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7c3d0ac1-13d9-45d3-bfae-dbc4be808c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>split2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>split2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id       token token_offsets  \\\n",
       "0               0            0   99999         0  Identifier       (0, 10)   \n",
       "1               0            0   99999         1           :      (10, 11)   \n",
       "2               0            0   99999         2         AA5      (12, 15)   \n",
       "3               1            1   99999         3       Title      (17, 22)   \n",
       "4               1            1   99999         4           :      (22, 23)   \n",
       "\n",
       "  pos tag       field    fold  \n",
       "0  NN   O  Identifier  split4  \n",
       "1   :   O  Identifier  split4  \n",
       "2  NN   O  Identifier  split4  \n",
       "3  NN   O       Title  split2  \n",
       "4   :   O       Title  split2  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(token_data, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8695dd5f-201c-4072-b4ef-d4ae66ab30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_labels = [\"B-Generalization\", \"I-Generalization\", \"B-Gendered-Role\", \"I-Gendered-Role\", \"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e1e45797-6c10-40d6-a4e3-c366c2fedcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_consider = ling_labels\n",
    "col = \"tag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "933a0988-209c-4ae1-bd7d-32d130a6d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implodeDataFrame(df, cols_to_groupby):\n",
    "    cols_to_agg = list(df.columns)\n",
    "    for col in cols_to_groupby:\n",
    "        cols_to_agg.remove(col)\n",
    "    agg_dict = dict.fromkeys(cols_to_agg, lambda x: x.tolist())\n",
    "    return df.groupby(cols_to_groupby).agg(agg_dict).reset_index().set_index(cols_to_groupby)\n",
    "\n",
    "def preprocessTokenData(df, col, label_list):\n",
    "    initial_shape = df.shape\n",
    "    # Change any tags not in label_list to \"O\"\n",
    "    df_l = df.loc[df[col].isin(label_list)]\n",
    "    df_o = df.loc[~df[col].isin(label_list)]\n",
    "    df_o = df_o.drop(columns=[col])\n",
    "    df_o.insert(len(df_o.columns), col, ([\"O\"]*(df_o.shape[0])))\n",
    "    df = pd.concat([df_l, df_o])\n",
    "    df = df.sort_values(by=\"token_id\")\n",
    "    assert initial_shape == df.shape, \"The DataFrame should have the same number of rows and columns after changing select column values.\"\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Replace tags with labels, removing \"B-\" and \"I-\" from the start of the tags\n",
    "    old_col = df[col]\n",
    "    new_col = [tag[2:] if tag != \"O\" else tag for tag in old_col]\n",
    "    df = df.drop(columns=[col])\n",
    "    df.insert((len(df.columns)-2), col, new_col)\n",
    "    \n",
    "    # Drop unneeded columns and then get group by token, so there's one row per token and \n",
    "    # lists of tags for that token\n",
    "    df = implodeDataFrame(df, [\"description_id\", \"sentence_id\", \"token_id\", \"token\", \"pos\", \"field\", \"token_offsets\"])\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Deduplicate tag lists and remove any \"O\" tags from lists with other values\n",
    "    old_col = list(df[col])\n",
    "    dedup_col = [list(set(value_list)) for value_list in old_col]\n",
    "    assert len(old_col) == len(dedup_col), \"The column should have the same number of rows.\"\n",
    "    new_col = []\n",
    "    for col_list in dedup_col:\n",
    "        if (\"O\" in col_list) and (len(col_list) > 1):\n",
    "            col_list.remove(\"O\")\n",
    "        new_col += [col_list]\n",
    "    assert len(new_col) == len(old_col), \"The column should have the same number of rows.\"\n",
    "    df = df.drop(columns=[col])\n",
    "    df.insert((len(df.columns)-2), col, new_col)\n",
    "    \n",
    "    return df  #.explode([col])  # one tag-token pair per row, tokens can repeat across rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c2249845-f534-4240-b03e-3837aec2b601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>field</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>tag</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>NN</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>NN</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>NN</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>Title</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  token_id       token pos       field  \\\n",
       "0               0            0         0  Identifier  NN  Identifier   \n",
       "1               0            0         1           :   :  Identifier   \n",
       "2               0            0         2         AA5  NN  Identifier   \n",
       "3               1            1         3       Title  NN       Title   \n",
       "4               1            1         4           :   :       Title   \n",
       "\n",
       "  token_offsets  tag   ann_id      fold  \n",
       "0       (0, 10)  [O]  [99999]  [split4]  \n",
       "1      (10, 11)  [O]  [99999]  [split4]  \n",
       "2      (12, 15)  [O]  [99999]  [split4]  \n",
       "3      (17, 22)  [O]  [99999]  [split2]  \n",
       "4      (22, 23)  [O]  [99999]  [split2]  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessTokenData(df, col, labels_to_consider)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d45102c3-a5ef-49fa-a861-58b92d35e83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[O]                                   744728\n",
       "[Gendered-Pronoun]                      3624\n",
       "[Gendered-Role]                         3151\n",
       "[Generalization]                        1808\n",
       "[Gendered-Pronoun, Generalization]       107\n",
       "[Gendered-Role, Generalization]          103\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94882af-3a43-4185-aa92-934047a752ef",
   "metadata": {},
   "source": [
    "### 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "10073110-da46-43a6-9bf6-cf54e46f996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(df, feature_cols=[\"token_id\", \"token\"]):\n",
    "    # Zip the features\n",
    "    feature_data = list(zip(df[feature_cols[0]], df[feature_cols[1]]))\n",
    "    \n",
    "    # Make FastText feature matrix\n",
    "    feature_list = [embedding_model.wv[token.lower()] for token_id,token in feature_data]\n",
    "    return np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "65deec86-1a15-4425-b8cb-741bc8374916",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = getFeatures(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8907d59-eaf5-4967-b3ab-a40c994f1a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c3f2a87-b536-4b8f-a703-c996b1842845",
   "metadata": {},
   "source": [
    "### 4. Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630c56d-3eab-41ce-bb86-501f775d99ca",
   "metadata": {},
   "source": [
    "Define the five splits of the data to combine iteratively into training and test sets using five-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0822d197-6b61-48f8-9877-f4659324b835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['split0' 'split1' 'split2' 'split3' 'split4']\n"
     ]
    }
   ],
   "source": [
    "split_col = \"fold\"\n",
    "splits = df[split_col].unique()\n",
    "splits.sort()\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd40aa4f-ee5a-4741-89c5-e6fba10c386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train0, test0 = list(splits[:4]), splits[4]\n",
    "train1, test1 = list(splits[1:]), splits[0]\n",
    "train2, test2 = list(splits[2:])+[splits[0]], splits[1]\n",
    "train3, test3 = list(splits[3:])+list(splits[:2]), splits[2]\n",
    "train4, test4 = [splits[4]]+list(splits[:3]), splits[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "782865c2-3aad-4f87-b2a2-6fbf35aaca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['split0', 'split1', 'split2', 'split3'], 'split4')\n",
      "(['split1', 'split2', 'split3', 'split4'], 'split0')\n",
      "(['split2', 'split3', 'split4', 'split0'], 'split1')\n",
      "(['split3', 'split4', 'split0', 'split1'], 'split2')\n",
      "(['split4', 'split0', 'split1', 'split2'], 'split3')\n"
     ]
    }
   ],
   "source": [
    "runs = [(train0, test0), (train1, test1), (train2, test2), (train3, test3), (train4, test4)]\n",
    "for run in runs:\n",
    "    print(run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias-env",
   "language": "python",
   "name": "gender-bias-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

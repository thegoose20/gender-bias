{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2620f5d3-7e3f-40c0-b228-133985206342",
   "metadata": {},
   "source": [
    "# Apply Trained Models\n",
    "## Token Classifiers for Linguistic Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b9a26-d012-4a19-bbb2-0ddd8f7a11bb",
   "metadata": {},
   "source": [
    "Import programming libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6321358c-2015-41a4-82ad-5c999323018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "# Libraries for data, file, and model loading\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os, re\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for word embeddings\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from gensim.utils import tokenize\n",
    "from gensim import utils\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "# Libraries for Experiment 1 scikit-learn estimators\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912e863-1a42-4c13-a067-206c6686d088",
   "metadata": {},
   "source": [
    "### 1. Create Word Embeddings\n",
    "\n",
    "Train custom word embeddings on metadata descriptions from the University of Edinburgh Heritage Collections' Archives catalog.\n",
    "\n",
    "* Data file: `descriptions_by_fonds`\n",
    "* Date of harvesting: October 2020\n",
    "* Harvesting and transformation code: [annot-prep/PreparationForAnnotation.ipynb](https://github.com/thegoose20/annot-prep/blob/main/PreparationForAnnotation.ipynb)\n",
    "\n",
    "References:\n",
    "* https://radimrehurek.com/gensim/models/fasttext.html\n",
    "* https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66007443-71ea-48be-9274-abfff3e4cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1079\n"
     ]
    }
   ],
   "source": [
    "dir_path = config.inf_data_path+<BT_RAWTEXT>\n",
    "file_list = os.listdir(dir_path)\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2390b5b7-8c8e-4dee-9ae0-78a39f813c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusIterator:\n",
    "    def __iter__(self):\n",
    "        file_list = os.listdir(dir_path)\n",
    "        for fonds_f in file_list:\n",
    "            assert \".txt\" in fonds_f, \"All files should be Plaintext.\" \n",
    "            file_path = dir_path+fonds_f\n",
    "            with utils.open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    # Lowercase the tokens\n",
    "                    yield list(tokenize(line.lower()))   #list(tokenize(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babb8d47-c8e6-4ac7-b149-de716c9a6fcd",
   "metadata": {},
   "source": [
    "Define the hyperparameters for the unsupervised training of the fastText model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac579512-62af-49b2-9921-317913e6f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training architecture (default = \"cbow\" for Continuous Bag of Words)\n",
    "training_arch = \"cbow\"  #\"skipgram\n",
    "# Specify the learning rate (default = 0.025)\n",
    "alpha = 0.025\n",
    "# Specify the training objective (default = \"ns\")\n",
    "# losses = [\"ns\", \"hs\", \"softmax\"]\n",
    "# loss = losses[0]\n",
    "# Specify the number of negative words to sample for 'ns' training objective (default = 5)\n",
    "negative = 5\n",
    "# Specify the threshold for downsampling higher-frequency words (default = 0.001)\n",
    "sample = 0.001\n",
    "# Specify the word embeddings' dimensions\n",
    "vector_dimensions = 100 #50 #300\n",
    "# Specify the context window (default is 5) \n",
    "context_window = 5\n",
    "# Specify the number of epochs (default is 5)\n",
    "epochs = 5\n",
    "# Specify the threshold of word occurrences (ignore words that occur less than specified number of times; default = 5)\n",
    "min_count = 5\n",
    "# Specify the minimum and maximum length of character ngrams (defaults are 3 and 6)\n",
    "min_n = 2\n",
    "max_n = 6  # if 0, no character n-grams (subword vectors) will be used\n",
    "# Specify the number of buckets for hashing ngrams (default = 2000000) \n",
    "bucket = 2000000\n",
    "# Sort vocabulary by descending frequency (default = 1)\n",
    "sorted_vocab = 1\n",
    "# Specify the number of threads to use (default = 12)\n",
    "# threads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e281573a-1d2b-496e-97e1-de864339bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = FastText(\n",
    "    alpha=alpha, negative=negative, sample=sample,\n",
    "    vector_size=vector_dimensions, window=context_window, \n",
    "    epochs=epochs, min_count=min_count, min_n=min_n, \n",
    "    max_n=max_n, bucket=bucket, sorted_vocab=sorted_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8ffa0e-57e3-427c-aa71-0963dceb5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.build_vocab(corpus_iterable=CorpusIterator())\n",
    "total_examples = embedding_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e1e193-68c9-4978-9829-5cd43aa90c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7321545, 10119275)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.train(corpus_iterable=CorpusIterator(), total_examples=total_examples, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b5ce59-4c0b-4deb-a63b-ea73de316f2a",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5039db-14a1-4ea7-8931-e4da1282134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext_cbow_100d.model\n"
     ]
    }
   ],
   "source": [
    "file_name = \"fasttext_{a}_{d}d.model\".format(a=training_arch, d=vector_dimensions)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed9ca23-eedc-4470-9890-742045f37504",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.save(\"models/\"+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfa0d8-cf04-40b2-a943-7c315bba436d",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8ce43-7abb-4ddf-b4d3-aeba320ecd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data so columns for text and field name\n",
    "# Assign IDs to fields\n",
    "# Tokenize text\n",
    "# Part-of-speech tag text\n",
    "# Assign IDs to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e37077-79fb-443e-8827-4b4e32818db6",
   "metadata": {},
   "source": [
    "### 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2fad9072-f046-4b97-bc35-c5b9e1758aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the features\n",
    "feature_data = list(zip(feature_list1, feature_list2))\n",
    "\n",
    "# Make FastText feature matrix\n",
    "feature_list = [embedding_model.wv[token.lower()] for token_id,token in feature_data]\n",
    "X = np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e0460b1-f9bf-475b-88df-ad20983fe20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52353590-fadc-47b8-bd2d-66f5c4d312be",
   "metadata": {},
   "source": [
    "### 4. Prediction with Trained Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3270a903-4f9e-4c8f-b83d-07a25bf4d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Stereotype & Omission labels, no features\n",
    "# baseline_model = \"models/baseline/sgd-svm_F-tfidf_T-so.joblib\"\n",
    "\n",
    "# [Baseline] Linguistic Label\n",
    "clf = \"models/for_reuse/multilabel_linguistic/cc-{a}_F-fastText{d}_T-linglabels.joblib\"\n",
    "mlb = \"models/for_reuse/multilabel_linguistic/mlb_linglabels.joblib\"\n",
    "\n",
    "# Experiment 2:\n",
    "# exp2_model_file = \n",
    "\n",
    "# Experiment 3:\n",
    "# exp3_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4c6b1cf3-1e96-4380-8da3-fb09070c26e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = joblib.load(exp1_ling_model)   #baseline_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7d40c45e-8da2-4964-ace8-1a89bc54cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trained_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6e546e90-c5e3-4249-bb8f-aad9723865fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "tags = df[col]\n",
    "y = mlb.fit_transform(tags)\n",
    "print(y[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2397ec9e-eb17-49f0-b056-75a840de8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_text = mlb.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cec84722-d904-4ee6-bedd-56707216146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',), ('O',)]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e4cf7-bbb5-4ae4-aa14-04294c811a3a",
   "metadata": {},
   "source": [
    "Add the predictions to the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eeab8ce4-3156-4b8c-874d-3dd2438a509f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>field</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>tag</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>pred_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>NN</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split4]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split4]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>NN</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split4]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>NN</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>Title</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>Title</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>Title</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>Title</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[14384, 24275, 52952]</td>\n",
       "      <td>[split2, split2, split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Very</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Title</td>\n",
       "      <td>(38, 42)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[14384, 52952, 24275]</td>\n",
       "      <td>[split2, split2, split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Rev</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Title</td>\n",
       "      <td>(43, 46)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[14384, 24275, 26233, 52952]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Prof</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Title</td>\n",
       "      <td>(47, 51)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[52952, 24275, 14384, 26233]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>James</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Title</td>\n",
       "      <td>(52, 57)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[14384, 52952, 24275, 26233]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Title</td>\n",
       "      <td>(58, 63)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[14384, 24275, 26233, 52952]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>Title</td>\n",
       "      <td>(64, 65)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1920-2005</td>\n",
       "      <td>CD</td>\n",
       "      <td>Title</td>\n",
       "      <td>(65, 74)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>Title</td>\n",
       "      <td>(74, 75)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split2]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Scope</td>\n",
       "      <td>NN</td>\n",
       "      <td>Scope and Contents</td>\n",
       "      <td>(77, 82)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split1]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>Scope and Contents</td>\n",
       "      <td>(83, 86)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split1]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Contents</td>\n",
       "      <td>NNS</td>\n",
       "      <td>Scope and Contents</td>\n",
       "      <td>(87, 95)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split1]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>Scope and Contents</td>\n",
       "      <td>(95, 96)</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[99999]</td>\n",
       "      <td>[split1]</td>\n",
       "      <td>(O,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    description_id  sentence_id  token_id       token  pos  \\\n",
       "0                0            0         0  Identifier   NN   \n",
       "1                0            0         1           :    :   \n",
       "2                0            0         2         AA5   NN   \n",
       "3                1            1         3       Title   NN   \n",
       "4                1            1         4           :    :   \n",
       "5                1            1         5      Papers  NNS   \n",
       "6                1            1         6          of   IN   \n",
       "7                1            1         7         The   DT   \n",
       "8                1            1         8        Very  NNP   \n",
       "9                1            1         9         Rev  NNP   \n",
       "10               1            1        10        Prof  NNP   \n",
       "11               1            1        11       James  NNP   \n",
       "12               1            1        12       Whyte  NNP   \n",
       "13               1            1        13           (    (   \n",
       "14               1            1        14   1920-2005   CD   \n",
       "15               1            1        15           )    )   \n",
       "16               2            2        16       Scope   NN   \n",
       "17               2            2        17         and   CC   \n",
       "18               2            2        18    Contents  NNS   \n",
       "19               2            2        19           :    :   \n",
       "\n",
       "                 field token_offsets  tag                        ann_id  \\\n",
       "0           Identifier       (0, 10)  [O]                       [99999]   \n",
       "1           Identifier      (10, 11)  [O]                       [99999]   \n",
       "2           Identifier      (12, 15)  [O]                       [99999]   \n",
       "3                Title      (17, 22)  [O]                       [99999]   \n",
       "4                Title      (22, 23)  [O]                       [99999]   \n",
       "5                Title      (24, 30)  [O]                       [99999]   \n",
       "6                Title      (31, 33)  [O]                       [99999]   \n",
       "7                Title      (34, 37)  [O]         [14384, 24275, 52952]   \n",
       "8                Title      (38, 42)  [O]         [14384, 52952, 24275]   \n",
       "9                Title      (43, 46)  [O]  [14384, 24275, 26233, 52952]   \n",
       "10               Title      (47, 51)  [O]  [52952, 24275, 14384, 26233]   \n",
       "11               Title      (52, 57)  [O]  [14384, 52952, 24275, 26233]   \n",
       "12               Title      (58, 63)  [O]  [14384, 24275, 26233, 52952]   \n",
       "13               Title      (64, 65)  [O]                       [99999]   \n",
       "14               Title      (65, 74)  [O]                       [99999]   \n",
       "15               Title      (74, 75)  [O]                       [99999]   \n",
       "16  Scope and Contents      (77, 82)  [O]                       [99999]   \n",
       "17  Scope and Contents      (83, 86)  [O]                       [99999]   \n",
       "18  Scope and Contents      (87, 95)  [O]                       [99999]   \n",
       "19  Scope and Contents      (95, 96)  [O]                       [99999]   \n",
       "\n",
       "                                fold pred_tag  \n",
       "0                           [split4]     (O,)  \n",
       "1                           [split4]     (O,)  \n",
       "2                           [split4]     (O,)  \n",
       "3                           [split2]     (O,)  \n",
       "4                           [split2]     (O,)  \n",
       "5                           [split2]     (O,)  \n",
       "6                           [split2]     (O,)  \n",
       "7           [split2, split2, split2]     (O,)  \n",
       "8           [split2, split2, split2]     (O,)  \n",
       "9   [split2, split2, split2, split2]     (O,)  \n",
       "10  [split2, split2, split2, split2]     (O,)  \n",
       "11  [split2, split2, split2, split2]     (O,)  \n",
       "12  [split2, split2, split2, split2]     (O,)  \n",
       "13                          [split2]     (O,)  \n",
       "14                          [split2]     (O,)  \n",
       "15                          [split2]     (O,)  \n",
       "16                          [split1]     (O,)  \n",
       "17                          [split1]     (O,)  \n",
       "18                          [split1]     (O,)  \n",
       "19                          [split1]     (O,)  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.insert(len(df.columns), \"pred_\"+col, predictions_text)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "66fd7a52-3dbe-4d63-8ab6-f1aad821cfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(O,)    753521\n",
       "Name: pred_tag, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pred_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63401e9-edc2-4a3f-8dfa-f97780908453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias-env",
   "language": "python",
   "name": "gender-bias-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

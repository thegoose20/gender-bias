{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance Testing: Paired Bootstrap Test\n",
    "\n",
    "* **Sample Size:** 10% of DevTest data\n",
    "* **Number Samples:** 1000\n",
    "\n",
    "Compare the performance of two models and determine whether one is significantly better than the other.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For path variables\n",
    "import config, utils\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For fastText embeddings\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For classification\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For statistical significance testing\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the paths to the model input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = config.tokc_path+\"model_input/token_train.csv\"\n",
    "train_data = utils.preprocess(train_data_path)\n",
    "dev_data_path = config.tokc_path+\"model_input/token_validate.csv\"\n",
    "dev_data_full = utils.preprocess(dev_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the dimensionality of word embeddings for the models being compared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]\n",
    "d = dimensions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the fraction of samples to include in each classifier training instance, and the number of classifier instances to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_samples = 0.1\n",
    "n_classifiers = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training and devtest data, extract features and binarize targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "target_col = \"tag\"\n",
    "feature_cols = [\"token_id\", \"token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Load data\n",
    "# ------------------------\n",
    "def zipTokensFeatures(loaded_data):\n",
    "    token_data = list(zip(loaded_data[feature_cols[0]], loaded_data[feature_cols[1]]))\n",
    "    return token_data\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Extract GloVe features\n",
    "# ------------------------\n",
    "glove = utils.getGloveEmbeddings(d)\n",
    "def extractGloveEmbedding(token, embedding_dict=glove, dimensions=int(d)):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    try:\n",
    "        embedding = embedding_dict[token]\n",
    "    except KeyError:\n",
    "        embedding = np.zeros((dimensions,))\n",
    "    return embedding.reshape(-1,1)\n",
    "\n",
    "def makeGloveFeatureMatrix(token_data, dimensions=int(d)):    \n",
    "    feature_list = [extractGloveEmbedding(token) for token_id,token in token_data]\n",
    "    return np.array(feature_list).reshape(-1,dimensions)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Extract fastText features\n",
    "# ------------------------\n",
    "file_name = config.fasttext_path+\"fasttext{}_lowercased.model\".format(d)\n",
    "embedding_model = FastText.load(file_name)\n",
    "def extractFastTextEmbedding(token, fasttext_model=embedding_model):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    embedding = fasttext_model.wv[token]\n",
    "    return embedding\n",
    "\n",
    "def makeFastTextFeatureMatrix(token_data):\n",
    "    feature_list = [extractFastTextEmbedding(token) for token_id,token in token_data]\n",
    "    return np.array(feature_list)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Binarize targets\n",
    "# ------------------------\n",
    "def binarizeTrainTargets(train_data):\n",
    "    y_train_labels = train_data[target_col]\n",
    "    y_train = mlb.fit_transform(y_train_labels)\n",
    "    return mlb, y_train\n",
    "\n",
    "def binarizeDevTargets(mlb, dev_data):\n",
    "    y_dev_labels = dev_data[target_col]\n",
    "    y_dev = mlb.transform(y_dev_labels)\n",
    "    return y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "\n",
    "Train two classification models, one with GloVe word embeddings as features and one with custom fastText word embeddings as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess training data\n",
    "train_tokens = zipTokensFeatures(train_data)\n",
    "\n",
    "# Get GloVe features\n",
    "X_train_glove = makeGloveFeatureMatrix(train_tokens)\n",
    "\n",
    "# Get custom fastText features\n",
    "X_train_ft = makeFastTextFeatureMatrix(train_tokens)\n",
    "\n",
    "# Get targets\n",
    "mlb, y_train = binarizeTrainTargets(train_data)\n",
    "\n",
    "# Train a model with GloVe embeddings as features\n",
    "clf_glove = ClassifierChain(classifier = RandomForestClassifier(random_state=22))\n",
    "clf_glove.fit(X_train_glove, y_train)\n",
    "\n",
    "# Train a model with custom fastText embeddings as features\n",
    "clf_ft = ClassifierChain(classifier = RandomForestClassifier(random_state=22))\n",
    "clf_ft.fit(X_train_ft, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and Evaluate\n",
    "\n",
    "Test the two classifiers on random samples of equal size from the devtest data, and calculate the macro (average across all labels) precision, recall, and F1 scores for the model's performance for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_f1_scores, glove_precision_scores, glove_recall_scores = [], [], []\n",
    "ft_f1_scores, ft_precision_scores, ft_recall_scores = [], [], []\n",
    "import time\n",
    "start = time.time()\n",
    "counter = 0\n",
    "for n in range(n_classifiers):\n",
    "    # Load and preprocess a sample of devtest data\n",
    "    dev_data = dev_data_full.sample(frac=frac_samples, replace=True)\n",
    "    dev_tokens = zipTokensFeatures(dev_data)\n",
    "    \n",
    "    # Extract GloVe and custom fastText features for the devtest data sample\n",
    "    X_dev_glove = makeGloveFeatureMatrix(dev_tokens)\n",
    "    X_dev_ft = makeFastTextFeatureMatrix(dev_tokens)\n",
    "    \n",
    "    # Get targets\n",
    "    y_dev = binarizeDevTargets(mlb, dev_data)\n",
    "    \n",
    "    # Predict and evaluate the model with GloVe embeddings as features\n",
    "    predictions_glove = clf_glove.predict(X_dev_glove)\n",
    "    glove_precision_scores += [metrics.precision_score(y_dev, predictions_glove, average=\"macro\", zero_division=0)]\n",
    "    glove_recall_scores += [metrics.recall_score(y_dev, predictions_glove, average=\"macro\", zero_division=0)]\n",
    "    glove_f1_scores += [metrics.f1_score(y_dev, predictions_glove, average=\"macro\", zero_division=0)]\n",
    "    \n",
    "    # Predict and evaluate the model with custom fastText embeddings as features\n",
    "    predictions_ft = clf_ft.predict(X_dev_ft)\n",
    "    ft_precision_scores += [metrics.precision_score(y_dev, predictions_ft, average=\"macro\", zero_division=0)]\n",
    "    ft_recall_scores += [metrics.recall_score(y_dev, predictions_ft, average=\"macro\", zero_division=0)]\n",
    "    ft_f1_scores += [metrics.f1_score(y_dev, predictions_ft, average=\"macro\", zero_division=0)]\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    if (counter%10 == 0):\n",
    "        print(counter, \"iterations\")\n",
    "    \n",
    "assert len(glove_f1_scores) == n_classifiers\n",
    "assert len(ft_f1_scores) == n_classifiers\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_precision = ttest_ind(ft_precision_scores, glove_precision_scores)\n",
    "print(ttest_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_recall = ttest_ind(ft_recall_scores, glove_recall_scores)\n",
    "print(ttest_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_f1 = ttest_ind(ft_f1_scores, glove_f1_scores)\n",
    "print(ttest_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does order of input arrays matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_precision2 = ttest_ind(glove_precision_scores, ft_precision_scores)\n",
    "print(ttest_precision2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_recall2 = ttest_ind(glove_recall_scores, ft_recall_scores)\n",
    "print(ttest_recall2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_f12 = ttest_ind(glove_f1_scores, ft_f1_scores)\n",
    "print(ttest_f12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "1st run (no random state defined):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Statistical Significance\n",
    "\n",
    "* Alpha = 0.5 (5%)\n",
    "\n",
    "* Null Hypothesis: The performance of a model with pre-trained GloVe embeddings is not significantly different (better or worse) than a model trained on fastText embeddings custom-trained on the model's corpus.\n",
    "\n",
    "* Alternative Hypothesis: The performance of a model with pre-trained GloVe embeddings is significantly different than a model trained on fastText embeddings custom-trained on the model's corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=14.044422128917015, pvalue=8.4308482295255e-43)\n"
     ]
    }
   ],
   "source": [
    "ttest_precision = ttest_ind(ft_precision_scores, glove_precision_scores)\n",
    "print(ttest_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=45.939822675124056, pvalue=4.21469359e-315)\n"
     ]
    }
   ],
   "source": [
    "ttest_recall = ttest_ind(ft_recall_scores, glove_recall_scores)\n",
    "print(ttest_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=38.907545359180126, pvalue=5.559508823533222e-247)\n"
     ]
    }
   ],
   "source": [
    "ttest_f1 = ttest_ind(ft_f1_scores, glove_f1_scores)\n",
    "print(ttest_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does order of input arrays matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-14.044422128917015, pvalue=8.4308482295255e-43)\n"
     ]
    }
   ],
   "source": [
    "ttest_precision2 = ttest_ind(glove_precision_scores, ft_precision_scores)\n",
    "print(ttest_precision2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-45.939822675124056, pvalue=4.21469359e-315)\n"
     ]
    }
   ],
   "source": [
    "ttest_recall2 = ttest_ind(glove_recall_scores, ft_recall_scores)\n",
    "print(ttest_recall2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-38.907545359180126, pvalue=5.559508823533222e-247)\n"
     ]
    }
   ],
   "source": [
    "ttest_f12 = ttest_ind(glove_f1_scores, ft_f1_scores)\n",
    "print(ttest_f12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

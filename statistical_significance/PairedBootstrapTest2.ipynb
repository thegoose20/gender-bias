{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance Testing: Paired Bootstrap Test\n",
    "\n",
    "* **Sample Size:** 20% of DevTest data\n",
    "* **Number Samples:** 10,000\n",
    "\n",
    "Compare the performance of two models and determine whether one is significantly better than the other.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For path variables\n",
    "import config, utils\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For fastText embeddings\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For classification\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For statistical significance testing\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the paths to the model input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = config.tokc_path+\"model_input/token_train.csv\"\n",
    "train_data = utils.preprocess(train_data_path)\n",
    "dev_data_path = config.tokc_path+\"model_input/token_validate.csv\"\n",
    "dev_data_full = utils.preprocess(dev_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the dimensionality of word embeddings for the models being compared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]\n",
    "d = dimensions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the fraction of samples to include in each classifier training instance, and the number of classifier instances to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_samples = 0.2\n",
    "# n_samples = 100\n",
    "n_classifiers = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training and devtest data, extract features and binarize targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "target_col = \"tag\"\n",
    "feature_cols = [\"token_id\", \"token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Load data\n",
    "# ------------------------\n",
    "def zipTokensFeatures(loaded_data):\n",
    "    token_data = list(zip(loaded_data[feature_cols[0]], loaded_data[feature_cols[1]]))\n",
    "    return token_data\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Extract GloVe features\n",
    "# ------------------------\n",
    "glove = utils.getGloveEmbeddings(d)\n",
    "def extractGloveEmbedding(token, embedding_dict=glove, dimensions=int(d)):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    try:\n",
    "        embedding = embedding_dict[token]\n",
    "    except KeyError:\n",
    "        embedding = np.zeros((dimensions,))\n",
    "    return embedding.reshape(-1,1)\n",
    "\n",
    "def makeGloveFeatureMatrix(token_data, dimensions=int(d)):    \n",
    "    feature_list = [extractGloveEmbedding(token) for token_id,token in token_data]\n",
    "    return np.array(feature_list).reshape(-1,dimensions)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Extract fastText features\n",
    "# ------------------------\n",
    "file_name = config.fasttext_path+\"fasttext{}_lowercased.model\".format(d)\n",
    "embedding_model = FastText.load(file_name)\n",
    "def extractFastTextEmbedding(token, fasttext_model=embedding_model):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    embedding = fasttext_model.wv[token]\n",
    "    return embedding\n",
    "\n",
    "def makeFastTextFeatureMatrix(token_data):\n",
    "    feature_list = [extractFastTextEmbedding(token) for token_id,token in token_data]\n",
    "    return np.array(feature_list)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Binarize targets\n",
    "# ------------------------\n",
    "def binarizeTrainTargets(train_data):\n",
    "    y_train_labels = train_data[target_col]\n",
    "    y_train = mlb.fit_transform(y_train_labels)\n",
    "    return mlb, y_train\n",
    "\n",
    "def binarizeDevTargets(mlb, dev_data):\n",
    "    y_dev_labels = dev_data[target_col]\n",
    "    y_dev = mlb.transform(y_dev_labels)\n",
    "    return y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "\n",
    "Train two classification models, one with GloVe word embeddings as features and one with custom fastText word embeddings as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess training data\n",
    "train_tokens = zipTokensFeatures(train_data)\n",
    "\n",
    "# Get GloVe features\n",
    "X_train_glove = makeGloveFeatureMatrix(train_tokens)\n",
    "\n",
    "# Get custom fastText features\n",
    "X_train_ft = makeFastTextFeatureMatrix(train_tokens)\n",
    "\n",
    "# Get targets\n",
    "mlb, y_train = binarizeTrainTargets(train_data)\n",
    "\n",
    "# Train a model with GloVe embeddings as features\n",
    "clf_glove = ClassifierChain(classifier = RandomForestClassifier(random_state=22))\n",
    "clf_glove.fit(X_train_glove, y_train)\n",
    "\n",
    "# Train a model with custom fastText embeddings as features\n",
    "clf_ft = ClassifierChain(classifier = RandomForestClassifier(random_state=22))\n",
    "clf_ft.fit(X_train_ft, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and Evaluate\n",
    "\n",
    "Test the two classifiers on random samples of equal size from the devtest data, and calculate the macro (average across all labels) precision, recall, and F1 scores for the model's performance for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_f1_scores, glove_precision_scores, glove_recall_scores = [], [], []\n",
    "ft_f1_scores, ft_precision_scores, ft_recall_scores = [], [], []\n",
    "counter = 0\n",
    "start = time.time()\n",
    "for n in range(n_classifiers):\n",
    "    # Load and preprocess a sample of devtest data\n",
    "    dev_data = dev_data_full.sample(frac=frac_samples, replace=True)\n",
    "    dev_tokens = zipTokensFeatures(dev_data)\n",
    "    \n",
    "    # Extract GloVe and custom fastText features for the devtest data sample\n",
    "    X_dev_glove = makeGloveFeatureMatrix(dev_tokens)\n",
    "    X_dev_ft = makeFastTextFeatureMatrix(dev_tokens)\n",
    "    \n",
    "    # Get targets\n",
    "    y_dev = binarizeDevTargets(mlb, dev_data)\n",
    "    \n",
    "    # Predict and evaluate the model with GloVe embeddings as features\n",
    "    predictions_glove = clf_glove.predict(X_dev_glove)\n",
    "    glove_precision_scores += [metrics.precision_score(y_dev, predictions_glove, average=\"macro\", zero_division=0)]\n",
    "    glove_recall_scores += [metrics.recall_score(y_dev, predictions_glove, average=\"macro\", zero_division=0)]\n",
    "    glove_f1_scores += [metrics.f1_score(y_dev, predictions_glove, average=\"macro\", zero_division=0)]\n",
    "    \n",
    "    # Predict and evaluate the model with custom fastText embeddings as features\n",
    "    predictions_ft = clf_ft.predict(X_dev_ft)\n",
    "    ft_precision_scores += [metrics.precision_score(y_dev, predictions_ft, average=\"macro\", zero_division=0)]\n",
    "    ft_recall_scores += [metrics.recall_score(y_dev, predictions_ft, average=\"macro\", zero_division=0)]\n",
    "    ft_f1_scores += [metrics.f1_score(y_dev, predictions_ft, average=\"macro\", zero_division=0)]\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    if (counter%10 == 0):\n",
    "        print(counter, \"iterations\")\n",
    "    \n",
    "assert len(glove_f1_scores) == n_classifiers\n",
    "assert len(ft_f1_scores) == n_classifiers\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Statistical Significance\n",
    "\n",
    "* Alpha = 0.5 (5%)\n",
    "\n",
    "* Null Hypothesis: The performance of a model with pre-trained GloVe embeddings is not significantly different (better or worse) than a model trained on fastText embeddings custom-trained on the model's corpus.\n",
    "\n",
    "* Alternative Hypothesis: The performance of a model with pre-trained GloVe embeddings is significantly different than a model trained on fastText embeddings custom-trained on the model's corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_precision = ttest_ind(ft_precision_scores, glove_precision_scores)\n",
    "print(ttest_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_recall = ttest_ind(ft_recall_scores, glove_recall_scores)\n",
    "print(ttest_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_f1 = ttest_ind(ft_f1_scores, glove_f1_scores)\n",
    "print(ttest_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does order of input arrays matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_precision2 = ttest_ind(glove_precision_scores, ft_precision_scores)\n",
    "print(ttest_precision2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_recall2 = ttest_ind(glove_recall_scores, ft_recall_scores)\n",
    "print(ttest_recall2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_f12 = ttest_ind(glove_f1_scores, ft_f1_scores)\n",
    "print(ttest_f12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to the Paired Bootstrap Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Select random sample sets of equal size, sampling with replacement (the same row can selected repeatedly in a sample).\n",
    "\n",
    "**Step 2:** On each sample set, run the model and calculate the precision, recall, and F1 score.\n",
    "\n",
    "**Step 3:** Compute the difference between each model's precision, recall, and F1 score for each random sample set.\n",
    "\n",
    "**Step 4:** Zero-center the data, subtracting the performance difference observed in the models overall from the sampled model performance scores.\n",
    "\n",
    "**Step 5:** Calculate the *p*-value, which is the proportion of samples that the model with GloVe embeddings performed better than the model with fastText embeddings???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165954, 7) (165740, 7)\n"
     ]
    }
   ],
   "source": [
    "print(glove_preds.shape, ft_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSampleScores(df, sample_size, n_samples, model, agmt_col):\n",
    "    sample_list, prec_list, rec_list, f1_list = [], [], [], []\n",
    "    for i in range(n_samples):\n",
    "        # Get a random selection of sample_size rows from df\n",
    "        sample = df.sample(n=sample_size)\n",
    "        # Calculate precision, recall, and f1 scores for that sample\n",
    "        agmts = list(sample[agmt_col])\n",
    "        precision, recall, f1 = utils.precisionRecallF1(\n",
    "            agmts.count(\"true positive\"), agmts.count(\"false positive\"), agmts.count(\"false negative\")\n",
    "        )\n",
    "        sample_list += [model+\" \"+str(i)]\n",
    "        prec_list += [precision]\n",
    "        rec_list += [recall]\n",
    "        f1_list += [f1]\n",
    "        \n",
    "    # Create a DataFrame (table) of the scores\n",
    "    return pd.DataFrame({\"sample\":sample_list, \"precision\":prec_list, \"recall\": rec_list, \"f_1\":f1_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100000\n",
    "agmt_col = \"_merge\"\n",
    "n_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"glove\"\n",
    "glove_samples_scores = getSampleScores(glove_preds, sample_size, n_samples, model, agmt_col)\n",
    "# glove_samples_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"fasttext\"\n",
    "ft_samples_scores = getSampleScores(ft_preds, sample_size, n_samples, model, agmt_col)\n",
    "# ft_samples_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=655.4455914909287, pvalue=0.0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_result = ttest_ind(ft_samples_scores[\"f_1\"], glove_samples_scores[\"f_1\"])\n",
    "ttest_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=0.0, pvalue=0.0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilcoxon_result = wilcoxon(ft_samples_scores[\"f_1\"], glove_samples_scores[\"f_1\"])\n",
    "wilcoxon_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtract the fastText scores from the GloVe scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_glove_f1 = np.mean(glove_samples_scores[\"f_1\"])\n",
    "mean_glove_f1 = np.mean(glove_samples_scores[\"f_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (glove_samples_scores.drop(columns=[\"sample\"])).subtract((ft_samples_scores.drop(columns=[\"sample\"])))\n",
    "# diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-center the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.010803</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>0.003004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.022315</td>\n",
       "      <td>-0.004442</td>\n",
       "      <td>-0.007194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.020473</td>\n",
       "      <td>-0.009523</td>\n",
       "      <td>-0.009204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.019699</td>\n",
       "      <td>-0.003604</td>\n",
       "      <td>-0.005568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.016723</td>\n",
       "      <td>-0.004196</td>\n",
       "      <td>-0.004569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall       f_1\n",
       "0  -0.010803  0.004652  0.003004\n",
       "1  -0.022315 -0.004442 -0.007194\n",
       "2  -0.020473 -0.009523 -0.009204\n",
       "3  -0.019699 -0.003604 -0.005568\n",
       "4  -0.016723 -0.004196 -0.004569"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_diff = perf_diff.iloc[0,0]\n",
    "rec_diff = perf_diff.iloc[0,1]\n",
    "f1_diff = perf_diff.iloc[0,2]\n",
    "overall_diff = pd.DataFrame({\n",
    "    \"precision\":([prec_diff]*diff.shape[0]), \"recall\":([rec_diff]*diff.shape[0]), \"f_1\":([f1_diff]*diff.shape[0])\n",
    "})\n",
    "zero_centered = diff.subtract(overall_diff)\n",
    "zero_centered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=12852233.0, pvalue=0.0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilcoxon(zero_centered[\"f_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePValue(df, col_name):\n",
    "    values = list(df[col_name])\n",
    "    count = 0\n",
    "    for value in values:\n",
    "        if value > 0:\n",
    "            count += 1\n",
    "    return count/(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.461 0.33\n"
     ]
    }
   ],
   "source": [
    "prec_p = calculatePValue(zero_centered, \"precision\")\n",
    "rec_p = calculatePValue(zero_centered, \"recall\")\n",
    "f1_p = calculatePValue(zero_centered, \"f_1\")\n",
    "print(prec_p, rec_p, f1_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

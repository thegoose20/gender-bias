{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Gender Biased Token Classifiers\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory ../data/token_clf_data/model_input/\n",
    "* Multilabel classification\n",
    "    * 3 categories of labels:\n",
    "        1. Person Name: Unknown, Non-binary, Feminine, Masculine\n",
    "        2. Linguistic: Generalization, Gendered Pronoun, Gendered Role\n",
    "        3. Contextual: Empowering, Occupation, Omission, Stereotype\n",
    "* Also try a [classifier chain](https://scikit-learn.org/stable/modules/multiclass.html#classifierchain)!!!\n",
    "\n",
    "***\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[0.](#0) Preprocessing\n",
    "\n",
    "[1.](#1) Logistic Regression (LR)\n",
    "\n",
    "[2.](#2) Perceptron\n",
    "\n",
    "[3.](#3) Stochastic Gradient Descent (SDG)\n",
    "\n",
    "[4.](#4) Support Vector Machines (SVM) (a.k.a. support vector classification (SVC))\n",
    "\n",
    "[5.](#5) Error Analysis\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "* https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets \n",
    "* https://towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2\n",
    "* Text Analysis with Python for Social Scienctists (Hovy, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# import spacy\n",
    "from scipy import spatial\n",
    "\n",
    "# For classifcation\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer  # does binary one-hot encoding if features are strings\n",
    "from sklearn.linear_model import LogisticRegression #, LogisticRegressionCV, SGDClassifier, Perceptron\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## 0. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and validation (dev) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Non-binary labels as these were mistaken labels identified early on that were meant to be excluded, and because only one token has this label, it prevents the data from being input into the models with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_train.loc[df_train.tag == \"B-Nonbinary\"]\n",
    "# df_train.loc[df_train.token_id == 230697]\n",
    "# # df_train.loc[df_train.tag == \"I-Nonbinary\"]\n",
    "# df_train.loc[df_train.token_id == 230697]\n",
    "# df_dev.loc[df_dev.tag == \"B-Nonbinary\"]  # No results\n",
    "# df_dev.loc[df_dev.tag == \"I-Nonbinary\"]  # No results\n",
    "df_train = df_train.loc[df_train.tag != \"B-Nonbinary\"]\n",
    "df_train = df_train.loc[df_train.tag != \"I-Nonbinary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467562, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not classifying all labels at once, consider only the rows with tags for the select subset of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_subset = [\"B-Stereotype\", \"I-Stereotype\", \"B-Omission\", \"I-Omission\", \"B-Occupation\", \"I-Occupation\"]\n",
    "# label_subset = [\"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \"B-Masculine\", \"I-Masculine\", \"B-Nonbinary\", \"I-Nonbinary\"]\n",
    "# label_subset = [\"B-Generalization\", \"I-Generalization\", \"B-Gendered-Role\", \"I-Gendered-Role\", \"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"]\n",
    "# df_train = df_train.loc[df_train.tag.isin(label_subset)]\n",
    "# df_dev = df_dev.loc[df_dev.tag.isin(label_subset)]\n",
    "# print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, lemmatize the tokens (a form of normalization, a.k.a. standardization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = list(df_train.token)\n",
    "lemmas_train = [lmtzr.lemmatize(token) for token in tokens_train]\n",
    "tokens_dev = list(df_dev.token)\n",
    "lemmas_dev = [lmtzr.lemmatize(token) for token in tokens_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.insert((list(df_train.columns).index(\"token\")+1), \"lemma\", lemmas_train)\n",
    "df_dev.insert((list(df_dev.columns).index(\"token\")+1), \"lemma\", lemmas_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>After</td>\n",
       "      <td>(907, 912)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>14379</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>his</td>\n",
       "      <td>(913, 916)</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>ordination</td>\n",
       "      <td>(917, 927)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>14380</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>(928, 930)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>spent</td>\n",
       "      <td>(931, 936)</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     description_id  sentence_id  ann_id  token_id       token       lemma  \\\n",
       "172               3            5   99999       154       After       After   \n",
       "173               3            5   14379       155         his         his   \n",
       "174               3            5   99999       156  ordination  ordination   \n",
       "175               3            5   14380       157          he          he   \n",
       "176               3            5   99999       158       spent       spent   \n",
       "\n",
       "    token_offsets   pos                 tag                      field subset  \n",
       "172    (907, 912)    IN                   O  Biographical / Historical    dev  \n",
       "173    (913, 916)  PRP$  B-Gendered-Pronoun  Biographical / Historical    dev  \n",
       "174    (917, 927)    NN                   O  Biographical / Historical    dev  \n",
       "175    (928, 930)   PRP  B-Gendered-Pronoun  Biographical / Historical    dev  \n",
       "176    (931, 936)   VBD                   O  Biographical / Historical    dev  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train.tail()\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarize and encode the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = DictVectorizer(sparse=True)\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels2numbers = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-Feminine': 0, 'B-Gendered-Pronoun': 1, 'B-Gendered-Role': 2, 'B-Generalization': 3, 'B-Masculine': 4, 'B-Occupation': 5, 'B-Omission': 6, 'B-Stereotype': 7, 'B-Unknown': 8, 'I-Feminine': 9, 'I-Gendered-Pronoun': 10, 'I-Gendered-Role': 11, 'I-Generalization': 12, 'I-Masculine': 13, 'I-Occupation': 14, 'I-Omission': 15, 'I-Stereotype': 16, 'I-Unknown': 17, 'O': 18}\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\"sentence_id\", \"token\"] #, \"lemma\"]\n",
    "target_col = \"tag\"\n",
    "labels = list(np.unique(df_train.tag))\n",
    "labels2numbers = LabelEncoder()\n",
    "y = labels2numbers.fit_transform(labels)\n",
    "label_to_no = dict(zip(labels,list(y)))\n",
    "no_to_label = dict(zip(list(y),labels))\n",
    "print(label_to_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467562, 35969)\n",
      "(467562,) (467562, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train[feature_cols]\n",
    "X_train = v.fit_transform(X_train.to_dict('records'))  # v.fit() - make sure column count same as X_dev!\n",
    "print(X_train.shape)\n",
    "\n",
    "y_train = df_train[target_col].values\n",
    "# Convert the string labels to numeric labels\n",
    "y_train_numeric = utils.getNumericLabels(y_train, label_to_no)\n",
    "# Convert each iterable of iterables above to a multilabel format\n",
    "y_train_binarized = mlb.fit_transform(y_train_numeric)\n",
    "print(y_train.shape, y_train_binarized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157740, 35969)\n",
      "(157740,) (157740, 19)\n"
     ]
    }
   ],
   "source": [
    "X_dev = df_dev[feature_cols]\n",
    "# X_dev = df_dev.drop(\"tag\", axis=1)\n",
    "X_dev = v.transform(X_dev.to_dict('records'))\n",
    "print(X_dev.shape)\n",
    "\n",
    "y_dev = df_dev[target_col].values\n",
    "# Convert the string labels to numeric labels\n",
    "y_dev_numeric = utils.getNumericLabels(y_dev, label_to_no)\n",
    "# Convert each iterable of iterables above to a multilabel format\n",
    "y_dev_binarized = mlb.transform(y_dev_numeric)\n",
    "print(y_dev.shape, y_dev_binarized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_dev.shape[1] == X_train.shape[1], \"The train and dev data must have the same number of columns.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = OneVsRestClassifier(LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\", random_state=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = log_reg.fit(X_train, y_train_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "predicted_dev = clf.predict(X_dev)\n",
    "print(predicted_dev[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>true_neg</th>\n",
       "      <th>false_neg</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>false_pos</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>157417</td>\n",
       "      <td>323</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>156996</td>\n",
       "      <td>744</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-Gendered-Role</td>\n",
       "      <td>157150</td>\n",
       "      <td>590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-Generalization</td>\n",
       "      <td>157495</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>156716</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>157085</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-Omission</td>\n",
       "      <td>156658</td>\n",
       "      <td>1082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-Stereotype</td>\n",
       "      <td>157481</td>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>155680</td>\n",
       "      <td>2060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>156894</td>\n",
       "      <td>846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-Gendered-Pronoun</td>\n",
       "      <td>157724</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-Gendered-Role</td>\n",
       "      <td>157606</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I-Generalization</td>\n",
       "      <td>157594</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>156362</td>\n",
       "      <td>1378</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>156959</td>\n",
       "      <td>781</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I-Omission</td>\n",
       "      <td>156012</td>\n",
       "      <td>1728</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I-Stereotype</td>\n",
       "      <td>156873</td>\n",
       "      <td>867</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>154234</td>\n",
       "      <td>3506</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141356</td>\n",
       "      <td>16384</td>\n",
       "      <td>0.896133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                labels  true_neg  false_neg  true_pos  false_pos  precision  \\\n",
       "0           B-Feminine    157417        323         0          0   0.000000   \n",
       "1   B-Gendered-Pronoun    156996        744         0          0   0.000000   \n",
       "2      B-Gendered-Role    157150        590         0          0   0.000000   \n",
       "3     B-Generalization    157495        245         0          0   0.000000   \n",
       "4          B-Masculine    156716       1024         0          0   0.000000   \n",
       "5         B-Occupation    157085        655         0          0   0.000000   \n",
       "6           B-Omission    156658       1082         0          0   0.000000   \n",
       "7         B-Stereotype    157481        259         0          0   0.000000   \n",
       "8            B-Unknown    155680       2060         0          0   0.000000   \n",
       "9           I-Feminine    156894        846         0          0   0.000000   \n",
       "10  I-Gendered-Pronoun    157724         16         0          0   0.000000   \n",
       "11     I-Gendered-Role    157606        134         0          0   0.000000   \n",
       "12    I-Generalization    157594        146         0          0   0.000000   \n",
       "13         I-Masculine    156362       1378         0          0   0.000000   \n",
       "14        I-Occupation    156959        781         0          0   0.000000   \n",
       "15          I-Omission    156012       1728         0          0   0.000000   \n",
       "16        I-Stereotype    156873        867         0          0   0.000000   \n",
       "17           I-Unknown    154234       3506         0          0   0.000000   \n",
       "18                   O         0          0    141356      16384   0.896133   \n",
       "\n",
       "    recall       f_1  \n",
       "0      0.0  0.000000  \n",
       "1      0.0  0.000000  \n",
       "2      0.0  0.000000  \n",
       "3      0.0  0.000000  \n",
       "4      0.0  0.000000  \n",
       "5      0.0  0.000000  \n",
       "6      0.0  0.000000  \n",
       "7      0.0  0.000000  \n",
       "8      0.0  0.000000  \n",
       "9      0.0  0.000000  \n",
       "10     0.0  0.000000  \n",
       "11     0.0  0.000000  \n",
       "12     0.0  0.000000  \n",
       "13     0.0  0.000000  \n",
       "14     0.0  0.000000  \n",
       "15     0.0  0.000000  \n",
       "16     0.0  0.000000  \n",
       "17     0.0  0.000000  \n",
       "18     1.0  0.945222  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_labels = mlb.classes_\n",
    "dev_matrix = multilabel_confusion_matrix(y_dev_binarized, predicted_dev, labels=mlb.classes_)\n",
    "df_dev_perf = utils.getPerformanceMetrics(y_dev_binarized, predicted_dev, dev_matrix, mlb.classes_, original_labels, no_to_label)\n",
    "df_dev_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Accuracy (all labels) on `token` col: 0.9890666186195806\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev Accuracy (all labels) on `token` col:\", np.mean(predicted_dev == y_dev_binarized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using cross-validation (stratified k fold, where k=3) with Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# k = 3 # number of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# log_reg_cv = OneVsRestClassifier(LogisticRegressionCV(\n",
    "#     solver=\"liblinear\", multi_class=\"ovr\", cv=k, scoring=\"f1\", random_state=22)  #max_iter=500, --> default is 100 iterations\n",
    "#                                 )\n",
    "# clf1 = log_reg_cv.fit(X_train, y_train_binarized)\n",
    "# pred1_dev = clf1.predict(X_dev)\n",
    "# # print(\"Dev Accuracy (all labels) on `lemma` col:\", np.mean(pred1_dev == y_dev))  # 90%\n",
    "# # print(\"Dev Accuracy (all labels) on `token` col:\", np.mean(pred1_dev == y_dev))  # 90%\n",
    "# # from sklearn import metrics\n",
    "# # metrics.SCORERS.keys()\n",
    "# # print(\"Accuracy:\", clf1.score(pred1_dev, y_dev_binarized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# original_labels = mlb.classes_\n",
    "# dev_matrix1 = multilabel_confusion_matrix(y_dev_binarized, pred1_dev, labels=mlb.classes_)\n",
    "# df_dev_perf1 = utils.getPerformanceMetrics(y_dev_binarized, pred1_dev, dev_matrix1, mlb.classes_, original_labels, no_to_label)\n",
    "# df_dev_perf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**QUESTION:** Are scores averaged across the 3 folds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a>\n",
    "## 1.1. With Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the vocabulary of the annotated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_dev])  # df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32957 31157 26757\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = list(set(list(df.token)))\n",
    "unique_lemmas = list(set(list(df.lemma))) \n",
    "unique_lemmas = [lemma for lemma in unique_lemmas if lemma.isalpha()]\n",
    "lemmas_lower = [lemma.lower() for lemma in unique_lemmas]\n",
    "unique_lemmas_lower = list(set(lemmas_lower))\n",
    "unique_words = [token for token in unique_tokens if token.isalpha()]  # keep tokens with only alphabetic characters\n",
    "print(len(unique_words), len(unique_lemmas), len(unique_lemmas_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [spaCy's contextual word embeddings](https://github.com/explosion/sense2vec), which were trained on 2015 Reddit posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sense2vec.component.Sense2VecComponent at 0x7f87677c5d30>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "s2v = nlp.add_pipe(\"sense2vec\")\n",
    "s2v.from_disk(config.s2v_reddit_path)\n",
    "\n",
    "#-------------\n",
    "# doc = nlp(\"A sentence about natural language processing.\")\n",
    "# assert doc[3:6].text == \"natural language processing\"\n",
    "# freq = doc[3:6]._.s2v_freq\n",
    "# vector = doc[3:6]._.s2v_vec\n",
    "# most_similar = doc[3:6]._.s2v_most_similar(3)\n",
    "# # [(('machine learning', 'NOUN'), 0.8986967),\n",
    "# #  (('computer vision', 'NOUN'), 0.8636297),\n",
    "# #  (('deep learning', 'NOUN'), 0.8573361)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cliffs', 'rpm', 'Heal', 'hereditary', 'Medjid', 'presumedly', 'Mondays', 'Routine', 'recipientESPMedawar', 'Kirkuk', 'Seton', 'Venado', 'Edith', 'Mackay', 'Visiting', 'interwar', 'atherogenic', 'Cawdor', 'jockey', 'Burgesses']\n"
     ]
    }
   ],
   "source": [
    "print(unique_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary not in Sense2Vec: 11529\n",
      "Proportion of vocabulary not in Sense2Vec: 0.40388859695218077\n"
     ]
    }
   ],
   "source": [
    "not_in_s2v = []\n",
    "for word in unique_words:\n",
    "    lowercased = word.lower()\n",
    "    w = (nlp(lowercased))[0]\n",
    "    if w._.s2v_vec is None:\n",
    "        not_in_s2v += [word]\n",
    "\n",
    "print(\"Total words in vocabulary not in Sense2Vec:\", len(not_in_s2v))\n",
    "print(\"Proportion of vocabulary not in Sense2Vec:\",(len(not_in_s2v))/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recipient', 'squabs', 'Darby', 'poulterer', 'Scotsman', 'sepolero', 'Morphogenetic', 'Hynes', 'Repleta', 'Simal', 'Mai', 'Arithmetic', 'Duce', 'Mme', 'lectureKatchalsky', 'inA', 'Tulsk', 'Berg', 'Mode', 'Bohme', 'Envelope', 'furnitureKoestler', 'Burmester', 'Evang', 'Ilona', 'compagne', 'BarnArthur', 'Cant', 'GollyArthur', 'Hatano', 'ElectionsAccompanied', 'accomodement', 'Finney', 'poetarum', 'Realites', 'Margaropus', 'LI', 'Lennox', 'Basberg', 'Ignacio', 'Glennie', 'Duffus', 'Takagi', 'Aliza', 'emir', 'Hersham', 'Rossini', 'Bald', 'Magnus', 'Pattison', 'Skefhill', 'Sorrel', 'Landolphin', 'Staub', 'ME', 'Alumbadi', 'Model', 'Majesties', 'Harian', 'trichocysts', 'Wandervogel', 'Rumped', 'Waetjen', 'Verbena', 'ofThe', 'sturzte', 'Lee', 'Copernicus', 'Verasis', 'Altenberg', 'unnumbered', 'environs', 'Neutral', 'Wynne', 'Mossman', 'ReneeESPCutten', 'ReadingSent', 'sequelae', 'Calary', 'Soviets', 'AustriaKoestler', 'Karlsburg', 'Peckham', 'Macdougall', 'Goldschmidt', 'electroencephalogram', 'neckHewitt', 'Simulans', 'Rothwell', 'prayeel', 'prickly', 'Dusen', 'Montagu', 'Prince', 'JournalSent', 'Microsatellite', 'Caius', 'Bubalus', 'Tyttenhanger', 'Conurbations']\n"
     ]
    }
   ],
   "source": [
    "# print(not_in_s2v[:100])\n",
    "# print(not_in_s2v[1000:1100])\n",
    "print(not_in_s2v[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2659\n"
     ]
    }
   ],
   "source": [
    "# x = \"lettersBaillie\"\n",
    "# y = \"writer'\"\n",
    "# z = \"MunsterSent\"\n",
    "newly_found = 0\n",
    "for word in not_in_s2v:\n",
    "    found = re.findall(\"[A-Z]{0,1}[a-z]+\", word)\n",
    "    for f in found:\n",
    "        lowercased = f.lower()\n",
    "        w = nlp(lowercased)[0]\n",
    "        if not w._.s2v_vec is None:\n",
    "            newly_found += 1\n",
    "print(newly_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion not found but possible to find: 0.23063578801283718\n",
      "Proportion of vocabulary not possible to find in Sense2Vec: 0.3107374321247154\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion not found but possible to find:\", newly_found/(len(not_in_s2v)))\n",
    "print(\"Proportion of vocabulary not possible to find in Sense2Vec:\",(len(not_in_s2v)-newly_found)/len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure it's worth reworking the tokenization and part-of-speech tagging to increase Sense2Vec's coverage of the vocabulary by only about 9%, so we'll keep going with the model input data as it is for now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [GloVe word embeddings](https://github.com/stanfordnlp/GloVe), which were trained on 2014 English Wikipedia entries and Gigaword 5:\n",
    "\n",
    "*Note: could also try [GN-GloVe](https://github.com/uclanlp/gn_glove), which supposedly has gender-neutral word embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\n",
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]  # pretrained GloVe embeddings come as vectors with one of these four dimensions\n",
    "d = dimensions[0]  # start small to begin with\n",
    "glove_path = config.inf_data_path+\"glove.6B/glove.6B.{}d.txt\".format(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13412    1.5041     0.39706   -0.41357    0.71336    0.63438\n",
      "  0.1214     0.16746    1.4104     0.013127   0.68386    0.85236\n",
      "  0.92599    0.098158   0.425     -0.83799    0.08482    0.22135\n",
      " -0.15247    0.72654    0.052728  -1.0064    -0.032626  -0.63342\n",
      "  0.0039789 -1.5288    -0.74005   -1.2051    -1.0227    -0.10588\n",
      "  1.2225     0.14845   -0.1641    -0.52887   -0.29012    0.59774\n",
      "  0.62847    0.49003   -0.14227   -1.2193     0.56094   -0.17673\n",
      " -0.11216   -0.41801   -0.40841   -0.41748   -0.40276    0.25091\n",
      " -0.43016    0.26412  ]\n"
     ]
    }
   ],
   "source": [
    "glove = dict()\n",
    "with open(glove_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        glove[word] = vector\n",
    "print(glove[\"recipient\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate GloVe's coverage of our vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary not in GloVe: 6453\n",
      "Proportion of vocabulary not in GloVe: 0.1958005886458112\n"
     ]
    }
   ],
   "source": [
    "not_in_glove = []\n",
    "in_glove = dict()\n",
    "for word in unique_words:\n",
    "    lowercased = word.lower()\n",
    "    try:\n",
    "        vector = glove[lowercased]\n",
    "        in_glove[word] = vector\n",
    "    except KeyError:\n",
    "        not_in_glove += [word]\n",
    "\n",
    "print(\"Total words in vocabulary not in GloVe:\", len(not_in_glove))\n",
    "print(\"Proportion of vocabulary not in GloVe:\",(len(not_in_glove))/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not possible to find in GloVe: 4187\n",
      "Proportion not found but possible to find in GloVe: 0.7311328064466139\n",
      "Proportion of vocabulary not possible to find in GloVe: 0.12704433049124617\n"
     ]
    }
   ],
   "source": [
    "still_not_found = []\n",
    "newly_found = 0\n",
    "# partial_glove_match = dict()\n",
    "for word in not_in_glove:\n",
    "    found = re.findall(\"[A-Z]{0,1}[a-z]+\", word)\n",
    "    for f in found:\n",
    "        lowercased = f.lower()\n",
    "        try:\n",
    "            vector = glove[lowercased]\n",
    "            in_glove[word] = vector  # partial_glove_match[word] = vector\n",
    "            newly_found += 1\n",
    "        except KeyError:\n",
    "            still_not_found += [word]\n",
    "print(\"Not possible to find in GloVe:\", len(still_not_found))\n",
    "print(\"Proportion not found but possible to find in GloVe:\", newly_found/(len(not_in_glove)))\n",
    "print(\"Proportion of vocabulary not possible to find in GloVe:\",(len(still_not_found))/len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe has much better coverage than sense2vec, as expected due to the better domain match (Wikipedia entries are more similar to archival metadata descriptions than Reddit comments!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_vectors = in_glove.copy()\n",
    "key_array = np.array(words_to_vectors.keys())\n",
    "for word in unique_words:\n",
    "    if word not in key_array:\n",
    "        if word.lower() in key_array:\n",
    "            vector = words_to_vectors[word.lower()]\n",
    "        else:\n",
    "            vector = np.array([])\n",
    "        words_to_vectors[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(words_to_vectors) == len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token   lemma  \\\n",
       "3               1            1   99999         3   Title   Title   \n",
       "4               1            1   99999         4       :       :   \n",
       "5               1            1   99999         5  Papers  Papers   \n",
       "6               1            1   99999         6      of      of   \n",
       "7               1            1   14384         7     The     The   \n",
       "\n",
       "  token_offsets  pos        tag  field subset  \n",
       "3      (17, 22)   NN          O  Title  train  \n",
       "4      (22, 23)    :          O  Title  train  \n",
       "5      (24, 30)  NNS          O  Title  train  \n",
       "6      (31, 33)   IN          O  Title  train  \n",
       "7      (34, 37)   DT  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# # Model that assigns a GloVe embedding to a token.\n",
    "# class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "    \n",
    "#     # Transform tokens from the input data to their corresponding GloVe embedding\n",
    "#     def transform(self, tokens):\n",
    "#         return [words_to_vectors[token] for token in tokens]\n",
    "class EmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, embedding_dict=words_to_vectors):\n",
    "        tokens = list(self.token)\n",
    "        embeddings = np.array([])\n",
    "        for token in tokens:\n",
    "            embedding = embedding_dict[token]\n",
    "            embeddings += [embedding]\n",
    "        return embeddings\n",
    "\n",
    "# embedding_transformer = FunctionTransformer(embedding_extractor)\n",
    "\n",
    "class PosTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self):\n",
    "        # STOPPED HERE\n",
    "        # use multilabel binarizer?\n",
    "        # REFERENCE: https://stackoverflow.com/questions/56774862/how-to-add-a-feature-using-a-pipeline-and-featureunion\n",
    "        \n",
    "        return #np.array(self.pos)\n",
    "\n",
    "# pos_transformer = FunctionTransformer(pos_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = OneVsRestClassifier(LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\", random_state=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_union = FeatureUnion([(\"embeddings\", EmbeddingTransformer), (\"pos\", PosTransformer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"features\", feature_union), (\"classifier\", log_reg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "fit not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-06b6beeca3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_binarized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0msum\u001b[0m \u001b[0mof\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mover\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;31m# All transformers are None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_parallel_func\u001b[0;34m(self, X, y, fit_params, func)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mtransformers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return Parallel(n_jobs=self.n_jobs)(delayed(func)(\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FeatureUnion'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    256\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    256\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: fit not found"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformer = ColumnTransformer([\"vectorizer\", GloveVectorizer(), 'token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = FeatureUnion([\"vectorizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ColumnTransformer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-f6bbd88d19da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     [\"vectorizer\", GloveVectorizer()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m pipe = Pipeline(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"transformer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_transformer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"estimator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps, memory, verbose)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# validate names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ColumnTransformer' object is not iterable"
     ]
    }
   ],
   "source": [
    "# vectorizer = Pipeline(\n",
    "#     [\"col_selector\", FunctionTransformer(lambda df: df[\"token\"])],\n",
    "#     [\"vectorizer\", GloveVectorizer()]\n",
    "# )\n",
    "pipe = Pipeline(\n",
    "    [\"transformer\", col_transformer],\n",
    "    [\"estimator\", log_reg]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformer = ColumnTransformer([\"vectorizer\", vectorizer, 'token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ColumnTransformer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f84041e03494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pipe = Pipeline(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"transformer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_transformer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"clf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps, memory, verbose)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# validate names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ColumnTransformer' object is not iterable"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "    [\"transformer\", col_transformer],\n",
    "    [\"clf\", log_reg]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

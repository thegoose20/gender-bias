{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1, Model 1\n",
    "\n",
    "#### Model Setup\n",
    "\n",
    "Run models in the following order, using their output labels as features for the next model:\n",
    "\n",
    "1. Multilabel Linguistic Classifier\n",
    "2. Multiclass Person Name + Occupation Sequence Classifier\n",
    "3. Multilabel StereDocument Classifier\n",
    "\n",
    "Train the first model and then run it over the entire dataset.\n",
    "\n",
    "***\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/experiment_input/`\n",
    "    * Prediction Data: Data: under directory `../data/token_clf_data/model_output/experiment1/`\n",
    "* Word Embeddings\n",
    "    * Custom fastText (word2vec with subwords) embeddings of 100 dimensions trained on the CRC Archives catalog's descriptive metadata (harvested October 2020)\n",
    "    \n",
    "***\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I.](#i) Train the Linguistic Classifier\n",
    "\n",
    "[II.](#ii) Predict Over All Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load programming resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, utils1, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For multilabel token classification\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define resources for the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(config.experiment_input_path).mkdir(parents=True, exist_ok=True)    # For train, devtest, and blind test data\n",
    "predictions_dir = config.experiment1_output_path+\"5fold/\"\n",
    "Path(predictions_dir).mkdir(parents=True, exist_ok=True)  # For predictions\n",
    "agreement_dir = config.experiment1_agmt_path+\"5fold/\"\n",
    "Path(agreement_dir).mkdir(parents=True, exist_ok=True)    # For agreement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1:\n",
    "ling_label_subset = [\"B-Generalization\", \"I-Generalization\", \"B-Gendered-Role\", \"I-Gendered-Role\", \"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"]\n",
    "# Model 2:\n",
    "pers_o_label_subset = [\"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \"B-Masculine\", \"I-Masculine\", \"B-Occupation\", \"I-Occupation\"]\n",
    "# Model 3:\n",
    "so_label_subset = [\"B-Stereotype\", \"I-Stereotype\", \"B-Omission\", \"I-Omission\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_label_tags = {\n",
    "    \"Gendered-Pronoun\": [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"], \"Gendered-Role\": [\"B-Gendered-Role\", \"I-Gendered-Role\"],\"Generalization\": [\"B-Generalization\", \"I-Generalization\"]\n",
    "    }\n",
    "pers_o_label_tags = {\n",
    "    \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "     \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "    }\n",
    "so_label_tags = {\n",
    "    \"Stereotype\": [\"B-Stereotype\", \"I-Stereotype\"], \"Omission\": [\"B-Omission\", \"I-Omission\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100  # dimensions of word embeddings (should match utils1.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"i\"></a>\n",
    "## I. Train the Linguistic Classifier\n",
    "\n",
    "Run a multilabel classifier on the train set of the data, focusing only on applying the Linguistic category of labels: Gendered Pronoun, Gendered Role, and Generalization.\n",
    "\n",
    "Use a Classifier Chain with Random Forest, as this was the highest-performing multilabel model setup from previous algorithm experiments for the Linguistic labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For this experiment, we'll train the model on 40% of the data, rather than 60%.\n",
    "We'll use fastText embeddings of 100 dimensions, as was used in the model that achieved the above scores.\n",
    "'''\n",
    "# train_df = pd.read_csv(config.tokc_path+\"experiment_input/token_train.csv\", index_col=0)\n",
    "# dev_df = pd.read_csv(config.tokc_path+\"experiment_input/token_validate.csv\", index_col=0)\n",
    "# test_df = pd.read_csv(config.tokc_path+\"experiment_input/token_test.csv\", index_col=0)\n",
    "# ling_train = utils1.selectDataForLabels(train_df, \"tag\", ling_label_subset)\n",
    "# ling_dev = utils1.selectDataForLabels(dev_df, \"tag\", ling_label_subset)\n",
    "# ling_test = utils1.selectDataForLabels(test_df, \"tag\", ling_label_subset)\n",
    "# train_data = utils1.loadData(ling_train)\n",
    "# dev_data = utils1.loadData(ling_dev)\n",
    "# test_data = utils1.loadData(ling_test)\n",
    "# assert train_data.shape[0] == len(train_data.token_id.unique())\n",
    "# assert dev_data.shape[0] == len(dev_data.token_id.unique())\n",
    "# assert test_data.shape[0] == len(test_data.token_id.unique())\n",
    "# print(train_data.shape, dev_data.shape, test_data.shape)\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, we'll repeatedly train models on different 80% selections of data and predict on the remaining 20% split, for a modified 5-fold cross-validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>split2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>split2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id       token token_offsets  \\\n",
       "0               0            0   99999         0  Identifier       (0, 10)   \n",
       "1               0            0   99999         1           :      (10, 11)   \n",
       "2               0            0   99999         2         AA5      (12, 15)   \n",
       "3               1            1   99999         3       Title      (17, 22)   \n",
       "4               1            1   99999         4           :      (22, 23)   \n",
       "\n",
       "  pos tag       field    fold  \n",
       "0  NN   O  Identifier  split4  \n",
       "1   :   O  Identifier  split4  \n",
       "2  NN   O  Identifier  split4  \n",
       "3  NN   O       Title  split2  \n",
       "4   :   O       Title  split2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(config.tokc_path+\"experiment_input/token_5fold.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the five groups of training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['split0' 'split1' 'split2' 'split3' 'split4']\n"
     ]
    }
   ],
   "source": [
    "split_col = \"fold\"\n",
    "splits = df[split_col].unique()\n",
    "splits.sort()\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0, test0 = list(splits[:4]), splits[4]\n",
    "train1, test1 = list(splits[1:]), splits[0]\n",
    "train2, test2 = list(splits[2:])+[splits[0]], splits[1]\n",
    "train3, test3 = list(splits[3:])+list(splits[:2]), splits[2]\n",
    "train4, test4 = [splits[4]]+list(splits[:3]), splits[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['split0', 'split1', 'split2', 'split3'], 'split4')\n",
      "(['split1', 'split2', 'split3', 'split4'], 'split0')\n",
      "(['split2', 'split3', 'split4', 'split0'], 'split1')\n",
      "(['split3', 'split4', 'split0', 'split1'], 'split2')\n",
      "(['split4', 'split0', 'split1', 'split2'], 'split3')\n"
     ]
    }
   ],
   "source": [
    "runs = [(train0, test0), (train1, test1), (train2, test2), (train3, test3), (train4, test4)]\n",
    "for run in runs:\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Create feature matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_tokens = utils1.zipTokensFeatures(train_data)\n",
    "# dev_tokens = utils1.zipTokensFeatures(dev_data)\n",
    "# all_tokens = utils1.zipTokensFeatures(all_data)\n",
    "# X_train = utils1.makeFastTextFeatureMatrix(train_tokens)\n",
    "# X_dev = utils1.makeFastTextFeatureMatrix(dev_tokens)\n",
    "# X_all = utils1.makeFastTextFeatureMatrix(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Binarize targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# mlb, y_train = utils1.binarizeTrainTargets(train_data)\n",
    "# y_dev = utils1.binarizeDevTargets(mlb, dev_data)\n",
    "# y_all = utils1.binarizeDevTargets(mlb, all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: ['split0', 'split1', 'split2', 'split3']\n",
      "Predicting on: split4\n",
      "Training on: ['split1', 'split2', 'split3', 'split4']\n",
      "Predicting on: split0\n",
      "Training on: ['split2', 'split3', 'split4', 'split0']\n",
      "Predicting on: split1\n",
      "Training on: ['split3', 'split4', 'split0', 'split1']\n",
      "Predicting on: split2\n",
      "Training on: ['split4', 'split0', 'split1', 'split2']\n",
      "Predicting on: split3\n",
      "Modified 5-fold cross-validation complete!\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "for run in runs:\n",
    "    # Get the train (80%) and test (20%) subsets of data\n",
    "    train_splits, test_split = run[0], run[1]\n",
    "    print(\"Training on:\", train_splits)\n",
    "    train_df = df.loc[df[split_col].isin(train_splits)]\n",
    "    dev_df = df.loc[df[split_col] == test_split]\n",
    "    \n",
    "    # Make sure only Linguistic tags are considered\n",
    "    ling_train = utils1.selectDataForLabels(train_df, \"tag\", ling_label_subset)\n",
    "    ling_dev = utils1.selectDataForLabels(dev_df, \"tag\", ling_label_subset)\n",
    "    ling_train = ling_train.rename(columns={\"fold\":\"subset\"})  # Change column name to next function's expected column name\n",
    "    ling_dev = ling_dev.rename(columns={\"fold\":\"subset\"})      # Change column name to next function's expected column name\n",
    "    train_data = utils1.loadData(ling_train)\n",
    "    dev_data = utils1.loadData(ling_dev)\n",
    "    \n",
    "    # Create feature matrices\n",
    "    train_tokens = utils1.zipTokensFeatures(train_data)\n",
    "    dev_tokens = utils1.zipTokensFeatures(dev_data)\n",
    "    X_train = utils1.makeFastTextFeatureMatrix(train_tokens)\n",
    "    X_dev = utils1.makeFastTextFeatureMatrix(dev_tokens)\n",
    "    \n",
    "    # Binarize targets\n",
    "    mlb, y_train = utils1.binarizeTrainTargets(train_data)\n",
    "    y_dev = utils1.binarizeDevTargets(mlb, dev_data)\n",
    "\n",
    "    # Train a classification model\n",
    "    a = \"rf\"\n",
    "    clf = ClassifierChain(\n",
    "        classifier = RandomForestClassifier(random_state=22),\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict with the trained model\n",
    "    print(\"Predicting on:\", test_split)\n",
    "    predictions = clf.predict(X_dev)\n",
    "    if pred_df.shape[0] > 0:\n",
    "        next_pred_df = utils.makePredictionDF(predictions, dev_data, \"tag\", \"predicted_tag\", \"O\", mlb)\n",
    "        pred_df = pd.concat([pred_df, next_pred_df])\n",
    "    else:\n",
    "        pred_df = utils.makePredictionDF(predictions, dev_data, \"tag\", \"predicted_tag\", \"O\", mlb)\n",
    "\n",
    "assert pred_df.loc[pred_df.predicted_tag.isna()].shape[0] == 0, \"Any NaN values should be replaced with 'O'\"\n",
    "print(\"Modified 5-fold cross-validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated time to complete: 2.5 hours (~30 min. per model)\n",
    "# Estimated finish time: 12:30 PM\n",
    "# actual time: about 1.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=RandomForestClassifier(random_state=22),\n",
       "                require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=RandomForestClassifier(random_state=22),\n",
       "                require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=22)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=22)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ClassifierChain(classifier=RandomForestClassifier(random_state=22),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = \"rf\"\n",
    "# clf = ClassifierChain(\n",
    "#     classifier = RandomForestClassifier(random_state=22),\n",
    "# )\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# predictions = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Strict, All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - macro: 0.46238272522632073\n",
      "Recall - macro: 0.42298863525280694\n",
      "F1 Score - macro: 0.42251820432443793\n",
      "Accuracy - normalized: 0.9925798564349315\n",
      "Accuracy - unnormalized: 303654\n"
     ]
    }
   ],
   "source": [
    "# print(\"Precision - macro:\", sklearn.metrics.precision_score(y_dev, predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "# print(\"Recall - macro:\", sklearn.metrics.recall_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "# print(\"F1 Score - macro:\", sklearn.metrics.f1_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "# print(\"Accuracy - normalized:\", sklearn.metrics.accuracy_score(y_dev, predictions, normalize=True))  # fraction of correctly classified samples\n",
    "# print(\"Accuracy - unnormalized:\", sklearn.metrics.accuracy_score(y_dev, predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 305924\n"
     ]
    }
   ],
   "source": [
    "# print(\"Total samples:\", X_dev.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>predicted_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id   token  pos predicted_tag\n",
       "0            1         3   Title   NN             O\n",
       "1            1         4       :    :             O\n",
       "2            1         5  Papers  NNS             O\n",
       "3            1         6      of   IN             O\n",
       "4            1         7     The   DT             O"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_df = utils.makePredictionDF(predictions, dev_data, \"tag\", \"predicted_tag\", \"O\", mlb)\n",
    "# assert pred_df.loc[pred_df.predicted_tag.isna()].shape[0] == 0, \"Any NaN values should be replaced with 'O'\"\n",
    "# pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755932 753521\n"
     ]
    }
   ],
   "source": [
    "print(pred_df.shape[0], len(pred_df.token_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(predictions_dir+\"cc-{a}_ling_baseline_fastText{d}_predictions.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more predictions than unique tokens, because with multilabel classification, one token can have multiple predicted tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos        expected_tag\n",
       "0            5       154       After    IN                   O\n",
       "1            5       155         his  PRP$  B-Gendered-Pronoun\n",
       "2            5       156  ordination    NN                   O\n",
       "3            5       157          he   PRP  B-Gendered-Pronoun\n",
       "4            5       158       spent   VBD                   O"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_df = dev_data.explode([\"tag\"])\n",
    "exp_df = exp_df.rename(columns={\"tag\":\"expected_tag\"})\n",
    "exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153237 152455\n"
     ]
    }
   ],
   "source": [
    "print(exp_df.shape[0], len(exp_df.token_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160144, 7)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_pred_df = pd.merge(\n",
    "    left=exp_df, \n",
    "    right=pred_df.loc[pred_df.predicted_tag != \"O\"], # only include the predictions of Linguistic labels\n",
    "    how=\"outer\",\n",
    "    left_on=[\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_tag\"],\n",
    "    right_on=[\"sentence_id\", \"token_id\", \"token\", \"pos\", \"predicted_tag\"],\n",
    "    suffixes=[\"\", \"_pred\"],\n",
    "    indicator=True\n",
    ")\n",
    "exp_pred_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreement type for each row, ignoring rows with `'O'` and `NaN` value pairs (the `true negative` agreement type, which doesn't go into the precision, recall, or F1 score calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_col = \"expected_tag\"\n",
    "pred_col = \"predicted_tag\"\n",
    "no_tag_value = \"O\"\n",
    "# Find true negatives based on the expected and predicted tags\n",
    "sub_exp_pred_df = exp_pred_df.loc[exp_pred_df[exp_col] == no_tag_value]\n",
    "sub_exp_pred_df = sub_exp_pred_df.loc[sub_exp_pred_df[pred_col].isna()]\n",
    "# sub_exp_pred_df.replace(to_replace=\"left_only\", value=\"true negative\", inplace=True)\n",
    "tn_tokens = list(sub_exp_pred_df[\"token_id\"])\n",
    "\n",
    "# Record false negatives, false positives, and true positives based on the merge values\n",
    "eval_df = exp_pred_df.loc[~exp_pred_df[\"token_id\"].isin(tn_tokens)]\n",
    "eval_df = eval_df.replace(to_replace=\"left_only\", value=\"false negative\")\n",
    "eval_df = eval_df.replace(to_replace=\"right_only\", value=\"false positive\")\n",
    "eval_df = eval_df.replace(to_replace=\"both\", value=\"true positive\")\n",
    "eval_df = eval_df.sort_index()\n",
    "# eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7607, 7)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(agreement_dir+\"cc-{a}_ling_baseline_fastText{d}_evaluation.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strict Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the true positives, false positives, false negatives, precision, recall, and F1 metrics for all tags and each tag individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>237</td>\n",
       "      <td>6529</td>\n",
       "      <td>841</td>\n",
       "      <td>0.457052</td>\n",
       "      <td>0.433591</td>\n",
       "      <td>0.431610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Generalization</td>\n",
       "      <td>55</td>\n",
       "      <td>341</td>\n",
       "      <td>80</td>\n",
       "      <td>0.190024</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.287770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Generalization</td>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Gendered-Role</td>\n",
       "      <td>51</td>\n",
       "      <td>2433</td>\n",
       "      <td>228</td>\n",
       "      <td>0.085682</td>\n",
       "      <td>0.817204</td>\n",
       "      <td>0.155102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Gendered-Role</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>9</td>\n",
       "      <td>3731</td>\n",
       "      <td>1374</td>\n",
       "      <td>0.269148</td>\n",
       "      <td>0.993492</td>\n",
       "      <td>0.423551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Gendered-Pronoun</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag(s)  false negative  false positive  true positive  \\\n",
       "0                 all             237            6529            841   \n",
       "0    B-Generalization              55             341             80   \n",
       "0    I-Generalization              43              24              0   \n",
       "0     B-Gendered-Role              51            2433            228   \n",
       "0     I-Gendered-Role              65               0              0   \n",
       "0  B-Gendered-Pronoun               9            3731           1374   \n",
       "0  I-Gendered-Pronoun              14               0              0   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.457052  0.433591  0.431610  \n",
       "0   0.190024  0.592593  0.287770  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.085682  0.817204  0.155102  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.269148  0.993492  0.423551  \n",
       "0   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agmt_stats = utils.getAgreementStatsForAllTags(eval_df, \"_merge\", \"token_id\", \"tag(s)\", y_dev, predictions)\n",
    "for label_tag in ling_label_subset:\n",
    "    label_agmt_stats = utils.getScoresByTags(eval_df, \"_merge\", [label_tag])\n",
    "    agmt_stats = pd.concat([agmt_stats, label_agmt_stats])\n",
    "agmt_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_stats.to_csv(agreement_dir+\"cc-{a}_baseline_fastText{d}_ling_strict_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Annotation-level Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the manual annotations' offsets to the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv(config.agg_path+\"aggregated_final.csv\")#, usecols=[\"description_id\",\"agg_ann_id\", \"ann_offsets\"])\n",
    "# Get only the Linguistic annotations\n",
    "annot_df = annot_df.loc[annot_df.category == \"Linguistic\"]\n",
    "annot_df = annot_df[[\"agg_ann_id\", \"ann_offsets\", \"label\"]]\n",
    "annot_df = annot_df.rename(columns={\"agg_ann_id\":\"ann_id\"})\n",
    "# annot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos                   tag\n",
       "0            5       154       After    IN                   [O]\n",
       "1            5       155         his  PRP$  [B-Gendered-Pronoun]\n",
       "2            5       156  ordination    NN                   [O]\n",
       "3            5       157          he   PRP  [B-Gendered-Pronoun]\n",
       "4            5       158       spent   VBD                   [O]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10835, 9)\n",
      "(10835, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>predicted_tag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>ann_offsets</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14379.0</td>\n",
       "      <td>(913, 916)</td>\n",
       "      <td>(913, 916)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14380.0</td>\n",
       "      <td>(928, 930)</td>\n",
       "      <td>(928, 930)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152.0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>668</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>9523.0</td>\n",
       "      <td>(1982, 1984)</td>\n",
       "      <td>(1982, 1984)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158.0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>674</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>9524.0</td>\n",
       "      <td>(2013, 2015)</td>\n",
       "      <td>(2013, 2015)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220.0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>756</td>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>9525.0</td>\n",
       "      <td>(2471, 2473)</td>\n",
       "      <td>(2471, 2473)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id  token_id token   pos        expected_tag  \\\n",
       "1.0            5.0       155   his  PRP$  B-Gendered-Pronoun   \n",
       "3.0            5.0       157    he   PRP  B-Gendered-Pronoun   \n",
       "152.0         24.0       668    he   PRP  B-Gendered-Pronoun   \n",
       "158.0         24.0       674    he   PRP  B-Gendered-Pronoun   \n",
       "220.0         28.0       756    He   PRP  B-Gendered-Pronoun   \n",
       "\n",
       "            predicted_tag         _merge   ann_id token_offsets   ann_offsets  \\\n",
       "1.0    B-Gendered-Pronoun  true positive  14379.0    (913, 916)    (913, 916)   \n",
       "3.0    B-Gendered-Pronoun  true positive  14380.0    (928, 930)    (928, 930)   \n",
       "152.0  B-Gendered-Pronoun  true positive   9523.0  (1982, 1984)  (1982, 1984)   \n",
       "158.0  B-Gendered-Pronoun  true positive   9524.0  (2013, 2015)  (2013, 2015)   \n",
       "220.0  B-Gendered-Pronoun  true positive   9525.0  (2471, 2473)  (2471, 2473)   \n",
       "\n",
       "         expected_label  \n",
       "1.0    Gendered-Pronoun  \n",
       "3.0    Gendered-Pronoun  \n",
       "152.0  Gendered-Pronoun  \n",
       "158.0  Gendered-Pronoun  \n",
       "220.0  Gendered-Pronoun  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_add = df[[\"ann_id\", \"token_id\", \"token_offsets\"]]\n",
    "# Only include annotations with Linguistic labels\n",
    "to_add = to_add.loc[to_add.ann_id.isin(list(annot_df.ann_id))]\n",
    "eval_df_joined = eval_df.join(to_add.set_index(\"token_id\"), on=\"token_id\", how=\"outer\")\n",
    "# Join on the left, as there will be annotations from outside the devtest set in annot_df\n",
    "print(eval_df_joined.shape)\n",
    "eval_df_joined = eval_df_joined.join(annot_df.set_index(\"ann_id\"), on=\"ann_id\", how=\"left\")\n",
    "print(eval_df_joined.shape)  # Looks good!  Same as before join.\n",
    "eval_df_joined = eval_df_joined.rename(columns={\"label\":\"expected_label\"})\n",
    "eval_df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the predicted tags with their corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_joined.expected_label = eval_df_joined.expected_label.fillna(\"no_label\")\n",
    "eval_df_joined.expected_tag = eval_df_joined.expected_tag.fillna(\"no_label\")\n",
    "eval_df_joined.predicted_tag = eval_df_joined.predicted_tag.fillna(\"no_label\")\n",
    "# eval_df_joined.predicted_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = list(eval_df_joined.predicted_tag)\n",
    "predicted_labels = [tag[2:] if tag != \"no_label\" else tag for tag in predicted_labels]\n",
    "eval_df_joined.insert(len(eval_df_joined.columns), \"predicted_label\", predicted_labels)\n",
    "# eval_df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"sentence_id\", \"token_id\", \"token\", \"expected_label\", \"predicted_label\", \"_merge\", \"token_offsets\", \"ann_offsets\", \"ann_id\"]\n",
    "eval_by_ann = utils.implodeDataFrame(eval_df_joined[cols_to_keep], [\"ann_id\", \"ann_offsets\"]).reset_index()\n",
    "exp_labels = list(eval_by_ann[\"expected_label\"])\n",
    "exp_labels = [labels[0] for labels in exp_labels]\n",
    "eval_by_ann[\"expected_label\"] = exp_labels\n",
    "# eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert eval_by_ann.loc[eval_by_ann.expected_label == \"no_label\"].shape[0] == 0\n",
    "assert eval_by_ann.loc[eval_by_ann.expected_label.isna()].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row should have an annotation label (a Linguistic label in `expected_label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>ann_offsets</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>expected_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>_merge</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>annotation_agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>(1407, 1415)</td>\n",
       "      <td>[5760.0, 5760.0]</td>\n",
       "      <td>[133674, 133674]</td>\n",
       "      <td>[knighted, knighted]</td>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>[no_label, Generalization]</td>\n",
       "      <td>[false negative, false positive]</td>\n",
       "      <td>[(1407, 1415), (1407, 1415)]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(9625, 9635)</td>\n",
       "      <td>[10365.0, nan]</td>\n",
       "      <td>[228678, 228679]</td>\n",
       "      <td>[knighthood, nan]</td>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>[Gendered-Role, no_label]</td>\n",
       "      <td>[false positive, nan]</td>\n",
       "      <td>[(9625, 9635), (9635, 9636)]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>(2426, 2439)</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[196525, 196526, 196527]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>[no_label, no_label, no_label]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[(2426, 2432), (2433, 2439), (2439, 2440)]</td>\n",
       "      <td>false negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>(9993, 10003)</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[236354, 236355]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>[no_label, no_label]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[(9993, 10003), (10003, 10004)]</td>\n",
       "      <td>false negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>(7192, 7195)</td>\n",
       "      <td>[10763.0]</td>\n",
       "      <td>[239212]</td>\n",
       "      <td>[Sir]</td>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>[Gendered-Role]</td>\n",
       "      <td>[false positive]</td>\n",
       "      <td>[(7192, 7195)]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id    ann_offsets       sentence_id                  token_id  \\\n",
       "0     0.0   (1407, 1415)  [5760.0, 5760.0]          [133674, 133674]   \n",
       "1     1.0   (9625, 9635)    [10365.0, nan]          [228678, 228679]   \n",
       "2     2.0   (2426, 2439)   [nan, nan, nan]  [196525, 196526, 196527]   \n",
       "3     3.0  (9993, 10003)        [nan, nan]          [236354, 236355]   \n",
       "4     4.0   (7192, 7195)         [10763.0]                  [239212]   \n",
       "\n",
       "                  token expected_label                 predicted_label  \\\n",
       "0  [knighted, knighted]  Gendered-Role      [no_label, Generalization]   \n",
       "1     [knighthood, nan]  Gendered-Role       [Gendered-Role, no_label]   \n",
       "2       [nan, nan, nan]  Gendered-Role  [no_label, no_label, no_label]   \n",
       "3            [nan, nan]  Gendered-Role            [no_label, no_label]   \n",
       "4                 [Sir]  Gendered-Role                 [Gendered-Role]   \n",
       "\n",
       "                             _merge  \\\n",
       "0  [false negative, false positive]   \n",
       "1             [false positive, nan]   \n",
       "2                   [nan, nan, nan]   \n",
       "3                        [nan, nan]   \n",
       "4                  [false positive]   \n",
       "\n",
       "                                token_offsets annotation_agreement  \n",
       "0                [(1407, 1415), (1407, 1415)]       false positive  \n",
       "1                [(9625, 9635), (9635, 9636)]       false positive  \n",
       "2  [(2426, 2432), (2433, 2439), (2439, 2440)]       false negative  \n",
       "3             [(9993, 10003), (10003, 10004)]       false negative  \n",
       "4                              [(7192, 7195)]       false positive  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_agmts = []\n",
    "token_agmts = (eval_by_ann[\"_merge\"])\n",
    "for agmts in token_agmts:\n",
    "    if \"true positive\" in agmts:\n",
    "        ann_agmt = \"true positive\"\n",
    "    elif \"false positive\" in agmts:\n",
    "        ann_agmt = \"false positive\"\n",
    "    else:\n",
    "        ann_agmt = \"false negative\"\n",
    "    ann_agmts += [ann_agmt]\n",
    "assert len(ann_agmts) == eval_by_ann.shape[0]\n",
    "eval_by_ann.insert(len(eval_by_ann.columns), \"annotation_agreement\", ann_agmts)\n",
    "eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann[[\"ann_id\", \"ann_offsets\", \"token_id\", \"expected_label\", \"predicted_label\", \"annotation_agreement\"]].to_csv(\n",
    "    agreement_dir+\"cc-{a}_baseline_fastText{d}_ling_annot_evaluation.csv\".format(a=a,d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate annotation agreement metrics for each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_agmt = pd.DataFrame.from_dict({\n",
    "        \"label\":[], \"false negative\":[], \"false positive\":[],\n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2918.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>0.190569</td>\n",
       "      <td>0.899215</td>\n",
       "      <td>0.314488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>847.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.062436</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.083276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>843.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.131111</td>\n",
       "      <td>0.065410</td>\n",
       "      <td>0.087278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label  false negative  false positive  true positive  precision  \\\n",
       "0  Gendered-Pronoun            77.0          2918.0          687.0   0.190569   \n",
       "0     Gendered-Role           847.0          1817.0          121.0   0.062436   \n",
       "0    Generalization           843.0           391.0           59.0   0.131111   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.899215  0.314488  \n",
       "0  0.125000  0.083276  \n",
       "0  0.065410  0.087278  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ling_label_tags.keys()\n",
    "for label in labels:\n",
    "    agmt_df = eval_by_ann.loc[eval_by_ann.expected_label == label]\n",
    "    tp = agmt_df.loc[agmt_df.annotation_agreement == \"true positive\"].shape[0]\n",
    "    fp = agmt_df.loc[agmt_df.annotation_agreement == \"false positive\"].shape[0]\n",
    "    fn = agmt_df.loc[agmt_df.annotation_agreement == \"false negative\"].shape[0]\n",
    "    prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "    label_agmt = pd.DataFrame.from_dict({\n",
    "            \"label\":[label], \"false negative\":[fn], \"false positive\":[fp],\n",
    "             \"true positive\":[tp], \"precision\":[prec], \"recall\":[rec], \"f1\":[f1]\n",
    "        })\n",
    "    annot_agmt = pd.concat([annot_agmt, label_agmt])\n",
    "annot_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Gendered-Role\"][\"annotation_agreement\"].value_counts()     # Looks good\n",
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Gendered-Pronoun\"][\"annotation_agreement\"].value_counts()  # Looks good\n",
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Generalization\"][\"annotation_agreement\"].value_counts()    # Looks good\n",
    "annot_agmt.to_csv(agreement_dir+\"cc-{a}_baseline_fastText{d}_ling_annot_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loose Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each label, where a correct prediction is a prediction with the correct annotation label (not necessarily the correct IOB tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the evaluation DataFrame where tags are replaced by label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"rf\"\n",
    "eval_df = pd.read_csv(agreement_dir+\"cc-{a}_ling_baseline_fastText{d}_evaluation.csv\".format(a=a,d=d), index_col=0)\n",
    "loose_eval_df = eval_df.copy()\n",
    "for label,tags in ling_label_tags.items():\n",
    "    for tag in tags:\n",
    "        loose_eval_df[\"expected_tag\"] = loose_eval_df[\"expected_tag\"].replace(to_replace=tag, value=label)\n",
    "        loose_eval_df[\"predicted_tag\"] = loose_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 7)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose_eval_df.loc[loose_eval_df.predicted_tag.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>predicted_tag</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>24</td>\n",
       "      <td>668</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>24</td>\n",
       "      <td>674</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>28</td>\n",
       "      <td>756</td>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id  token_id token   pos      expected_tag     predicted_tag  \\\n",
       "1              5       155   his  PRP$  Gendered-Pronoun  Gendered-Pronoun   \n",
       "3              5       157    he   PRP  Gendered-Pronoun  Gendered-Pronoun   \n",
       "152           24       668    he   PRP  Gendered-Pronoun  Gendered-Pronoun   \n",
       "158           24       674    he   PRP  Gendered-Pronoun  Gendered-Pronoun   \n",
       "220           28       756    He   PRP  Gendered-Pronoun  Gendered-Pronoun   \n",
       "\n",
       "            _merge  \n",
       "1    true positive  \n",
       "3    true positive  \n",
       "152  true positive  \n",
       "158  true positive  \n",
       "220  true positive  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose_eval_df = loose_eval_df.fillna(\"O\")\n",
    "loose_eval_df = loose_eval_df.drop(columns=[\"_merge\"])\n",
    "loose_eval_df = utils.compareExpectedPredicted(loose_eval_df, \"_merge\", \"O\")\n",
    "loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df.to_csv(agreement_dir+\"cc-{a}_ling_baseline_fastText{d}_evaluation_loose.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt = pd.DataFrame.from_dict({\n",
    "        \"tag(s)\":[], \"false negative\":[], \"false positive\":[],\n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1374.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983536</td>\n",
       "      <td>0.991700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.662791</td>\n",
       "      <td>0.797203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.620155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0  Gendered-Pronoun            23.0             0.0         1374.0        1.0   \n",
       "0     Gendered-Role           116.0             0.0          228.0        1.0   \n",
       "0    Generalization            98.0             0.0           80.0        1.0   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.983536  0.991700  \n",
       "0  0.662791  0.797203  \n",
       "0  0.449438  0.620155  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label,tags in ling_label_tags.items():\n",
    "    labels_agmt_stats = utils.getScoresByTags(loose_eval_df, \"_merge\", [label])\n",
    "    loose_agmt = pd.concat([loose_agmt, labels_agmt_stats])\n",
    "loose_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt.to_csv(agreement_dir+\"cc-{a}_baseline_fastText{d}_ling_loose_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<a id=\"ii\"></a>\n",
    "\n",
    "#### *For train-dev-test (i.e., 40-40-20) approach:*\n",
    "\n",
    "## II. Predict Over All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = clf.predict(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>predicted_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Scope</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Contents</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Sermons</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id     token  pos predicted_tag\n",
       "0            2        16     Scope   NN             O\n",
       "1            2        17       and   CC             O\n",
       "2            2        18  Contents  NNS             O\n",
       "3            2        19         :    :             O\n",
       "4            2        20   Sermons  NNS             O"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = utils.makePredictionDF(all_predictions, all_data, \"tag\", \"predicted_tag\", \"O\", mlb)\n",
    "assert pred_df.loc[pred_df.predicted_tag.isna()].shape[0] == 0, \"Any NaN values should be replaced with 'O'\"\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755963 753521\n"
     ]
    }
   ],
   "source": [
    "print(pred_df.shape[0], len(pred_df.token_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(config.experiment1_output_path+\"cc-{a}_ling_baseline_fastText{d}_predictions_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Strict, All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - macro: 0.5920109983470005\n",
      "Recall - macro: 0.44067573805476495\n",
      "F1 Score - macro: 0.44051951753599433\n",
      "Accuracy - normalized: 0.9929769707811726\n",
      "Accuracy - unnormalized: 748229\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - macro:\", sklearn.metrics.precision_score(y_all, all_predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "print(\"Recall - macro:\", sklearn.metrics.recall_score(y_all, all_predictions, average=\"macro\", zero_division=0))\n",
    "print(\"F1 Score - macro:\", sklearn.metrics.f1_score(y_all, all_predictions, average=\"macro\", zero_division=0))\n",
    "print(\"Accuracy - normalized:\", sklearn.metrics.accuracy_score(y_all, all_predictions, normalize=True))  # fraction of correctly classified samples\n",
    "print(\"Accuracy - unnormalized:\", sklearn.metrics.accuracy_score(y_all, all_predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 753521\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples:\", X_all.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more predictions than unique tokens, because with multilabel classification, one token can have multiple predicted tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Scope</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Contents</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Sermons</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id     token  pos expected_tag\n",
       "0            2        16     Scope   NN            O\n",
       "1            2        17       and   CC            O\n",
       "2            2        18  Contents  NNS            O\n",
       "3            2        19         :    :            O\n",
       "4            2        20   Sermons  NNS            O"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_df = all_data.explode([\"tag\"])\n",
    "exp_df = exp_df.rename(columns={\"tag\":\"expected_tag\"})\n",
    "exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757416 753521\n"
     ]
    }
   ],
   "source": [
    "print(exp_df.shape[0], len(exp_df.token_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759346, 7)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_pred_df = pd.merge(\n",
    "    left=exp_df, \n",
    "    right=pred_df.loc[pred_df.predicted_tag != \"O\"], # only include the predictions of Linguistic labels\n",
    "    how=\"outer\",\n",
    "    left_on=[\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_tag\"],\n",
    "    right_on=[\"sentence_id\", \"token_id\", \"token\", \"pos\", \"predicted_tag\"],\n",
    "    suffixes=[\"\", \"_pred\"],\n",
    "    indicator=True\n",
    ")\n",
    "exp_pred_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreement type for each row, ignoring rows with `'O'` and `NaN` value pairs (the `true negative` agreement type, which doesn't go into the precision, recall, or F1 score calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>predicted_tag</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>19</td>\n",
       "      <td>533</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>19</td>\n",
       "      <td>539</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>37</td>\n",
       "      <td>960</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>37</td>\n",
       "      <td>973</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>39</td>\n",
       "      <td>1002</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id  token_id token   pos        expected_tag  \\\n",
       "129           19       533    he   PRP  B-Gendered-Pronoun   \n",
       "135           19       539    he   PRP  B-Gendered-Pronoun   \n",
       "261           37       960    he   PRP  B-Gendered-Pronoun   \n",
       "274           37       973    he   PRP  B-Gendered-Pronoun   \n",
       "292           39      1002   his  PRP$  B-Gendered-Pronoun   \n",
       "\n",
       "          predicted_tag         _merge  \n",
       "129  B-Gendered-Pronoun  true positive  \n",
       "135  B-Gendered-Pronoun  true positive  \n",
       "261  B-Gendered-Pronoun  true positive  \n",
       "274  B-Gendered-Pronoun  true positive  \n",
       "292  B-Gendered-Pronoun  true positive  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_col = \"expected_tag\"\n",
    "pred_col = \"predicted_tag\"\n",
    "no_tag_value = \"O\"\n",
    "# Find true negatives based on the expected and predicted tags\n",
    "sub_exp_pred_df = exp_pred_df.loc[exp_pred_df[exp_col] == no_tag_value]\n",
    "sub_exp_pred_df = sub_exp_pred_df.loc[sub_exp_pred_df[pred_col].isna()]\n",
    "# sub_exp_pred_df.replace(to_replace=\"left_only\", value=\"true negative\", inplace=True)\n",
    "tn_tokens = list(sub_exp_pred_df[\"token_id\"])\n",
    "\n",
    "# Record false negatives, false positives, and true positives based on the merge values\n",
    "eval_df = exp_pred_df.loc[~exp_pred_df[\"token_id\"].isin(tn_tokens)]\n",
    "eval_df = eval_df.replace(to_replace=\"left_only\", value=\"false negative\")\n",
    "eval_df = eval_df.replace(to_replace=\"right_only\", value=\"false positive\")\n",
    "eval_df = eval_df.replace(to_replace=\"both\", value=\"true positive\")\n",
    "eval_df = eval_df.sort_index()\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5264, 7)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strict Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the true positives, false positives, false negatives, precision, recall, and F1 metrics for all tags and each tag individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>1045</td>\n",
       "      <td>16</td>\n",
       "      <td>4203</td>\n",
       "      <td>0.462383</td>\n",
       "      <td>0.422989</td>\n",
       "      <td>0.422518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Generalization</td>\n",
       "      <td>261</td>\n",
       "      <td>10</td>\n",
       "      <td>444</td>\n",
       "      <td>0.977974</td>\n",
       "      <td>0.629787</td>\n",
       "      <td>0.766178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Generalization</td>\n",
       "      <td>221</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008969</td>\n",
       "      <td>0.017621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Gendered-Role</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>1178</td>\n",
       "      <td>0.999152</td>\n",
       "      <td>0.869373</td>\n",
       "      <td>0.929755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Gendered-Role</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>6782</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.998528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Gendered-Pronoun</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag(s)  false negative  false positive  true positive  \\\n",
       "0                 all            1045              16           4203   \n",
       "0    B-Generalization             261              10            444   \n",
       "0    I-Generalization             221               2              2   \n",
       "0     B-Gendered-Role             177               1           1178   \n",
       "0     I-Gendered-Role             319               0              0   \n",
       "0  B-Gendered-Pronoun              17               3           6782   \n",
       "0  I-Gendered-Pronoun              50               0              0   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.462383  0.422989  0.422518  \n",
       "0   0.977974  0.629787  0.766178  \n",
       "0   0.500000  0.008969  0.017621  \n",
       "0   0.999152  0.869373  0.929755  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.999558  0.997500  0.998528  \n",
       "0   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agmt_stats = utils.getAgreementStatsForAllTags(eval_df, \"_merge\", \"token_id\", \"tag(s)\", y_dev, predictions)\n",
    "for label_tag in ling_label_subset:\n",
    "    label_agmt_stats = utils.getScoresByTags(eval_df, \"_merge\", [label_tag])\n",
    "    agmt_stats = pd.concat([agmt_stats, label_agmt_stats])\n",
    "agmt_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_stats.to_csv(config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_strict_agmt_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Annotation-level Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the manual annotations' offsets to the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv(config.agg_path+\"aggregated_final.csv\")#, usecols=[\"description_id\",\"agg_ann_id\", \"ann_offsets\"])\n",
    "# Get only the Linguistic annotations\n",
    "annot_df = annot_df.loc[annot_df.category == \"Linguistic\"]\n",
    "annot_df = annot_df[[\"agg_ann_id\", \"ann_offsets\", \"label\"]]\n",
    "annot_df = annot_df.rename(columns={\"agg_ann_id\":\"ann_id\"})\n",
    "# annot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_token_ids = list(dev_data.token_id.unique())\n",
    "ling_dev_subset = ling_dev.loc[ling_dev.token_id.isin(dev_token_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7083, 9)\n",
      "(7083, 11)\n"
     ]
    }
   ],
   "source": [
    "to_add = ling_dev_subset[[\"ann_id\", \"token_id\", \"token_offsets\"]]\n",
    "# Only include annotations with Linguistic labels\n",
    "to_add = to_add.loc[to_add.ann_id.isin(list(annot_df.ann_id))]\n",
    "eval_df_joined = eval_df.join(to_add.set_index(\"token_id\"), on=\"token_id\", how=\"outer\")\n",
    "# Join on the left, as there will be annotations from outside the devtest set in annot_df\n",
    "print(eval_df_joined.shape)\n",
    "eval_df_joined = eval_df_joined.join(annot_df.set_index(\"ann_id\"), on=\"ann_id\", how=\"left\")\n",
    "print(eval_df_joined.shape)  # Looks good!  Same as before join.\n",
    "eval_df_joined = eval_df_joined.rename(columns={\"label\":\"expected_label\"})\n",
    "# eval_df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the predicted tags with their corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_joined.expected_label = eval_df_joined.expected_label.fillna(\"no_label\")\n",
    "eval_df_joined.expected_tag = eval_df_joined.expected_tag.fillna(\"no_label\")\n",
    "eval_df_joined.predicted_tag = eval_df_joined.predicted_tag.fillna(\"no_label\")\n",
    "# eval_df_joined.predicted_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = list(eval_df_joined.predicted_tag)\n",
    "predicted_labels = [tag[2:] if tag != \"no_label\" else tag for tag in predicted_labels]\n",
    "eval_df_joined.insert(len(eval_df_joined.columns), \"predicted_label\", predicted_labels)\n",
    "# eval_df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"sentence_id\", \"token_id\", \"token\", \"expected_label\", \"predicted_label\", \"_merge\", \"token_offsets\", \"ann_offsets\", \"ann_id\"]\n",
    "eval_by_ann = utils.implodeDataFrame(eval_df_joined[cols_to_keep], [\"ann_id\", \"ann_offsets\"]).reset_index()\n",
    "exp_labels = list(eval_by_ann[\"expected_label\"])\n",
    "exp_labels = [labels[0] for labels in exp_labels]\n",
    "eval_by_ann[\"expected_label\"] = exp_labels\n",
    "# eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert eval_by_ann.loc[eval_by_ann.expected_label == \"no_label\"].shape[0] == 0\n",
    "assert eval_by_ann.loc[eval_by_ann.expected_label.isna()].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row should have an annotation label (a Linguistic label in `expected_label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_agmts = []\n",
    "token_agmts = (eval_by_ann[\"_merge\"])\n",
    "for agmts in token_agmts:\n",
    "    if \"true positive\" in agmts:\n",
    "        ann_agmt = \"true positive\"\n",
    "    elif \"false positive\" in agmts:\n",
    "        ann_agmt = \"false positive\"\n",
    "    else:\n",
    "        ann_agmt = \"false negative\"\n",
    "    ann_agmts += [ann_agmt]\n",
    "assert len(ann_agmts) == eval_by_ann.shape[0]\n",
    "eval_by_ann.insert(len(eval_by_ann.columns), \"annotation_agreement\", ann_agmts)\n",
    "# eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann[[\"ann_id\", \"ann_offsets\", \"token_id\", \"expected_label\", \"predicted_label\", \"annotation_agreement\"]].to_csv(\n",
    "    config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_annot_evaluation_ALLDATA.csv\".format(a=a,d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate annotation agreement metrics for each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_agmt = pd.DataFrame.from_dict({\n",
    "        \"label\":[], \"false negative\":[], \"false positive\":[],\n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914491</td>\n",
       "      <td>0.955336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>976.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.190713</td>\n",
       "      <td>0.319444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>408.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>0.233083</td>\n",
       "      <td>0.376328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label  false negative  false positive  true positive  precision  \\\n",
       "0  Gendered-Pronoun           131.0             0.0         1401.0   1.000000   \n",
       "0     Gendered-Role           976.0             4.0          230.0   0.982906   \n",
       "0    Generalization           408.0             3.0          124.0   0.976378   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.914491  0.955336  \n",
       "0  0.190713  0.319444  \n",
       "0  0.233083  0.376328  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ling_label_tags.keys()\n",
    "for label in labels:\n",
    "    agmt_df = eval_by_ann.loc[eval_by_ann.expected_label == label]\n",
    "    tp = agmt_df.loc[agmt_df.annotation_agreement == \"true positive\"].shape[0]\n",
    "    fp = agmt_df.loc[agmt_df.annotation_agreement == \"false positive\"].shape[0]\n",
    "    fn = agmt_df.loc[agmt_df.annotation_agreement == \"false negative\"].shape[0]\n",
    "    prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "    label_agmt = pd.DataFrame.from_dict({\n",
    "            \"label\":[label], \"false negative\":[fn], \"false positive\":[fp],\n",
    "             \"true positive\":[tp], \"precision\":[prec], \"recall\":[rec], \"f1\":[f1]\n",
    "        })\n",
    "    annot_agmt = pd.concat([annot_agmt, label_agmt])\n",
    "annot_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Gendered-Role\"][\"annotation_agreement\"].value_counts()     # Looks good\n",
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Gendered-Pronoun\"][\"annotation_agreement\"].value_counts()  # Looks good\n",
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Generalization\"][\"annotation_agreement\"].value_counts()    # Looks good\n",
    "annot_agmt.to_csv(config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_annot_agmt_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loose Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each label, where a correct prediction is a prediction with the correct annotation label (not necessarily the correct IOB tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the evaluation DataFrame where tags are replaced by label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"rf\"\n",
    "eval_df = pd.read_csv(config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_ALLDATA.csv\".format(a=a,d=d), index_col=0)\n",
    "loose_eval_df = eval_df.copy()\n",
    "for label,tags in ling_label_tags.items():\n",
    "    for tag in tags:\n",
    "        loose_eval_df[\"expected_tag\"] = loose_eval_df[\"expected_tag\"].replace(to_replace=tag, value=label)\n",
    "        loose_eval_df[\"predicted_tag\"] = loose_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1045, 7)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose_eval_df.loc[loose_eval_df.predicted_tag.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df = loose_eval_df.fillna(\"O\")\n",
    "loose_eval_df = loose_eval_df.drop(columns=[\"_merge\"])\n",
    "loose_eval_df = utils.compareExpectedPredicted(loose_eval_df, \"_merge\", \"O\")\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df.to_csv(config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_loose_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt = pd.DataFrame.from_dict({\n",
    "        \"tag(s)\":[], \"false negative\":[], \"false positive\":[],\n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6782.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990218</td>\n",
       "      <td>0.995085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>482.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>446.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.480603</td>\n",
       "      <td>0.649199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag(s)  false negative  false positive  true negative  \\\n",
       "0  Gendered-Pronoun            67.0             0.0            NaN   \n",
       "0     Gendered-Role           496.0             0.0            NaN   \n",
       "0    Generalization           482.0             0.0            NaN   \n",
       "\n",
       "   true positive  precision    recall        f1  \n",
       "0         6782.0        1.0  0.990218  0.995085  \n",
       "0         1178.0        1.0  0.703704  0.826087  \n",
       "0          446.0        1.0  0.480603  0.649199  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label,tags in ling_label_tags.items():\n",
    "    labels_agmt_stats = utils.getScoresByTags(loose_eval_df, \"_merge\", [label])\n",
    "    loose_agmt = pd.concat([loose_agmt, labels_agmt_stats])\n",
    "loose_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt.to_csv(config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_loose_agmt_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

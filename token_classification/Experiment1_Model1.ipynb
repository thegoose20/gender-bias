{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1, Model 1\n",
    "\n",
    "#### Model Setup\n",
    "\n",
    "Run models in the following order, using their output labels as features for the next model:\n",
    "\n",
    "1. Multilabel Linguistic Classifier\n",
    "2. Multiclass Person Name + Occupation Sequence Classifier\n",
    "3. Multilabel Document Classifier\n",
    "\n",
    "Train the first model and then run it over the entire dataset.\n",
    "\n",
    "***\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/experiment_input/`\n",
    "    * Prediction Data: Data: under directory `../data/token_clf_data/model_output/experiment1/`\n",
    "* Word Embeddings\n",
    "    * Custom fastText (word2vec with subwords) embeddings of 100 dimensions trained on the CRC Archives catalog's descriptive metadata (harvested October 2020)\n",
    "    \n",
    "***\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[1.](#1) Train the Linguistic Classifier\n",
    "\n",
    "[2.](#2) Predict Over All Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load programming resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, utils1, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For multilabel token classification\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "# For multiclass sequence classification\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define resources for the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.experiment_input_path).mkdir(parents=True, exist_ok=True)    # For train, devtest, and blind test data\n",
    "Path(config.experiment1_output_path).mkdir(parents=True, exist_ok=True)  # For predictions\n",
    "Path(config.experiment1_agmt_path).mkdir(parents=True, exist_ok=True)    # For agreement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1:\n",
    "ling_label_subset = [\"B-Generalization\", \"I-Generalization\", \"B-Gendered-Role\", \"I-Gendered-Role\", \"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"]\n",
    "# Model 2:\n",
    "pers_o_label_subset = [\"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \"B-Masculine\", \"I-Masculine\", \"B-Occupation\", \"I-Occupation\"]\n",
    "# Model 3:\n",
    "so_label_subset = [\"B-Stereotype\", \"I-Stereotype\", \"B-Omission\", \"I-Omission\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_label_tags = {\n",
    "    \"Gendered-Pronoun\": [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"], \"Gendered-Role\": [\"B-Gendered-Role\", \"I-Gendered-Role\"],\"Generalization\": [\"B-Generalization\", \"I-Generalization\"]\n",
    "    }\n",
    "pers_o_label_tags = {\n",
    "    \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "     \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "    }\n",
    "so_label_tags = {\n",
    "    \"Stereotype\": [\"B-Stereotype\", \"I-Stereotype\"], \"Omission\": [\"B-Omission\", \"I-Omission\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100  # dimensions of word embeddings (should match utils1.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Train the Linguistic Classifier\n",
    "\n",
    "Run a multilabel classifier on the train set of the data, focusing only on applying the Linguistic category of labels: Gendered Pronoun, Gendered Role, and Generalization.\n",
    "\n",
    "Use a Classifier Chain with Random Forest, as this was the highest-performing multilabel model setup from previous algorithm experiments for the Linguistic labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, we'll train the model on 40% of the data, rather than 60%.  We'll use fastText embeddings of 100 dimensions, as was used in the model that achieved the above scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config.tokc_path+\"experiment_input/token_train.csv\", index_col=0)\n",
    "dev_df = pd.read_csv(config.tokc_path+\"experiment_input/token_validate.csv\", index_col=0)\n",
    "test_df = pd.read_csv(config.tokc_path+\"experiment_input/token_validate.csv\", index_col=0)\n",
    "ling_train = utils1.selectDataForLabels(train_df, \"tag\", ling_label_subset)\n",
    "ling_dev = utils1.selectDataForLabels(dev_df, \"tag\", ling_label_subset)\n",
    "ling_test = utils1.selectDataForLabels(test_df, \"tag\", ling_label_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298617, 5) (305924, 5) (305924, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Scope</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Contents</td>\n",
       "      <td>NNS</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Sermons</td>\n",
       "      <td>NNS</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id     token  pos  tag\n",
       "0            2        16     Scope   NN  [O]\n",
       "1            2        17       and   CC  [O]\n",
       "2            2        18  Contents  NNS  [O]\n",
       "3            2        19         :    :  [O]\n",
       "4            2        20   Sermons  NNS  [O]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = utils1.loadData(ling_train)\n",
    "dev_data = utils1.loadData(ling_dev)\n",
    "test_data = utils1.loadData(ling_test)\n",
    "print(train_data.shape, dev_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_data.shape[0] == len(train_data.token_id.unique())\n",
    "assert dev_data.shape[0] == len(dev_data.token_id.unique())\n",
    "assert test_data.shape[0] == len(test_data.token_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the data to run the model over after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_data, dev_data, test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = utils1.zipTokensFeatures(train_data)\n",
    "dev_tokens = utils1.zipTokensFeatures(dev_data)\n",
    "all_tokens = utils1.zipTokensFeatures(all_data)\n",
    "X_train = utils1.makeFastTextFeatureMatrix(train_tokens)\n",
    "X_dev = utils1.makeFastTextFeatureMatrix(dev_tokens)\n",
    "X_all = utils1.makeFastTextFeatureMatrix(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarize targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb, y_train = utils1.binarizeTrainTargets(train_data)\n",
    "y_dev = utils1.binarizeDevTargets(mlb, dev_data)\n",
    "y_all = utils1.binarizeDevTargets(mlb, all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=RandomForestClassifier(random_state=22),\n",
       "                require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=RandomForestClassifier(random_state=22),\n",
       "                require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=22)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=22)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ClassifierChain(classifier=RandomForestClassifier(random_state=22),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"rf\"\n",
    "clf = ClassifierChain(\n",
    "    classifier = RandomForestClassifier(random_state=22),\n",
    ")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Strict, All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - macro: 0.46238272522632073\n",
      "Recall - macro: 0.42298863525280694\n",
      "F1 Score - macro: 0.42251820432443793\n",
      "Accuracy - normalized: 0.9925798564349315\n",
      "Accuracy - unnormalized: 303654\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - macro:\", sklearn.metrics.precision_score(y_dev, predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "print(\"Recall - macro:\", sklearn.metrics.recall_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print(\"F1 Score - macro:\", sklearn.metrics.f1_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print(\"Accuracy - normalized:\", sklearn.metrics.accuracy_score(y_dev, predictions, normalize=True))  # fraction of correctly classified samples\n",
    "print(\"Accuracy - unnormalized:\", sklearn.metrics.accuracy_score(y_dev, predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 305924\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples:\", X_dev.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Annotation-level Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the manual annotations' offsets to the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv(config.agg_path+\"aggregated_final.csv\")#, usecols=[\"description_id\",\"agg_ann_id\", \"ann_offsets\"])\n",
    "# Get only the Linguistic annotations\n",
    "annot_df = annot_df.loc[annot_df.category == \"Linguistic\"]\n",
    "annot_df = annot_df[[\"agg_ann_id\", \"ann_offsets\", \"label\"]]\n",
    "annot_df = annot_df.rename(columns={\"agg_ann_id\":\"ann_id\"})\n",
    "# annot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_token_ids = list(dev_data.token_id.unique())\n",
    "ling_dev_subset = ling_dev.loc[ling_dev.token_id.isin(dev_token_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3996, 9)\n",
      "(3996, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>predicted_tag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>ann_offsets</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14379</td>\n",
       "      <td>(913, 916)</td>\n",
       "      <td>(913, 916)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14380</td>\n",
       "      <td>(928, 930)</td>\n",
       "      <td>(928, 930)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>216</td>\n",
       "      <td>His</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14382</td>\n",
       "      <td>(1241, 1244)</td>\n",
       "      <td>(1241, 1244)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>226</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14383</td>\n",
       "      <td>(1315, 1317)</td>\n",
       "      <td>(1315, 1317)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218.0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>435</td>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>9516</td>\n",
       "      <td>(677, 679)</td>\n",
       "      <td>(677, 679)</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id  token_id token   pos        expected_tag  \\\n",
       "39.0           5.0       155   his  PRP$  B-Gendered-Pronoun   \n",
       "41.0           5.0       157    he   PRP  B-Gendered-Pronoun   \n",
       "62.0           7.0       216   His  PRP$  B-Gendered-Pronoun   \n",
       "72.0           7.0       226    he   PRP  B-Gendered-Pronoun   \n",
       "218.0         16.0       435    He   PRP  B-Gendered-Pronoun   \n",
       "\n",
       "            predicted_tag         _merge  ann_id token_offsets   ann_offsets  \\\n",
       "39.0   B-Gendered-Pronoun  true positive   14379    (913, 916)    (913, 916)   \n",
       "41.0   B-Gendered-Pronoun  true positive   14380    (928, 930)    (928, 930)   \n",
       "62.0   B-Gendered-Pronoun  true positive   14382  (1241, 1244)  (1241, 1244)   \n",
       "72.0   B-Gendered-Pronoun  true positive   14383  (1315, 1317)  (1315, 1317)   \n",
       "218.0  B-Gendered-Pronoun  true positive    9516    (677, 679)    (677, 679)   \n",
       "\n",
       "         expected_label  \n",
       "39.0   Gendered-Pronoun  \n",
       "41.0   Gendered-Pronoun  \n",
       "62.0   Gendered-Pronoun  \n",
       "72.0   Gendered-Pronoun  \n",
       "218.0  Gendered-Pronoun  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_add = ling_dev_subset[[\"ann_id\", \"token_id\", \"token_offsets\"]]\n",
    "# Only include annotations with Linguistic labels\n",
    "to_add = to_add.loc[to_add.ann_id.isin(list(annot_df.ann_id))]\n",
    "eval_df_joined = eval_df.join(to_add.set_index(\"token_id\"), on=\"token_id\", how=\"outer\")\n",
    "# Join on the left, as there will be annotations from outside the devtest set in annot_df\n",
    "print(eval_df_joined.shape)\n",
    "eval_df_joined = eval_df_joined.join(annot_df.set_index(\"ann_id\"), on=\"ann_id\", how=\"left\")\n",
    "print(eval_df_joined.shape)  # Looks good!  Same as before join.\n",
    "eval_df_joined = eval_df_joined.rename(columns={\"label\":\"expected_label\"})\n",
    "eval_df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the predicted tags with their corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_label              2234\n",
       "B-Gendered-Pronoun    1447\n",
       "B-Gendered-Role        232\n",
       "B-Generalization        82\n",
       "I-Generalization         1\n",
       "Name: predicted_tag, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df_joined.expected_label = eval_df_joined.expected_label.fillna(\"no_label\")\n",
    "eval_df_joined.expected_tag = eval_df_joined.expected_tag.fillna(\"no_label\")\n",
    "eval_df_joined.predicted_tag = eval_df_joined.predicted_tag.fillna(\"no_label\")\n",
    "eval_df_joined.predicted_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = list(eval_df_joined.predicted_tag)\n",
    "predicted_labels = [tag[2:] if tag != \"no_label\" else tag for tag in predicted_labels]\n",
    "eval_df_joined.insert(len(eval_df_joined.columns), \"predicted_label\", predicted_labels)\n",
    "# eval_df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"sentence_id\", \"token_id\", \"token\", \"expected_label\", \"predicted_label\", \"_merge\", \"token_offsets\", \"ann_offsets\", \"ann_id\"]\n",
    "eval_by_ann = utils.implodeDataFrame(eval_df_joined[cols_to_keep], [\"ann_id\", \"ann_offsets\"]).reset_index()\n",
    "exp_labels = list(eval_by_ann[\"expected_label\"])\n",
    "exp_labels = [labels[0] for labels in exp_labels]\n",
    "eval_by_ann[\"expected_label\"] = exp_labels\n",
    "# eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert eval_by_ann.loc[eval_by_ann.expected_label == \"no_label\"].shape[0] == 0\n",
    "assert eval_by_ann.loc[eval_by_ann.expected_label.isna()].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row should have an annotation label (a Linguistic label in `expected_label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>ann_offsets</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>expected_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>_merge</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>annotation_agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(1407, 1415)</td>\n",
       "      <td>[5760.0, 5760.0]</td>\n",
       "      <td>[133674, 133674]</td>\n",
       "      <td>[knighted, knighted]</td>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>[no_label, Generalization]</td>\n",
       "      <td>[false negative, false positive]</td>\n",
       "      <td>[(1407, 1415), (1407, 1415)]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(9625, 9635)</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[228678, 228679]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>[no_label, no_label]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[(9625, 9635), (9635, 9636)]</td>\n",
       "      <td>false negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(2426, 2439)</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[196525, 196526, 196527]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>[no_label, no_label, no_label]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[(2426, 2432), (2433, 2439), (2439, 2440)]</td>\n",
       "      <td>false negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>(4141, 4148)</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[4714, 4715]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>[no_label, no_label]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[(4141, 4148), (4148, 4149)]</td>\n",
       "      <td>false negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1063</td>\n",
       "      <td>(1532, 1535)</td>\n",
       "      <td>[12112.0]</td>\n",
       "      <td>[272117]</td>\n",
       "      <td>[his]</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>[Gendered-Pronoun]</td>\n",
       "      <td>[true positive]</td>\n",
       "      <td>[(1532, 1535)]</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id   ann_offsets       sentence_id                  token_id  \\\n",
       "0       0  (1407, 1415)  [5760.0, 5760.0]          [133674, 133674]   \n",
       "1       1  (9625, 9635)        [nan, nan]          [228678, 228679]   \n",
       "2       2  (2426, 2439)   [nan, nan, nan]  [196525, 196526, 196527]   \n",
       "3      93  (4141, 4148)        [nan, nan]              [4714, 4715]   \n",
       "4    1063  (1532, 1535)         [12112.0]                  [272117]   \n",
       "\n",
       "                  token    expected_label                 predicted_label  \\\n",
       "0  [knighted, knighted]     Gendered-Role      [no_label, Generalization]   \n",
       "1            [nan, nan]     Gendered-Role            [no_label, no_label]   \n",
       "2       [nan, nan, nan]     Gendered-Role  [no_label, no_label, no_label]   \n",
       "3            [nan, nan]     Gendered-Role            [no_label, no_label]   \n",
       "4                 [his]  Gendered-Pronoun              [Gendered-Pronoun]   \n",
       "\n",
       "                             _merge  \\\n",
       "0  [false negative, false positive]   \n",
       "1                        [nan, nan]   \n",
       "2                   [nan, nan, nan]   \n",
       "3                        [nan, nan]   \n",
       "4                   [true positive]   \n",
       "\n",
       "                                token_offsets annotation_agreement  \n",
       "0                [(1407, 1415), (1407, 1415)]       false positive  \n",
       "1                [(9625, 9635), (9635, 9636)]       false negative  \n",
       "2  [(2426, 2432), (2433, 2439), (2439, 2440)]       false negative  \n",
       "3                [(4141, 4148), (4148, 4149)]       false negative  \n",
       "4                              [(1532, 1535)]        true positive  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_agmts = []\n",
    "token_agmts = (eval_by_ann[\"_merge\"])\n",
    "for agmts in token_agmts:\n",
    "    if \"true positive\" in agmts:\n",
    "        ann_agmt = \"true positive\"\n",
    "    elif \"false positive\" in agmts:\n",
    "        ann_agmt = \"false positive\"\n",
    "    else:\n",
    "        ann_agmt = \"false negative\"\n",
    "    ann_agmts += [ann_agmt]\n",
    "assert len(ann_agmts) == eval_by_ann.shape[0]\n",
    "eval_by_ann.insert(len(eval_by_ann.columns), \"annotation_agreement\", ann_agmts)\n",
    "eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann[[\"ann_id\", \"ann_offsets\", \"token_id\", \"expected_label\", \"predicted_label\", \"annotation_agreement\"]].to_csv(\n",
    "    config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_annot_evaluation.csv\".format(a=a,d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate annotation agreement metrics for each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_agmt = pd.DataFrame.from_dict({\n",
    "        \"label\":[], \"false negative\":[], \"false positive\":[],\n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914491</td>\n",
       "      <td>0.955336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>976.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.190713</td>\n",
       "      <td>0.319444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>408.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>0.233083</td>\n",
       "      <td>0.376328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label  false negative  false positive  true positive  precision  \\\n",
       "0  Gendered-Pronoun           131.0             0.0         1401.0   1.000000   \n",
       "0     Gendered-Role           976.0             4.0          230.0   0.982906   \n",
       "0    Generalization           408.0             3.0          124.0   0.976378   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.914491  0.955336  \n",
       "0  0.190713  0.319444  \n",
       "0  0.233083  0.376328  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ling_label_tags.keys()\n",
    "for label in labels:\n",
    "    agmt_df = eval_by_ann.loc[eval_by_ann.expected_label == label]\n",
    "    tp = agmt_df.loc[agmt_df.annotation_agreement == \"true positive\"].shape[0]\n",
    "    fp = agmt_df.loc[agmt_df.annotation_agreement == \"false positive\"].shape[0]\n",
    "    fn = agmt_df.loc[agmt_df.annotation_agreement == \"false negative\"].shape[0]\n",
    "    prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "    label_agmt = pd.DataFrame.from_dict({\n",
    "            \"label\":[label], \"false negative\":[fn], \"false positive\":[fp],\n",
    "             \"true positive\":[tp], \"precision\":[prec], \"recall\":[rec], \"f1\":[f1]\n",
    "        })\n",
    "    annot_agmt = pd.concat([annot_agmt, label_agmt])\n",
    "annot_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Gendered-Role\"][\"annotation_agreement\"].value_counts()     # Looks good\n",
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Gendered-Pronoun\"][\"annotation_agreement\"].value_counts()  # Looks good\n",
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Generalization\"][\"annotation_agreement\"].value_counts()    # Looks good\n",
    "annot_agmt.to_csv(config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_annot_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loose Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each label, where a correct prediction is a prediction with the correct annotation label (not necessarily the correct IOB tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the evaluation DataFrame where tags are replaced by label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"rf\"\n",
    "eval_df = pd.read_csv(config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation.csv\".format(a=a,d=d), index_col=0)\n",
    "loose_eval_df = eval_df.copy()\n",
    "for label,tags in ling_label_tags.items():\n",
    "    for tag in tags:\n",
    "        loose_eval_df[\"expected_tag\"] = loose_eval_df[\"expected_tag\"].replace(to_replace=tag, value=label)\n",
    "        loose_eval_df[\"predicted_tag\"] = loose_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose_eval_df.loc[loose_eval_df.predicted_tag.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>predicted_tag</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>7</td>\n",
       "      <td>216</td>\n",
       "      <td>His</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7</td>\n",
       "      <td>226</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>16</td>\n",
       "      <td>435</td>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id  token_id token   pos      expected_tag     predicted_tag  \\\n",
       "39             5       155   his  PRP$  Gendered-Pronoun  Gendered-Pronoun   \n",
       "41             5       157    he   PRP  Gendered-Pronoun  Gendered-Pronoun   \n",
       "62             7       216   His  PRP$  Gendered-Pronoun  Gendered-Pronoun   \n",
       "72             7       226    he   PRP  Gendered-Pronoun  Gendered-Pronoun   \n",
       "218           16       435    He   PRP  Gendered-Pronoun  Gendered-Pronoun   \n",
       "\n",
       "            _merge  \n",
       "39   true positive  \n",
       "41   true positive  \n",
       "62   true positive  \n",
       "72   true positive  \n",
       "218  true positive  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose_eval_df = loose_eval_df.fillna(\"O\")\n",
    "loose_eval_df = loose_eval_df.drop(columns=[\"_merge\"])\n",
    "loose_eval_df = utils.compareExpectedPredicted(loose_eval_df, \"_merge\", \"O\")\n",
    "loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df.to_csv(config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_loose.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt = pd.DataFrame.from_dict({\n",
    "        \"tag(s)\":[], \"false negative\":[], \"false positive\":[], \"true negative\":[], \n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2802.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986620</td>\n",
       "      <td>0.993265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656672</td>\n",
       "      <td>0.792760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>209.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.414566</td>\n",
       "      <td>0.586139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag(s)  false negative  false positive  true negative  \\\n",
       "0  Gendered-Pronoun            38.0             0.0            NaN   \n",
       "0     Gendered-Role           229.0             0.0            NaN   \n",
       "0    Generalization           209.0             0.0            NaN   \n",
       "\n",
       "   true positive  precision    recall        f1  \n",
       "0         2802.0        1.0  0.986620  0.993265  \n",
       "0          438.0        1.0  0.656672  0.792760  \n",
       "0          148.0        1.0  0.414566  0.586139  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label,tags in ling_label_tags.items():\n",
    "    labels_agmt_stats = utils.getScoresByTags(loose_eval_df, \"_merge\", [label])\n",
    "    loose_agmt = pd.concat([loose_agmt, labels_agmt_stats])\n",
    "loose_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  The performance of this model looks comparable to the model trained on 60% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt.to_csv(config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_loose_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Predict Over All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = clf.predict(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>predicted_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Scope</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Contents</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Sermons</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id     token  pos predicted_tag\n",
       "0            2        16     Scope   NN             O\n",
       "1            2        17       and   CC             O\n",
       "2            2        18  Contents  NNS             O\n",
       "3            2        19         :    :             O\n",
       "4            2        20   Sermons  NNS             O"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = utils.makePredictionDF(all_predictions, all_data, \"tag\", \"predicted_tag\", \"O\", mlb)\n",
    "assert pred_df.loc[pred_df.predicted_tag.isna()].shape[0] == 0, \"Any NaN values should be replaced with 'O'\"\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306940 305924\n"
     ]
    }
   ],
   "source": [
    "print(pred_df.shape[0], len(pred_df.token_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(config.experiment1_output_path+\"cc-{a}_ling_baseline_fastText{d}_predictions_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Strict, All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - macro: 0.5549545231562777\n",
      "Recall - macro: 0.4357427346662331\n",
      "F1 Score - macro: 0.43723000467667783\n",
      "Accuracy - normalized: 0.9929530514627141\n",
      "Accuracy - unnormalized: 904049\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - macro:\", sklearn.metrics.precision_score(y_all, all_predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "print(\"Recall - macro:\", sklearn.metrics.recall_score(y_all, all_predictions, average=\"macro\", zero_division=0))\n",
    "print(\"F1 Score - macro:\", sklearn.metrics.f1_score(y_all, all_predictions, average=\"macro\", zero_division=0))\n",
    "print(\"Accuracy - normalized:\", sklearn.metrics.accuracy_score(y_all, all_predictions, normalize=True))  # fraction of correctly classified samples\n",
    "print(\"Accuracy - unnormalized:\", sklearn.metrics.accuracy_score(y_all, all_predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 910465\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples:\", X_all.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more predictions than unique tokens, because with multilabel classification, one token can have multiple predicted tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Scope</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>Contents</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Sermons</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id     token  pos expected_tag\n",
       "0            2        16     Scope   NN            O\n",
       "1            2        17       and   CC            O\n",
       "2            2        18  Contents  NNS            O\n",
       "3            2        19         :    :            O\n",
       "4            2        20   Sermons  NNS            O"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_df = all_data.explode([\"tag\"])\n",
    "exp_df = exp_df.rename(columns={\"tag\":\"expected_tag\"})\n",
    "exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915302 604541\n"
     ]
    }
   ],
   "source": [
    "print(exp_df.shape[0], len(exp_df.token_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(922577, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_pred_df = pd.merge(\n",
    "    left=exp_df, \n",
    "    right=pred_df.loc[pred_df.predicted_tag != \"O\"], # only include the predictions of Linguistic labels\n",
    "    how=\"outer\",\n",
    "    left_on=[\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_tag\"],\n",
    "    right_on=[\"sentence_id\", \"token_id\", \"token\", \"pos\", \"predicted_tag\"],\n",
    "    suffixes=[\"\", \"_pred\"],\n",
    "    indicator=True\n",
    ")\n",
    "exp_pred_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreement type for each row, ignoring rows with `'O'` and `NaN` value pairs (the `true negative` agreement type, which doesn't go into the precision, recall, or F1 score calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>predicted_tag</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>19</td>\n",
       "      <td>533</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>19</td>\n",
       "      <td>539</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>37</td>\n",
       "      <td>960</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>37</td>\n",
       "      <td>973</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>39</td>\n",
       "      <td>1002</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id  token_id token   pos        expected_tag  \\\n",
       "129           19       533    he   PRP  B-Gendered-Pronoun   \n",
       "135           19       539    he   PRP  B-Gendered-Pronoun   \n",
       "261           37       960    he   PRP  B-Gendered-Pronoun   \n",
       "274           37       973    he   PRP  B-Gendered-Pronoun   \n",
       "292           39      1002   his  PRP$  B-Gendered-Pronoun   \n",
       "\n",
       "          predicted_tag         _merge  \n",
       "129  B-Gendered-Pronoun  true positive  \n",
       "135  B-Gendered-Pronoun  true positive  \n",
       "261  B-Gendered-Pronoun  true positive  \n",
       "274  B-Gendered-Pronoun  true positive  \n",
       "292  B-Gendered-Pronoun  true positive  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_col = \"expected_tag\"\n",
    "pred_col = \"predicted_tag\"\n",
    "no_tag_value = \"O\"\n",
    "# Find true negatives based on the expected and predicted tags\n",
    "sub_exp_pred_df = exp_pred_df.loc[exp_pred_df[exp_col] == no_tag_value]\n",
    "sub_exp_pred_df = sub_exp_pred_df.loc[sub_exp_pred_df[pred_col].isna()]\n",
    "# sub_exp_pred_df.replace(to_replace=\"left_only\", value=\"true negative\", inplace=True)\n",
    "tn_tokens = list(sub_exp_pred_df[\"token_id\"])\n",
    "\n",
    "# Record false negatives, false positives, and true positives based on the merge values\n",
    "eval_df = exp_pred_df.loc[~exp_pred_df[\"token_id\"].isin(tn_tokens)]\n",
    "eval_df = eval_df.replace(to_replace=\"left_only\", value=\"false negative\")\n",
    "eval_df = eval_df.replace(to_replace=\"right_only\", value=\"false positive\")\n",
    "eval_df = eval_df.replace(to_replace=\"both\", value=\"true positive\")\n",
    "eval_df = eval_df.sort_index()\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9733, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strict Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the true positives, false positives, false negatives, precision, recall, and F1 metrics for all tags and each tag individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>1283</td>\n",
       "      <td>21</td>\n",
       "      <td>8429</td>\n",
       "      <td>0.462383</td>\n",
       "      <td>0.422989</td>\n",
       "      <td>0.422518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Generalization</td>\n",
       "      <td>322</td>\n",
       "      <td>11</td>\n",
       "      <td>788</td>\n",
       "      <td>0.986233</td>\n",
       "      <td>0.709910</td>\n",
       "      <td>0.825563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Generalization</td>\n",
       "      <td>246</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.015810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Gendered-Role</td>\n",
       "      <td>241</td>\n",
       "      <td>2</td>\n",
       "      <td>2266</td>\n",
       "      <td>0.999118</td>\n",
       "      <td>0.903869</td>\n",
       "      <td>0.949110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Gendered-Role</td>\n",
       "      <td>385</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>13802</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.997831</td>\n",
       "      <td>0.998734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Gendered-Pronoun</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag(s)  false negative  false positive  true positive  \\\n",
       "0                 all            1283              21           8429   \n",
       "0    B-Generalization             322              11            788   \n",
       "0    I-Generalization             246               3              2   \n",
       "0     B-Gendered-Role             241               2           2266   \n",
       "0     I-Gendered-Role             385               0              0   \n",
       "0  B-Gendered-Pronoun              30               5          13802   \n",
       "0  I-Gendered-Pronoun              59               0              0   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.462383  0.422989  0.422518  \n",
       "0   0.986233  0.709910  0.825563  \n",
       "0   0.400000  0.008065  0.015810  \n",
       "0   0.999118  0.903869  0.949110  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.999638  0.997831  0.998734  \n",
       "0   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agmt_stats = utils.getAgreementStatsForAllTags(eval_df, \"_merge\", \"token_id\", \"tag(s)\", y_dev, predictions)\n",
    "for label_tag in ling_label_subset:\n",
    "    label_agmt_stats = utils.getScoresByTags(eval_df, \"_merge\", [label_tag])\n",
    "    agmt_stats = pd.concat([agmt_stats, label_agmt_stats])\n",
    "agmt_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_stats.to_csv(config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_strict_agmt_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Annotation-level Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the manual annotations' offsets to the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = pd.read_csv(config.agg_path+\"aggregated_final.csv\")#, usecols=[\"description_id\",\"agg_ann_id\", \"ann_offsets\"])\n",
    "# Get only the Linguistic annotations\n",
    "annot_df = annot_df.loc[annot_df.category == \"Linguistic\"]\n",
    "annot_df = annot_df[[\"agg_ann_id\", \"ann_offsets\", \"label\"]]\n",
    "annot_df = annot_df.rename(columns={\"agg_ann_id\":\"ann_id\"})\n",
    "# annot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_token_ids = list(dev_data.token_id.unique())\n",
    "ling_dev_subset = ling_dev.loc[ling_dev.token_id.isin(dev_token_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11834, 9)\n",
      "(11834, 11)\n"
     ]
    }
   ],
   "source": [
    "to_add = ling_dev_subset[[\"ann_id\", \"token_id\", \"token_offsets\"]]\n",
    "# Only include annotations with Linguistic labels\n",
    "to_add = to_add.loc[to_add.ann_id.isin(list(annot_df.ann_id))]\n",
    "eval_df_joined = eval_df.join(to_add.set_index(\"token_id\"), on=\"token_id\", how=\"outer\")\n",
    "# Join on the left, as there will be annotations from outside the devtest set in annot_df\n",
    "print(eval_df_joined.shape)\n",
    "eval_df_joined = eval_df_joined.join(annot_df.set_index(\"ann_id\"), on=\"ann_id\", how=\"left\")\n",
    "print(eval_df_joined.shape)  # Looks good!  Same as before join.\n",
    "eval_df_joined = eval_df_joined.rename(columns={\"label\":\"expected_label\"})\n",
    "# eval_df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the predicted tags with their corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_joined.expected_label = eval_df_joined.expected_label.fillna(\"no_label\")\n",
    "eval_df_joined.expected_tag = eval_df_joined.expected_tag.fillna(\"no_label\")\n",
    "eval_df_joined.predicted_tag = eval_df_joined.predicted_tag.fillna(\"no_label\")\n",
    "# eval_df_joined.predicted_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = list(eval_df_joined.predicted_tag)\n",
    "predicted_labels = [tag[2:] if tag != \"no_label\" else tag for tag in predicted_labels]\n",
    "eval_df_joined.insert(len(eval_df_joined.columns), \"predicted_label\", predicted_labels)\n",
    "# eval_df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"sentence_id\", \"token_id\", \"token\", \"expected_label\", \"predicted_label\", \"_merge\", \"token_offsets\", \"ann_offsets\", \"ann_id\"]\n",
    "eval_by_ann = utils.implodeDataFrame(eval_df_joined[cols_to_keep], [\"ann_id\", \"ann_offsets\"]).reset_index()\n",
    "exp_labels = list(eval_by_ann[\"expected_label\"])\n",
    "exp_labels = [labels[0] for labels in exp_labels]\n",
    "eval_by_ann[\"expected_label\"] = exp_labels\n",
    "# eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert eval_by_ann.loc[eval_by_ann.expected_label == \"no_label\"].shape[0] == 0\n",
    "assert eval_by_ann.loc[eval_by_ann.expected_label.isna()].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row should have an annotation label (a Linguistic label in `expected_label`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_agmts = []\n",
    "token_agmts = (eval_by_ann[\"_merge\"])\n",
    "for agmts in token_agmts:\n",
    "    if \"true positive\" in agmts:\n",
    "        ann_agmt = \"true positive\"\n",
    "    elif \"false positive\" in agmts:\n",
    "        ann_agmt = \"false positive\"\n",
    "    else:\n",
    "        ann_agmt = \"false negative\"\n",
    "    ann_agmts += [ann_agmt]\n",
    "assert len(ann_agmts) == eval_by_ann.shape[0]\n",
    "eval_by_ann.insert(len(eval_by_ann.columns), \"annotation_agreement\", ann_agmts)\n",
    "# eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann[[\"ann_id\", \"ann_offsets\", \"token_id\", \"expected_label\", \"predicted_label\", \"annotation_agreement\"]].to_csv(\n",
    "    config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_annot_evaluation_ALLDATA.csv\".format(a=a,d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate annotation agreement metrics for each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_agmt = pd.DataFrame.from_dict({\n",
    "        \"label\":[], \"false negative\":[], \"false positive\":[],\n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914491</td>\n",
       "      <td>0.955336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>976.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.982906</td>\n",
       "      <td>0.190713</td>\n",
       "      <td>0.319444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>408.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>0.233083</td>\n",
       "      <td>0.376328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label  false negative  false positive  true positive  precision  \\\n",
       "0  Gendered-Pronoun           131.0             0.0         1401.0   1.000000   \n",
       "0     Gendered-Role           976.0             4.0          230.0   0.982906   \n",
       "0    Generalization           408.0             3.0          124.0   0.976378   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.914491  0.955336  \n",
       "0  0.190713  0.319444  \n",
       "0  0.233083  0.376328  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ling_label_tags.keys()\n",
    "for label in labels:\n",
    "    agmt_df = eval_by_ann.loc[eval_by_ann.expected_label == label]\n",
    "    tp = agmt_df.loc[agmt_df.annotation_agreement == \"true positive\"].shape[0]\n",
    "    fp = agmt_df.loc[agmt_df.annotation_agreement == \"false positive\"].shape[0]\n",
    "    fn = agmt_df.loc[agmt_df.annotation_agreement == \"false negative\"].shape[0]\n",
    "    prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "    label_agmt = pd.DataFrame.from_dict({\n",
    "            \"label\":[label], \"false negative\":[fn], \"false positive\":[fp],\n",
    "             \"true positive\":[tp], \"precision\":[prec], \"recall\":[rec], \"f1\":[f1]\n",
    "        })\n",
    "    annot_agmt = pd.concat([annot_agmt, label_agmt])\n",
    "annot_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Gendered-Role\"][\"annotation_agreement\"].value_counts()     # Looks good\n",
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Gendered-Pronoun\"][\"annotation_agreement\"].value_counts()  # Looks good\n",
    "# eval_by_ann.loc[eval_by_ann.expected_label == \"Generalization\"][\"annotation_agreement\"].value_counts()    # Looks good\n",
    "annot_agmt.to_csv(config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_annot_agmt_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loose Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each label, where a correct prediction is a prediction with the correct annotation label (not necessarily the correct IOB tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the evaluation DataFrame where tags are replaced by label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"rf\"\n",
    "eval_df = pd.read_csv(config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_ALLDATA.csv\".format(a=a,d=d), index_col=0)\n",
    "loose_eval_df = eval_df.copy()\n",
    "for label,tags in ling_label_tags.items():\n",
    "    for tag in tags:\n",
    "        loose_eval_df[\"expected_tag\"] = loose_eval_df[\"expected_tag\"].replace(to_replace=tag, value=label)\n",
    "        loose_eval_df[\"predicted_tag\"] = loose_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1283, 7)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose_eval_df.loc[loose_eval_df.predicted_tag.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df = loose_eval_df.fillna(\"O\")\n",
    "loose_eval_df = loose_eval_df.drop(columns=[\"_merge\"])\n",
    "loose_eval_df = utils.compareExpectedPredicted(loose_eval_df, \"_merge\", \"O\")\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df.to_csv(config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_loose_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt = pd.DataFrame.from_dict({\n",
    "        \"tag(s)\":[], \"false negative\":[], \"false positive\":[], \"true negative\":[], \n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13802.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.996786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>626.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2266.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783541</td>\n",
       "      <td>0.878635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>568.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>790.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.581738</td>\n",
       "      <td>0.735568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag(s)  false negative  false positive  true negative  \\\n",
       "0  Gendered-Pronoun            89.0             0.0            NaN   \n",
       "0     Gendered-Role           626.0             0.0            NaN   \n",
       "0    Generalization           568.0             0.0            NaN   \n",
       "\n",
       "   true positive  precision    recall        f1  \n",
       "0        13802.0        1.0  0.993593  0.996786  \n",
       "0         2266.0        1.0  0.783541  0.878635  \n",
       "0          790.0        1.0  0.581738  0.735568  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label,tags in ling_label_tags.items():\n",
    "    labels_agmt_stats = utils.getScoresByTags(loose_eval_df, \"_merge\", [label])\n",
    "    loose_agmt = pd.concat([loose_agmt, labels_agmt_stats])\n",
    "loose_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt.to_csv(config.experiment1_agmt_path+\"cc-{a}_baseline_fastText{d}_ling_loose_agmt_ALLDATA.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

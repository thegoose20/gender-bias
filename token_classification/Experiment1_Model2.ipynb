{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1, Model 2\n",
    "\n",
    "#### Model Setup\n",
    "\n",
    "Run models in the following order, using their output labels as features for the next model:\n",
    "\n",
    "1. Multilabel Linguistic Classifier\n",
    "2. Multiclass Person Name + Occupation Sequence Classifier\n",
    "3. Multilabel Stereotype + Omission Document Classifier\n",
    "\n",
    "***\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/experiment_input/`\n",
    "    * Prediction Data: Data: under directory `../data/token_clf_data/model_output/experiment1/`\n",
    "* Word Embeddings\n",
    "    * Custom fastText (word2vec with subwords) embeddings of 100 dimensions trained on the CRC Archives catalog's descriptive metadata (harvested October 2020)\n",
    "    \n",
    "***\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I.](#i) Person Name + Occupation Classifier\n",
    "* [Preprocessing](#prep)\n",
    "* [Training & Prediction](#tp)\n",
    "* [Evaluation](#eval)\n",
    "\n",
    "[II.](#ii) Predict Over All Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load programming resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, utils1, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For classification\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# For saving model\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define resources for the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(config.experiment_input_path).mkdir(parents=True, exist_ok=True)    # For train, devtest, and blind test data\n",
    "predictions_dir = config.experiment1_path+\"5fold/output/\"\n",
    "Path(predictions_dir).mkdir(parents=True, exist_ok=True)  # For predictions\n",
    "agreement_dir = config.experiment1_path+\"5fold/agreement/\"\n",
    "Path(agreement_dir).mkdir(parents=True, exist_ok=True)    # For agreement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_o_label_subset = [\n",
    "    \"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \n",
    "    \"B-Masculine\", \"I-Masculine\", \"B-Occupation\", \"I-Occupation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_label_tags = {\n",
    "    \"Gendered-Pronoun\": [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"], \"Gendered-Role\": [\"B-Gendered-Role\", \"I-Gendered-Role\"],\"Generalization\": [\"B-Generalization\", \"I-Generalization\"]\n",
    "    }\n",
    "pers_o_label_tags = {\n",
    "    \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "     \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100                   # dimensions of word embeddings (should match utils1.py) for file names\n",
    "target_labels = \"pers_o\"  # for file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"i\"></a>\n",
    "## I. Person Name + Occupation Labels\n",
    "\n",
    "Train a multiclass sequence classifier, using Conditional Random Field with Adaptive Regularization of Weight Vectors (AROW), on the Person Name and Occupation labels, **passing in the Linguistic labels (not specific BIO label-tag pair) from the previous model's predictions as features to this model.** The Person Name labels were assigned based on the presence or absence of gendered terminology referring to named people within a description, so the Linguistic labels passed as features will be rolled up to the description level. \n",
    "\n",
    "Multiclass is a suitable setup for these labels because they are mutually exclusive (no one token should have more than one of these labels).  The sequence classifier with AROW was the highest performing for past algorithm experiments with sequence classifiers for Person Name and Occupation labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Linguistic features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"rf\"\n",
    "# ling_filename = config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_ALLDATA.csv\".format(a=a,d=d)\n",
    "ling_filename = predictions_dir+\"cc-{a}_ling_baseline_fastText{d}_evaluation_loose.csv\".format(a=a, d=d)\n",
    "ling_eval_df = pd.read_csv(ling_filename, usecols=[\"description_id\", \"token_id\", \"predicted_tag\"])\n",
    "# Replace tags with labels\n",
    "for label,tags in ling_label_tags.items():\n",
    "    for tag in tags:\n",
    "        ling_eval_df[\"predicted_tag\"] = ling_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "ling_features = ling_eval_df.rename(columns={\"predicted_tag\":\"pred_ling_tag\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Linguistic predictions by description, removing duplicates from the list of Linguistic predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>description_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Gendered-Pronoun]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Gendered-Pronoun, Generalization]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Gendered-Pronoun, Gendered-Role, Generalization]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O]</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Gendered-Role]</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       pred_ling_tag  description_id\n",
       "0                                 [Gendered-Pronoun]               3\n",
       "1                 [Gendered-Pronoun, Generalization]               7\n",
       "2  [Gendered-Pronoun, Gendered-Role, Generalization]              11\n",
       "3                                                [O]             100\n",
       "4                                    [Gendered-Role]             155"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ling_features = utils.implodeDataFrame(ling_features, [\"description_id\"]).reset_index()\n",
    "col = \"pred_ling_tag\"\n",
    "pred_col = list(ling_features[col])\n",
    "# Remove duplicates\n",
    "unique_pred_col = [list(set(preds)) for preds in pred_col]\n",
    "# Remove \"O\" values\n",
    "unique_pred_col = [preds.remove(\"O\") if \"O\" in preds else preds for preds in unique_pred_col]\n",
    "# Sort the lists\n",
    "new_pred_col = []\n",
    "for preds in unique_pred_col:\n",
    "    if preds == None:\n",
    "        preds = [\"O\"]\n",
    "    else:\n",
    "        preds.sort()\n",
    "    new_pred_col += [preds]\n",
    "assert len(pred_col) == len(new_pred_col)\n",
    "ling_features = ling_features.drop(columns=[col, \"token_id\"])\n",
    "ling_features.insert((len(ling_features.columns)-1), col, new_pred_col)\n",
    "ling_features.head() # Want to keep desc ids in previous model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Gendered-Role]                                      893\n",
       "[Gendered-Pronoun]                                   697\n",
       "[Gendered-Pronoun, Gendered-Role]                    351\n",
       "[O]                                                  293\n",
       "[Generalization]                                     105\n",
       "[Gendered-Pronoun, Gendered-Role, Generalization]     58\n",
       "[Gendered-Pronoun, Generalization]                    44\n",
       "[Gendered-Role, Generalization]                       28\n",
       "Name: pred_ling_tag, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ling_features[col].value_counts()  # Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(779270, 10) (779270, 10)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(config.tokc_path+\"experiment_input/token_train.csv\", index_col=0)\n",
    "# dev_df =  pd.read_csv(config.tokc_path+\"experiment_input/token_validate.csv\", index_col=0)\n",
    "# test_df = pd.read_csv(config.tokc_path+\"experiment_input/token_test.csv\", index_col=0)\n",
    "df = pd.read_csv(config.tokc_path+\"experiment_input/token_5fold.csv\", index_col=0)\n",
    "perso_df = utils1.selectDataForLabels(df, \"tag\", pers_o_label_subset)\n",
    "# perso_train = utils1.selectDataForLabels(train_df, \"tag\", pers_o_label_subset)\n",
    "# perso_dev = utils1.selectDataForLabels(dev_df, \"tag\", pers_o_label_subset)\n",
    "# perso_test = utils1.selectDataForLabels(test_df, \"tag\", pers_o_label_subset)\n",
    "# print(perso_train.shape, perso_dev.shape, perso_test.shape)\n",
    "print(df.shape, perso_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the label associated with each annotation for future evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>14384</th>\n",
       "      <td>[7, 8, 9, 10, 11, 12]</td>\n",
       "      <td>[B-Unknown, I-Unknown, I-Unknown, I-Unknown, I...</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24275</th>\n",
       "      <td>[7, 8, 9, 10, 11, 12]</td>\n",
       "      <td>[B-Masculine, I-Masculine, I-Masculine, I-Masc...</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26233</th>\n",
       "      <td>[9, 10, 11, 12]</td>\n",
       "      <td>[B-Unknown, I-Unknown, I-Unknown, I-Unknown]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>14385</th>\n",
       "      <td>[113, 114, 115, 116]</td>\n",
       "      <td>[B-Masculine, I-Masculine, I-Masculine, I-Masc...</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14386</th>\n",
       "      <td>[178, 179]</td>\n",
       "      <td>[B-Masculine, I-Masculine]</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    token_id  \\\n",
       "description_id ann_id                          \n",
       "1              14384   [7, 8, 9, 10, 11, 12]   \n",
       "               24275   [7, 8, 9, 10, 11, 12]   \n",
       "               26233         [9, 10, 11, 12]   \n",
       "3              14385    [113, 114, 115, 116]   \n",
       "               14386              [178, 179]   \n",
       "\n",
       "                                                                     tag  \\\n",
       "description_id ann_id                                                      \n",
       "1              14384   [B-Unknown, I-Unknown, I-Unknown, I-Unknown, I...   \n",
       "               24275   [B-Masculine, I-Masculine, I-Masculine, I-Masc...   \n",
       "               26233        [B-Unknown, I-Unknown, I-Unknown, I-Unknown]   \n",
       "3              14385   [B-Masculine, I-Masculine, I-Masculine, I-Masc...   \n",
       "               14386                          [B-Masculine, I-Masculine]   \n",
       "\n",
       "                      expected_label  \n",
       "description_id ann_id                 \n",
       "1              14384         Unknown  \n",
       "               24275       Masculine  \n",
       "               26233         Unknown  \n",
       "3              14385       Masculine  \n",
       "               14386       Masculine  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_ann = pd.read_csv(config.tokc_path+\"experiment_input/token_5fold.csv\", usecols=[\"description_id\", \"ann_id\", \"token_id\", \"tag\"])\n",
    "df_by_ann = df_by_ann.drop_duplicates()\n",
    "df_by_ann = utils.implodeDataFrame(df_by_ann, [\"description_id\", \"ann_id\"])\n",
    "tags_col = list(df_by_ann.tag)\n",
    "labels = [[tag[2:] if tag != \"O\" else tag for tag in tags] for tags in tags_col]\n",
    "labels = [label_list[0] for label_list in labels]\n",
    "df_by_ann.insert(len(df_by_ann.columns), \"expected_label\", labels)\n",
    "perso_labels = list(pers_o_label_tags.keys())\n",
    "df_by_ann = df_by_ann.loc[df_by_ann.expected_label.isin(perso_labels)]\n",
    "df_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the linguistic labels of descriptions (which will be passed to the model as features) to the model input and evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                                                579586\n",
       "Gendered-Pronoun                                  52931\n",
       "Gendered-Role                                     52162\n",
       "Gendered-Pronoun,Gendered-Role                    51128\n",
       "Gendered-Pronoun,Gendered-Role,Generalization     23605\n",
       "Gendered-Pronoun,Generalization                    8846\n",
       "Gendered-Role,Generalization                       5717\n",
       "Generalization                                     5295\n",
       "Name: pred_ling_tag, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perso_train = perso_train.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "# assert perso_train.loc[perso_train.tag.isna()].shape[0] == 0\n",
    "# # perso_train.head()\n",
    "# perso_dev = perso_dev.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "# assert perso_dev.loc[perso_dev.tag.isna()].shape[0] == 0\n",
    "# perso_test = perso_test.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "# assert perso_test.loc[perso_test.tag.isna()].shape[0] == 0\n",
    "# ---------------------------------------------\n",
    "perso_df = perso_df.join(ling_features.set_index(\"description_id\"), on=\"description_id\", how=\"left\")\n",
    "assert perso_df.loc[perso_df.tag.isna()].shape[0] == 0\n",
    "perso_df.pred_ling_tag = perso_df.pred_ling_tag.fillna(\"O\")\n",
    "feature_col = list(perso_df.pred_ling_tag)\n",
    "new_feature_col = []\n",
    "for preds in feature_col:\n",
    "    if preds != \"O\":\n",
    "        preds.sort()\n",
    "        preds = \",\".join(preds)\n",
    "    new_feature_col += [preds]\n",
    "perso_df = perso_df.drop(columns=[\"pred_ling_tag\"])\n",
    "perso_df.insert(2, \"pred_ling_tag\", new_feature_col)\n",
    "perso_df.pred_ling_tag.value_counts() # Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perso_train.pred_ling_tag = perso_train.pred_ling_tag.fillna(\"O\")\n",
    "# perso_train.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perso_dev.pred_ling_tag = perso_dev.pred_ling_tag.fillna(\"O\")\n",
    "# perso_dev.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perso_test.pred_ling_tag = perso_test.pred_ling_tag.fillna(\"O\")\n",
    "# perso_test.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prep\"></a>\n",
    "### Preprocessing\n",
    "\n",
    "Group data by token and then by sentence, so each sentence is a list of tokens and each token has a list of tags associated with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = perso_train.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# dev_df = perso_dev.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# test_df = perso_test.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# ---------------------------------------------\n",
    "perso_data = perso_df.drop(columns=[\"ann_id\", \"token_offsets\", \"field\", \"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "perso_token_groups = utils.implodeDataFrame(perso_data, [\"description_id\", \"token_id\", \"sentence_id\", \"token\", \"fold\", col]).reset_index()\n",
    "# perso_token_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42030, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>split4</td>\n",
       "      <td>O</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Identifier, :, AA5]</td>\n",
       "      <td>[[O], [O], [O]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>split2</td>\n",
       "      <td>O</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n",
       "      <td>[Title, :, Papers, of, The, Very, Rev, Prof, J...</td>\n",
       "      <td>[[O], [O], [O], [O], [O, B-Unknown, B-Masculin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>split1</td>\n",
       "      <td>O</td>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>split2</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>[109, 110, 111, 112, 113, 114, 115, 116, 117, ...</td>\n",
       "      <td>[Biographical, /, Historical, :, Professor, Ja...</td>\n",
       "      <td>[[O], [O], [O], [O], [B-Masculine], [I-Masculi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>split4</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>[134, 135, 136, 137, 138, 139, 140, 141, 142, ...</td>\n",
       "      <td>[He, was, educated, at, Daniel, Stewart, 's, C...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id    fold     pred_ling_tag  \\\n",
       "0               0            0  split4                 O   \n",
       "1               1            1  split2                 O   \n",
       "2               2            2  split1                 O   \n",
       "3               3            3  split2  Gendered-Pronoun   \n",
       "4               3            4  split4  Gendered-Pronoun   \n",
       "\n",
       "                                            token_id  \\\n",
       "0                                          [0, 1, 2]   \n",
       "1      [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]   \n",
       "2  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...   \n",
       "3  [109, 110, 111, 112, 113, 114, 115, 116, 117, ...   \n",
       "4  [134, 135, 136, 137, 138, 139, 140, 141, 142, ...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0                               [Identifier, :, AA5]   \n",
       "1  [Title, :, Papers, of, The, Very, Rev, Prof, J...   \n",
       "2  [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "3  [Biographical, /, Historical, :, Professor, Ja...   \n",
       "4  [He, was, educated, at, Daniel, Stewart, 's, C...   \n",
       "\n",
       "                                                 tag  \n",
       "0                                    [[O], [O], [O]]  \n",
       "1  [[O], [O], [O], [O], [O, B-Unknown, B-Masculin...  \n",
       "2  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  \n",
       "3  [[O], [O], [O], [O], [B-Masculine], [I-Masculi...  \n",
       "4  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perso_grouped = utils.implodeDataFrame(perso_token_groups, [\"description_id\", \"sentence_id\", \"fold\", col]).reset_index()\n",
    "perso_grouped = perso_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "print(perso_grouped.shape)\n",
    "perso_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                                                34800\n",
       "Gendered-Role                                     2115\n",
       "Gendered-Pronoun                                  1858\n",
       "Gendered-Pronoun,Gendered-Role                    1696\n",
       "Gendered-Pronoun,Gendered-Role,Generalization      779\n",
       "Gendered-Pronoun,Generalization                    367\n",
       "Generalization                                     236\n",
       "Gendered-Role,Generalization                       179\n",
       "Name: pred_ling_tag, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perso_grouped[col].value_counts()  # Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the five groups of training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['split0' 'split1' 'split2' 'split3' 'split4']\n"
     ]
    }
   ],
   "source": [
    "split_col = \"fold\"\n",
    "splits = perso_grouped[split_col].unique()\n",
    "splits.sort()\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0, test0 = list(splits[:4]), splits[4]\n",
    "train1, test1 = list(splits[1:]), splits[0]\n",
    "train2, test2 = list(splits[2:])+[splits[0]], splits[1]\n",
    "train3, test3 = list(splits[3:])+list(splits[:2]), splits[2]\n",
    "train4, test4 = [splits[4]]+list(splits[:3]), splits[3]\n",
    "runs = [(train0, test0), (train1, test1), (train2, test2), (train3, test3), (train4, test4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_token_groups = utils.implodeDataFrame(train_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_train_token_groups = df_train_token_groups.reset_index()\n",
    "# # df_train_token_groups.head()\n",
    "# df_train_grouped = utils.implodeDataFrame(df_train_token_groups, ['sentence_id'])\n",
    "# df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_train_grouped = df_train_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev_token_groups = utils.implodeDataFrame(dev_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_dev_token_groups = df_dev_token_groups.reset_index()\n",
    "# df_dev_grouped = utils.implodeDataFrame(df_dev_token_groups, ['sentence_id'])\n",
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_dev_grouped = df_dev_grouped.reset_index()\n",
    "# # df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_token_groups = utils.implodeDataFrame(test_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_test_token_groups = df_test_token_groups.reset_index()\n",
    "# df_test_grouped = utils.implodeDataFrame(df_test_token_groups, ['sentence_id'])\n",
    "# df_test_grouped = df_test_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_test_grouped = df_test_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the linguistic label and BIO tags together with the tokens so each sentence item is a tuple: `(TOKEN, LING_LABEL, TAG_LIST)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentences_perso = utils1.zip2FeaturesAndTarget(df_train_grouped, \"tag\")\n",
    "# print(train_sentences_perso[2][:3])\n",
    "# dev_sentences_perso = utils1.zip2FeaturesAndTarget(df_dev_grouped, \"tag\")\n",
    "# print(dev_sentences_perso[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentences = train_sentences_perso\n",
    "# dev_sentences = dev_sentences_perso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Features\n",
    "# X_train = [utils1.extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "# X_dev = [utils1.extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# # Target\n",
    "# y_train = [utils1.extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "# y_dev = [utils1.extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tp\"></a>\n",
    "### Training & Prediction\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Person Name** category of tags.  We'll set the max iterations to 100 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: ['split4', 'split0', 'split1', 'split2']\n",
      "Predicting on: split3\n",
      "Predictions for split3 saved!\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "\n",
    "# Specify the run one at a time (with for loop, model remembers what it learned in previous runs)\n",
    "run = runs[4]  # 0, 1, 2, 3 \n",
    "\n",
    "# Get the train (80%) and test (20%) subsets of data\n",
    "# with Person Name tags as targets and Linguistic labels as features\n",
    "train_splits, test_split = run[0], run[1]\n",
    "print(\"Training on:\", train_splits)\n",
    "train_df = perso_grouped.loc[perso_grouped[split_col].isin(train_splits)]\n",
    "dev_df = perso_grouped.loc[perso_grouped[split_col] == test_split]\n",
    "\n",
    "# Zip the linguistic label and BIO tags together with the tokens so each \n",
    "# sentence item is a tuple: `(TOKEN, LING_LABEL(S), TAG_LIST)`\n",
    "train_sentences = utils1.zip2FeaturesAndTarget(train_df, \"tag\")\n",
    "dev_sentences = utils1.zip2FeaturesAndTarget(dev_df, \"tag\")\n",
    "\n",
    "# Extract features\n",
    "X_train = [utils1.extractSentenceFeatures(s) for s in train_sentences] \n",
    "X_dev = [utils1.extractSentenceFeatures(s) for s in dev_sentences]\n",
    "\n",
    "# Extract targets\n",
    "y_train = [utils1.extractSentenceTargets(s) for s in train_sentences]\n",
    "y_dev = [utils1.extractSentenceTargets(s) for s in dev_sentences]\n",
    "\n",
    "# Train a classification model\n",
    "a = \"arow\"\n",
    "clf = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict with the trained model\n",
    "print(\"Predicting on:\", test_split)\n",
    "predictions = clf.predict(X_dev)\n",
    "\n",
    "dev_df = dev_df.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "dev_df.insert(len(dev_df.columns), \"tag_pers_o_predicted\", predictions)\n",
    "dev_df = dev_df.set_index([\"description_id\", \"sentence_id\", \"fold\", col])\n",
    "dev_df_exploded = dev_df.explode(list(dev_df.columns))\n",
    "\n",
    "if pred_df.shape[0] > 0:\n",
    "    pred_df = pd.concat([pred_df, dev_df_exploded])\n",
    "else:\n",
    "    pred_df = dev_df_exploded\n",
    "\n",
    "assert pred_df.loc[pred_df[\"tag_pers_o_expected\"].isna()].shape[0] == 0, \"Any NaN values should be replaced with 'O'\"\n",
    "\n",
    "filename = \"crf_{a}_{t}_baseline_fastText{d}_predictions_{s}.csv\".format(a=a, t=target_labels, d=d, s=test_split)\n",
    "pred_df.to_csv(predictions_dir+filename)\n",
    "\n",
    "print(\"Predictions for {} saved!\".format(test_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model (the last model run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/experiment1/crf-arow_pn_F-fastText100Ling_T-PNOcc.joblib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"models/experiment1/\"\n",
    "Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "filename = model_dir+\"crf-{a}_pn_F-fastText{d}Ling_T-PNOcc.joblib\".format(a=a, d=d)  # include features (F) and targets (T) in model's file name\n",
    "dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(753521, 7)\n"
     ]
    }
   ],
   "source": [
    "pred_df0 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split0.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df1 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split1.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df2 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split2.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df3 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split3.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df4 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split4.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_perso = pd.concat([pred_df0, pred_df1, pred_df2, pred_df3, pred_df4])\n",
    "print(pred_perso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[B-Masculine]</td>\n",
       "      <td>B-Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[I-Masculine]</td>\n",
       "      <td>I-Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id    fold     pred_ling_tag  token_id sentence  \\\n",
       "0               3            8  split0  Gendered-Pronoun       233    James   \n",
       "1               3            8  split0  Gendered-Pronoun       234    Whyte   \n",
       "2               3            8  split0  Gendered-Pronoun       235      was   \n",
       "3               3            8  split0  Gendered-Pronoun       236   called   \n",
       "4               3            8  split0  Gendered-Pronoun       237     upon   \n",
       "\n",
       "  tag_pers_o_expected tag_pers_o_predicted  \n",
       "0       [B-Masculine]          B-Masculine  \n",
       "1       [I-Masculine]            I-Unknown  \n",
       "2                 [O]                    O  \n",
       "3                 [O]                    O  \n",
       "4                 [O]                    O  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso = pred_perso.reset_index()\n",
    "pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_pers_o_expected\")\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                                                565806\n",
       "Gendered-Pronoun                                  50153\n",
       "Gendered-Role                                     49046\n",
       "Gendered-Pronoun,Gendered-Role                    47276\n",
       "Gendered-Pronoun,Gendered-Role,Generalization     22086\n",
       "Gendered-Pronoun,Generalization                    8657\n",
       "Gendered-Role,Generalization                       5457\n",
       "Generalization                                     5040\n",
       "Name: pred_ling_tag, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert pred_perso.shape[0] == len(pred_perso.token_id.unique()), \"There should be one row per token.\"\n",
    "pred_perso.pred_ling_tag.value_counts()  # Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = \"arow\"\n",
    "# clf_perso = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "# try:\n",
    "#     clf_perso.fit(X_train, y_train)\n",
    "# except AttributeError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = list(clf_perso.classes_)\n",
    "# targets.remove('O')\n",
    "# print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = clf_perso.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"  - F1:\", metrics.flat_f1_score(y_dev, predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Prec:\", metrics.flat_precision_score(y_dev, predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Rec\", metrics.flat_recall_score(y_dev, predictions, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "# df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_pers_o_predicted\", predictions)\n",
    "# # df_dev_grouped.head()\n",
    "# df_dev_grouped = df_dev_grouped.set_index(\"sentence_id\")\n",
    "# df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns))\n",
    "# # df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"crf_{a}_{t}_baseline_fastText{d}_predictions.csv\".format(a=a, t=target_labels, d=d)\n",
    "# df_dev_exploded.to_csv(config.experiment1_output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval\"></a>\n",
    "### Evaluation\n",
    "#### Evaluate: Strict, Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"crf_{a}_{c}_baseline_fastText{d}_predictions.csv\".format(a=a, c=category, d=d)\n",
    "# pred_perso = pd.read_csv(config.experiment1_output_path+filename)\n",
    "# pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_{}_expected\".format(category))\n",
    "# # pred_pers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics for each category of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso = pred_perso.fillna(\"O\")\n",
    "pred_perso = utils.isPredictedInExpected(pred_perso, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the combined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_{c}_baseline_fastText{d}_predictions.csv\".format(a=a, c=category, d=d)\n",
    "pred_perso.to_csv(predictions_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>2143</td>\n",
       "      <td>2734</td>\n",
       "      <td>3605</td>\n",
       "      <td>0.568702</td>\n",
       "      <td>0.627175</td>\n",
       "      <td>0.596509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>4441</td>\n",
       "      <td>5738</td>\n",
       "      <td>6497</td>\n",
       "      <td>0.531018</td>\n",
       "      <td>0.593984</td>\n",
       "      <td>0.560739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>90</td>\n",
       "      <td>375</td>\n",
       "      <td>684</td>\n",
       "      <td>0.645892</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.746318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>449</td>\n",
       "      <td>1353</td>\n",
       "      <td>1420</td>\n",
       "      <td>0.512081</td>\n",
       "      <td>0.759765</td>\n",
       "      <td>0.611805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>642</td>\n",
       "      <td>1498</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.570281</td>\n",
       "      <td>0.755894</td>\n",
       "      <td>0.650098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>1430</td>\n",
       "      <td>4234</td>\n",
       "      <td>2273</td>\n",
       "      <td>0.349316</td>\n",
       "      <td>0.613827</td>\n",
       "      <td>0.445250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>1023</td>\n",
       "      <td>950</td>\n",
       "      <td>1534</td>\n",
       "      <td>0.617552</td>\n",
       "      <td>0.599922</td>\n",
       "      <td>0.608609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>1700</td>\n",
       "      <td>1456</td>\n",
       "      <td>1448</td>\n",
       "      <td>0.498623</td>\n",
       "      <td>0.459975</td>\n",
       "      <td>0.478519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     B-Unknown            2143            2734           3605   0.568702   \n",
       "0     I-Unknown            4441            5738           6497   0.531018   \n",
       "0    B-Feminine              90             375            684   0.645892   \n",
       "0    I-Feminine             449            1353           1420   0.512081   \n",
       "0   B-Masculine             642            1498           1988   0.570281   \n",
       "0   I-Masculine            1430            4234           2273   0.349316   \n",
       "0  B-Occupation            1023             950           1534   0.617552   \n",
       "0  I-Occupation            1700            1456           1448   0.498623   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.627175  0.596509  \n",
       "0  0.593984  0.560739  \n",
       "0  0.883721  0.746318  \n",
       "0  0.759765  0.611805  \n",
       "0  0.755894  0.650098  \n",
       "0  0.613827  0.445250  \n",
       "0  0.599922  0.608609  \n",
       "0  0.459975  0.478519  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso, \"_merge\", pers_o_label_subset[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(pers_o_label_subset)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso, \"_merge\", pers_o_label_subset[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_strict_agmt.csv\".format(a=a, c=category, d=d)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_{c}_baseline_fastText{d}_strict_agmt.csv\".format(a=a, c=category, d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Annotation Agreement\n",
    "\n",
    "Calculate agreement at the annotation level, so if the model labels any word correctly from a manually annotated text span, that annotation is recorded as being correctly labeled (`true positive`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the annotation data by token:\n",
    "\n",
    "*Note: `ann_id` of `9999` indicates no annotation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>8</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>9</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>10</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>11</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  ann_id token_id expected_tag expected_label\n",
       "0               1   14384        7    B-Unknown        Unknown\n",
       "1               1   14384        8    I-Unknown        Unknown\n",
       "2               1   14384        9    I-Unknown        Unknown\n",
       "3               1   14384       10    I-Unknown        Unknown\n",
       "4               1   14384       11    I-Unknown        Unknown"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_ann = df_by_ann.explode([\"token_id\", \"tag\"])\n",
    "df_by_ann = df_by_ann.rename(columns={\"tag\": \"expected_tag\"})\n",
    "df_by_ann = df_by_ann.reset_index()\n",
    "df_by_ann.head()\n",
    "# # df_by_ann.expected_label.value_counts()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the columns of the dev and prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename `sentence` column `token`\n",
    "pred_perso = pred_perso.rename(columns={\"sentence\":\"token\"})\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data, adding the annotation IDs (`ann_id` column) to the prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [\"description_id\", \"token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14387.0</td>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "      <td>14387.0</td>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id    fold     pred_ling_tag  token_id   token  \\\n",
       "0               3            8  split0  Gendered-Pronoun       233   James   \n",
       "1               3            8  split0  Gendered-Pronoun       234   Whyte   \n",
       "2               3            8  split0  Gendered-Pronoun       235     was   \n",
       "3               3            8  split0  Gendered-Pronoun       236  called   \n",
       "4               3            8  split0  Gendered-Pronoun       237    upon   \n",
       "\n",
       "  tag_pers_o_predicted          _merge   ann_id tag_pers_o_expected  \\\n",
       "0          B-Masculine   true positive  14387.0         B-Masculine   \n",
       "1            I-Unknown  false positive  14387.0         I-Masculine   \n",
       "2                    O   true negative  99999.0                   O   \n",
       "3                    O   true negative  99999.0                   O   \n",
       "4                    O   true negative  99999.0                   O   \n",
       "\n",
       "  expected_label  \n",
       "0      Masculine  \n",
       "1      Masculine  \n",
       "2              O  \n",
       "3              O  \n",
       "4              O  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann = pred_perso.join(df_by_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category)])  # duplicate of expected_tag\n",
    "pred_perso_ann = pred_perso_ann.rename(columns={\"expected_tag\":\"tag_{}_expected\".format(category)})\n",
    "pred_perso_ann[\"ann_id\"] = pred_perso_ann[\"ann_id\"].fillna(99999)\n",
    "pred_perso_ann[\"tag_pers_o_expected\"] = pred_perso_ann[\"tag_pers_o_expected\"].fillna(\"O\")\n",
    "pred_perso_ann[\"expected_label\"] = pred_perso_ann[\"expected_label\"].fillna(\"O\")\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_expected\"].isna()].shape[0] == 0\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the predicted BIO tags to label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "pred_labels = list(pred_perso_ann[\"tag_{}_predicted\".format(category)])\n",
    "pred_labels = [label if label == \"O\" else label[2:] for label in pred_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_predicted\".format(category), pred_labels)\n",
    "# Rename the expected labels column name\n",
    "pred_perso_ann = pred_perso_ann.rename(columns={\"expected_label\":\"label_{}_expected\".format(category)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>split4</td>\n",
       "      <td>O</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Identifier, :, AA5]</td>\n",
       "      <td>[true negative, true negative, true negative]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384.0</td>\n",
       "      <td>split2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12]</td>\n",
       "      <td>[The, Very, Rev, Prof, James, Whyte]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Masculine, Unknown, Unknown, Unknown, Unknown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24275.0</td>\n",
       "      <td>split2</td>\n",
       "      <td>Masculine</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12]</td>\n",
       "      <td>[The, Very, Rev, Prof, James, Whyte]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Masculine, Unknown, Unknown, Unknown, Unknown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26233.0</td>\n",
       "      <td>split2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "      <td>[9, 10, 11, 12]</td>\n",
       "      <td>[Rev, Prof, James, Whyte]</td>\n",
       "      <td>[true positive, false positive, true positive,...</td>\n",
       "      <td>[Unknown, Unknown, Unknown, Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>split2</td>\n",
       "      <td>O</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>[3, 4, 5, 6, 13, 14, 15]</td>\n",
       "      <td>[Title, :, Papers, of, (, 1920-2005, )]</td>\n",
       "      <td>[true negative, true negative, true negative, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id   ann_id    fold label_pers_o_expected  \\\n",
       "0               0            0  99999.0  split4                     O   \n",
       "1               1            1  14384.0  split2               Unknown   \n",
       "2               1            1  24275.0  split2             Masculine   \n",
       "3               1            1  26233.0  split2               Unknown   \n",
       "4               1            1  99999.0  split2                     O   \n",
       "\n",
       "           pred_ling_tag                  token_id  \\\n",
       "0              [O, O, O]                 [0, 1, 2]   \n",
       "1     [O, O, O, O, O, O]     [7, 8, 9, 10, 11, 12]   \n",
       "2     [O, O, O, O, O, O]     [7, 8, 9, 10, 11, 12]   \n",
       "3           [O, O, O, O]           [9, 10, 11, 12]   \n",
       "4  [O, O, O, O, O, O, O]  [3, 4, 5, 6, 13, 14, 15]   \n",
       "\n",
       "                                     token  \\\n",
       "0                     [Identifier, :, AA5]   \n",
       "1     [The, Very, Rev, Prof, James, Whyte]   \n",
       "2     [The, Very, Rev, Prof, James, Whyte]   \n",
       "3                [Rev, Prof, James, Whyte]   \n",
       "4  [Title, :, Papers, of, (, 1920-2005, )]   \n",
       "\n",
       "                                              _merge  \\\n",
       "0      [true negative, true negative, true negative]   \n",
       "1  [true positive, true positive, true positive, ...   \n",
       "2  [true positive, true positive, true positive, ...   \n",
       "3  [true positive, false positive, true positive,...   \n",
       "4  [true negative, true negative, true negative, ...   \n",
       "\n",
       "                              label_pers_o_predicted  \n",
       "0                                          [O, O, O]  \n",
       "1  [Masculine, Unknown, Unknown, Unknown, Unknown...  \n",
       "2  [Masculine, Unknown, Unknown, Unknown, Unknown...  \n",
       "3               [Unknown, Unknown, Unknown, Unknown]  \n",
       "4                              [O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_ann = utils.implodeDataFrame(pred_perso_ann, [\"description_id\", \"sentence_id\", \"ann_id\", \"fold\", \"label_{}_expected\".format(category)])\n",
    "pred_perso_ann = pred_perso_ann.reset_index()\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out the expected unannotated data (`ann_id` of `99999`) from the expected annotated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712167, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>split4</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>split4</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>split4</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>split2</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>split2</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id   ann_id    fold label_pers_o_expected  \\\n",
       "0               0            0  99999.0  split4                     O   \n",
       "0               0            0  99999.0  split4                     O   \n",
       "0               0            0  99999.0  split4                     O   \n",
       "4               1            1  99999.0  split2                     O   \n",
       "4               1            1  99999.0  split2                     O   \n",
       "\n",
       "  pred_ling_tag token_id       token         _merge label_pers_o_predicted  \n",
       "0             O        0  Identifier  true negative                      O  \n",
       "0             O        1           :  true negative                      O  \n",
       "0             O        2         AA5  true negative                      O  \n",
       "4             O        3       Title  true negative                      O  \n",
       "4             O        4           :  true negative                      O  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_df = pred_perso_ann.loc[pred_perso_ann.ann_id == 99999]\n",
    "fp_df = fp_df.explode([\"token_id\", \"token\", \"pred_ling_tag\", \"_merge\", \"label_{}_predicted\".format(category)])\n",
    "print(fp_df.shape)\n",
    "fp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.loc[pred_perso_ann.ann_id != 99999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann = pd.concat([pred_perso_ann, fp_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get unique lists of predicted labels per row, removing `O` values from lists with Linguistic labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueLabels(label_list):\n",
    "    final_labels = []\n",
    "    for labels in label_list:\n",
    "        if (len(labels) > 1) and (\"O\" in labels):\n",
    "            labels.remove(\"O\")\n",
    "        final_labels += [labels]\n",
    "    assert len(final_labels) == len(label_list), \"There should be the same number of sub-lists in final_labels and label_list.\"\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14384.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>split2</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12]</td>\n",
       "      <td>[The, Very, Rev, Prof, James, Whyte]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Masculine, Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24275.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>split2</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12]</td>\n",
       "      <td>[The, Very, Rev, Prof, James, Whyte]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Masculine</td>\n",
       "      <td>[Masculine, Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26233.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>split2</td>\n",
       "      <td>[9, 10, 11, 12]</td>\n",
       "      <td>[Rev, Prof, James, Whyte]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "      <td>[true positive, false positive, true positive,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14385.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>split2</td>\n",
       "      <td>[113, 114, 115, 116]</td>\n",
       "      <td>[Professor, James, Aitken, White]</td>\n",
       "      <td>[Gendered-Pronoun, Gendered-Pronoun, Gendered-...</td>\n",
       "      <td>[true positive, true positive, false positive,...</td>\n",
       "      <td>Masculine</td>\n",
       "      <td>[Masculine, Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41260.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>split2</td>\n",
       "      <td>[120, 121]</td>\n",
       "      <td>[Scottish, Theologian]</td>\n",
       "      <td>[Gendered-Pronoun, Gendered-Pronoun]</td>\n",
       "      <td>[true negative, true negative]</td>\n",
       "      <td>Occupation</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ann_id  description_id  sentence_id    fold               token_id  \\\n",
       "1  14384.0               1            1  split2  [7, 8, 9, 10, 11, 12]   \n",
       "2  24275.0               1            1  split2  [7, 8, 9, 10, 11, 12]   \n",
       "3  26233.0               1            1  split2        [9, 10, 11, 12]   \n",
       "6  14385.0               3            3  split2   [113, 114, 115, 116]   \n",
       "7  41260.0               3            3  split2             [120, 121]   \n",
       "\n",
       "                                  token  \\\n",
       "1  [The, Very, Rev, Prof, James, Whyte]   \n",
       "2  [The, Very, Rev, Prof, James, Whyte]   \n",
       "3             [Rev, Prof, James, Whyte]   \n",
       "6     [Professor, James, Aitken, White]   \n",
       "7                [Scottish, Theologian]   \n",
       "\n",
       "                                       pred_ling_tag  \\\n",
       "1                                 [O, O, O, O, O, O]   \n",
       "2                                 [O, O, O, O, O, O]   \n",
       "3                                       [O, O, O, O]   \n",
       "6  [Gendered-Pronoun, Gendered-Pronoun, Gendered-...   \n",
       "7               [Gendered-Pronoun, Gendered-Pronoun]   \n",
       "\n",
       "                                              _merge label_pers_o_expected  \\\n",
       "1  [true positive, true positive, true positive, ...               Unknown   \n",
       "2  [true positive, true positive, true positive, ...             Masculine   \n",
       "3  [true positive, false positive, true positive,...               Unknown   \n",
       "6  [true positive, true positive, false positive,...             Masculine   \n",
       "7                     [true negative, true negative]            Occupation   \n",
       "\n",
       "  label_pers_o_predicted  \n",
       "1   [Masculine, Unknown]  \n",
       "2   [Masculine, Unknown]  \n",
       "3              [Unknown]  \n",
       "6   [Masculine, Unknown]  \n",
       "7                    [O]  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = list(eval_by_ann[\"label_{}_predicted\".format(category)])\n",
    "predicted_labels = [list(set(predictions)) for predictions in predicted_labels]\n",
    "predicted_unique = getUniqueLabels(predicted_labels)\n",
    "# predicted_unique[:5]  # Looks good\n",
    "# for pred in predicted_unique:\n",
    "#     if len(pred) > 1:\n",
    "#         print(\"Multi-item lists exist\")\n",
    "#         break\n",
    "eval_by_ann[\"label_{}_predicted\".format(category)] = predicted_unique\n",
    "# Reorder columns:\n",
    "eval_by_ann = eval_by_ann[\n",
    "    [\"ann_id\", \"description_id\", \"sentence_id\", \"fold\", \"token_id\", \"token\", \"pred_ling_tag\", \"_merge\", \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category)]\n",
    "]\n",
    "eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreements and disagreements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "      <th>annotation_agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14384.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>split2</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12]</td>\n",
       "      <td>[The, Very, Rev, Prof, James, Whyte]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Masculine, Unknown]</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24275.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>split2</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12]</td>\n",
       "      <td>[The, Very, Rev, Prof, James, Whyte]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Masculine</td>\n",
       "      <td>[Masculine, Unknown]</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26233.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>split2</td>\n",
       "      <td>[9, 10, 11, 12]</td>\n",
       "      <td>[Rev, Prof, James, Whyte]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "      <td>[true positive, false positive, true positive,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Unknown]</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14385.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>split2</td>\n",
       "      <td>[113, 114, 115, 116]</td>\n",
       "      <td>[Professor, James, Aitken, White]</td>\n",
       "      <td>[Gendered-Pronoun, Gendered-Pronoun, Gendered-...</td>\n",
       "      <td>[true positive, true positive, false positive,...</td>\n",
       "      <td>Masculine</td>\n",
       "      <td>[Masculine, Unknown]</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41260.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>split2</td>\n",
       "      <td>[120, 121]</td>\n",
       "      <td>[Scottish, Theologian]</td>\n",
       "      <td>[Gendered-Pronoun, Gendered-Pronoun]</td>\n",
       "      <td>[true negative, true negative]</td>\n",
       "      <td>Occupation</td>\n",
       "      <td>[O]</td>\n",
       "      <td>false negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ann_id  description_id  sentence_id    fold               token_id  \\\n",
       "1  14384.0               1            1  split2  [7, 8, 9, 10, 11, 12]   \n",
       "2  24275.0               1            1  split2  [7, 8, 9, 10, 11, 12]   \n",
       "3  26233.0               1            1  split2        [9, 10, 11, 12]   \n",
       "6  14385.0               3            3  split2   [113, 114, 115, 116]   \n",
       "7  41260.0               3            3  split2             [120, 121]   \n",
       "\n",
       "                                  token  \\\n",
       "1  [The, Very, Rev, Prof, James, Whyte]   \n",
       "2  [The, Very, Rev, Prof, James, Whyte]   \n",
       "3             [Rev, Prof, James, Whyte]   \n",
       "6     [Professor, James, Aitken, White]   \n",
       "7                [Scottish, Theologian]   \n",
       "\n",
       "                                       pred_ling_tag  \\\n",
       "1                                 [O, O, O, O, O, O]   \n",
       "2                                 [O, O, O, O, O, O]   \n",
       "3                                       [O, O, O, O]   \n",
       "6  [Gendered-Pronoun, Gendered-Pronoun, Gendered-...   \n",
       "7               [Gendered-Pronoun, Gendered-Pronoun]   \n",
       "\n",
       "                                              _merge label_pers_o_expected  \\\n",
       "1  [true positive, true positive, true positive, ...               Unknown   \n",
       "2  [true positive, true positive, true positive, ...             Masculine   \n",
       "3  [true positive, false positive, true positive,...               Unknown   \n",
       "6  [true positive, true positive, false positive,...             Masculine   \n",
       "7                     [true negative, true negative]            Occupation   \n",
       "\n",
       "  label_pers_o_predicted annotation_agreement  \n",
       "1   [Masculine, Unknown]        true positive  \n",
       "2   [Masculine, Unknown]        true positive  \n",
       "3              [Unknown]        true positive  \n",
       "6   [Masculine, Unknown]        true positive  \n",
       "7                    [O]       false negative  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_labels = list(eval_by_ann[\"label_{}_expected\".format(category)])\n",
    "predicted_labels = list(eval_by_ann[\"label_{}_predicted\".format(category)])\n",
    "ann_agmts = []\n",
    "perso_labels = list(pers_o_label_tags.keys())\n",
    "for i,exp in enumerate(expected_labels):\n",
    "    pred = predicted_labels[i]\n",
    "    if (exp == \"O\"):\n",
    "        # If `O` was predicted as expected\n",
    "        if 'O' in pred:\n",
    "            ann_agmts += [\"true negative\"]\n",
    "        # If a label was predicted when `O` was expected\n",
    "        else:\n",
    "            ann_agmts += [\"false positive\"]\n",
    "    else:\n",
    "        # If the correct label was predicted\n",
    "        if exp in pred:\n",
    "            ann_agmts += [\"true positive\"]\n",
    "        # If there's a label mismatch\n",
    "        elif (exp != \"O\") and (not \"O\" in pred) and (not exp in pred):\n",
    "            ann_agmts += [\"false positive\"]\n",
    "        # If `O` was predicted when there was an expected label\n",
    "        else:\n",
    "            ann_agmts += [\"false negative\"]\n",
    "assert len(ann_agmts) == eval_by_ann.shape[0]\n",
    "\n",
    "# Insert the annotation agreement column recording TP, FP, TN, and FN\n",
    "eval_by_ann.insert(len(eval_by_ann.columns), \"annotation_agreement\", ann_agmts)\n",
    "eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann.to_csv(predictions_dir+\"cc-{a}_{c}_baseline_fastText{d}_annot_evaluation.csv\".format(a=a, c=category, d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_perso_all = utils1.getAnnotationAgreementMetrics(pred_perso_ann, \"all\")\n",
    "# metrics_perso_pn = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[~(pred_perso_ann.agreement_label.isin([\"Occupation\",\"O\"]))], \"Person Name\")\n",
    "# metrics_perso_unk = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Unknown\"], \"Unknown\")\n",
    "# metrics_perso_fem = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Feminine\"], \"Feminine\")\n",
    "# metrics_perso_mas = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Masculine\"], \"Masculine\")\n",
    "# metrics_perso_occ = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Occupation\"], \"Occupation\")\n",
    "# metrics_perso = pd.concat([metrics_perso_all, metrics_perso_pn, metrics_perso_unk, metrics_perso_fem, metrics_perso_mas, metrics_perso_occ])\n",
    "# metrics_perso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate annotation agreement metrics for each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_agmt = pd.DataFrame.from_dict({\n",
    "        \"label\":[], \"false negative\":[], \"false positive\":[],\n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>3426.0</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>5462.0</td>\n",
       "      <td>0.706963</td>\n",
       "      <td>0.614536</td>\n",
       "      <td>0.657518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>255.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>1151.0</td>\n",
       "      <td>0.797091</td>\n",
       "      <td>0.818634</td>\n",
       "      <td>0.807719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>2830.0</td>\n",
       "      <td>0.683740</td>\n",
       "      <td>0.652074</td>\n",
       "      <td>0.667532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1749.0</td>\n",
       "      <td>0.959934</td>\n",
       "      <td>0.604772</td>\n",
       "      <td>0.742045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown          3426.0          2264.0         5462.0   0.706963   \n",
       "0    Feminine           255.0           293.0         1151.0   0.797091   \n",
       "0   Masculine          1510.0          1309.0         2830.0   0.683740   \n",
       "0  Occupation          1143.0            73.0         1749.0   0.959934   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.614536  0.657518  \n",
       "0  0.818634  0.807719  \n",
       "0  0.652074  0.667532  \n",
       "0  0.604772  0.742045  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in perso_labels:\n",
    "    agmt_df = eval_by_ann.loc[eval_by_ann[\"label_{}_expected\".format(category)] == label]\n",
    "    tp = agmt_df.loc[agmt_df.annotation_agreement == \"true positive\"].shape[0]\n",
    "    fp = agmt_df.loc[agmt_df.annotation_agreement == \"false positive\"].shape[0]\n",
    "    fn = agmt_df.loc[agmt_df.annotation_agreement == \"false negative\"].shape[0]\n",
    "    prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "    label_agmt = pd.DataFrame.from_dict({\n",
    "            \"label\":[label], \"false negative\":[fn], \"false positive\":[fp],\n",
    "             \"true positive\":[tp], \"precision\":[prec], \"recall\":[rec], \"f1\":[f1]\n",
    "        })\n",
    "    annot_agmt = pd.concat([annot_agmt, label_agmt])\n",
    "annot_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_perso.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_annot_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "annot_agmt.to_csv(agreement_dir+\"crf_{a}_{c}_baseline_fastText{d}_annot_agmt.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Loose, Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the tokens' BIO tags to the label, and calculate agreement scores for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels = pred_perso.drop(columns=[\"_merge\"])\n",
    "tag_exp = list(pred_perso_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_perso_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_pers_labels.loc[pred_pers_labels.label_personname_predicted == \"Feminine\"].head()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the agreement metrics at the label level for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>6571</td>\n",
       "      <td>7616</td>\n",
       "      <td>10958</td>\n",
       "      <td>0.589964</td>\n",
       "      <td>0.625135</td>\n",
       "      <td>0.607041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>538</td>\n",
       "      <td>1500</td>\n",
       "      <td>2332</td>\n",
       "      <td>0.608559</td>\n",
       "      <td>0.812544</td>\n",
       "      <td>0.695912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2066</td>\n",
       "      <td>5246</td>\n",
       "      <td>4747</td>\n",
       "      <td>0.475033</td>\n",
       "      <td>0.696756</td>\n",
       "      <td>0.564917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>2723</td>\n",
       "      <td>2203</td>\n",
       "      <td>3185</td>\n",
       "      <td>0.591128</td>\n",
       "      <td>0.539100</td>\n",
       "      <td>0.563916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown            6571            7616          10958   0.589964   \n",
       "0    Feminine             538            1500           2332   0.608559   \n",
       "0   Masculine            2066            5246           4747   0.475033   \n",
       "0  Occupation            2723            2203           3185   0.591128   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.625135  0.607041  \n",
       "0  0.812544  0.695912  \n",
       "0  0.696756  0.564917  \n",
       "0  0.539100  0.563916  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['Unknown', 'Feminine', 'Masculine', 'Occupation']\n",
    "pred_perso_labels = utils.isPredictedInExpected(pred_perso_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine and save the performance measures and loose predictions (labels, not BIO tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_loose_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_{c}_baseline_fastText{d}_loose_agmt.csv\".format(a=a, d=d, c=category))\n",
    "pred_perso_labels.to_csv(predictions_dir+\"crf_{a}_{c}_baseline_fastText{d}_loose_evaluation.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### *For train-dev-test (i.e., 40-40-20) approach*\n",
    "\n",
    "<a id=\"ii\"></a>\n",
    "## II. Predict Over All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = utils1.zip2FeaturesAndTarget(df_test_grouped, \"tag\")\n",
    "X_test = [utils1.extractSentenceFeatures(sentence) for sentence in test_sentences]  # Features\n",
    "y_test = [utils1.extractSentenceTargets(sentence) for sentence in test_sentences]   # Target\n",
    "# Combine all data subsets' features and targets\n",
    "X_all = X_train+X_dev+X_test\n",
    "y_all = y_train+y_dev+y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = clf_perso.predict(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.5211899648086513\n",
      "  - Prec: 0.5561442804328083\n",
      "  - Rec 0.49419097345616575\n"
     ]
    }
   ],
   "source": [
    "print(\"  - F1:\", metrics.flat_f1_score(y_all, all_predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_all, all_predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_all, all_predictions, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_grouped = df_train_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "df_train_grouped = df_train_grouped.reset_index()\n",
    "df_dev_grouped = df_dev_grouped.drop(columns=\"tag_{}_predicted\".format(target_labels))\n",
    "df_dev_grouped = df_dev_grouped.reset_index()\n",
    "df_test_grouped = df_test_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "df_all_grouped = pd.concat([df_train_grouped, df_dev_grouped, df_test_grouped])\n",
    "# df_all_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Scope</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>and</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>Contents</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>Sermons</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token_id  sentence tag_pers_o_expected pred_ling_tag  \\\n",
       "sentence_id                                                        \n",
       "2                 16     Scope                 [O]           [O]   \n",
       "2                 17       and                 [O]           [O]   \n",
       "2                 18  Contents                 [O]           [O]   \n",
       "2                 19         :                 [O]           [O]   \n",
       "2                 20   Sermons                 [O]           [O]   \n",
       "\n",
       "            tag_pers_o_predicted  \n",
       "sentence_id                       \n",
       "2                              O  \n",
       "2                              O  \n",
       "2                              O  \n",
       "2                              O  \n",
       "2                              O  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_grouped.insert(len(df_all_grouped.columns), \"tag_pers_o_predicted\", all_predictions)\n",
    "df_all_grouped = df_all_grouped.set_index(\"sentence_id\")\n",
    "df_all_exploded = df_all_grouped.explode(list(df_all_grouped.columns))\n",
    "df_all_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_exploded = df_all_exploded.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_{t}_baseline_fastText{d}_predictions_ALLDATA.csv\".format(a=a, t=target_labels, d=d)\n",
    "df_all_exploded.to_csv(config.experiment1_output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the manual annotations, the predicted labels will be deemed incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics for each category of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso = df_all_exploded.copy()\n",
    "pred_perso = pred_perso.fillna(\"O\")\n",
    "pred_perso = utils.isPredictedInExpected(pred_perso, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>2082</td>\n",
       "      <td>2433</td>\n",
       "      <td>4549</td>\n",
       "      <td>0.651533</td>\n",
       "      <td>0.686020</td>\n",
       "      <td>0.668332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>3959</td>\n",
       "      <td>3797</td>\n",
       "      <td>7738</td>\n",
       "      <td>0.670828</td>\n",
       "      <td>0.661537</td>\n",
       "      <td>0.666150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>88</td>\n",
       "      <td>183</td>\n",
       "      <td>392</td>\n",
       "      <td>0.681739</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.743128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>337</td>\n",
       "      <td>1035</td>\n",
       "      <td>1620</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.827798</td>\n",
       "      <td>0.702515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>678</td>\n",
       "      <td>875</td>\n",
       "      <td>1356</td>\n",
       "      <td>0.607799</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.635873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>1491</td>\n",
       "      <td>1339</td>\n",
       "      <td>2103</td>\n",
       "      <td>0.610982</td>\n",
       "      <td>0.585142</td>\n",
       "      <td>0.597783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>896</td>\n",
       "      <td>623</td>\n",
       "      <td>1634</td>\n",
       "      <td>0.723970</td>\n",
       "      <td>0.645850</td>\n",
       "      <td>0.682682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>1304</td>\n",
       "      <td>892</td>\n",
       "      <td>1844</td>\n",
       "      <td>0.673977</td>\n",
       "      <td>0.585769</td>\n",
       "      <td>0.626785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     B-Unknown            2082            2433           4549   0.651533   \n",
       "0     I-Unknown            3959            3797           7738   0.670828   \n",
       "0    B-Feminine              88             183            392   0.681739   \n",
       "0    I-Feminine             337            1035           1620   0.610169   \n",
       "0   B-Masculine             678             875           1356   0.607799   \n",
       "0   I-Masculine            1491            1339           2103   0.610982   \n",
       "0  B-Occupation             896             623           1634   0.723970   \n",
       "0  I-Occupation            1304             892           1844   0.673977   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.686020  0.668332  \n",
       "0  0.661537  0.666150  \n",
       "0  0.816667  0.743128  \n",
       "0  0.827798  0.702515  \n",
       "0  0.666667  0.635873  \n",
       "0  0.585142  0.597783  \n",
       "0  0.645850  0.682682  \n",
       "0  0.585769  0.626785  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso, \"_merge\", pers_o_label_subset[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(pers_o_label_subset)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso, \"_merge\", pers_o_label_subset[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_stats.to_csv(\n",
    "    config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_strict_agmt_ALLDATA.csv\".format(a=a, c=category, d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Annotation Agreement\n",
    "\n",
    "Calculate agreement at the annotation level, so if the model labels any word correctly from a manually annotated text span, that annotation is recorded as being correctly labeled (`true positive`).  Note whether the models' labels are an `exact_match`, `label_match`, `category_match` or `mismatch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the annotation data:\n",
    "\n",
    "*Note: `ann_id` of `9999` indicates no annotation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the annotation data by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "perso_all = pd.concat([perso_train, perso_dev, perso_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778803, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>[B-Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>8</td>\n",
       "      <td>[I-Unknown]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id  token_id          tag\n",
       "0            0   99999         0          [O]\n",
       "1            0   99999         1          [O]\n",
       "2            0   99999         2          [O]\n",
       "3            1   14384         7  [B-Unknown]\n",
       "4            1   14384         8  [I-Unknown]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ann = perso_all[[\"sentence_id\", \"ann_id\", \"token_id\", \"tag\"]]\n",
    "df_ann = utils.implodeDataFrame(df_ann, [\"sentence_id\", \"ann_id\", \"token_id\"])\n",
    "df_ann = df_ann.reset_index()\n",
    "print(df_ann.shape)\n",
    "df_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the columns of the annotation and prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>604541</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604542</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604543</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298617</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298618</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id       token tag_pers_o_expected pred_ling_tag  \\\n",
       "604541            0         0  Identifier                 [O]           [O]   \n",
       "604542            0         1           :                 [O]           [O]   \n",
       "604543            0         2         AA5                 [O]           [O]   \n",
       "298617            1         3       Title                 [O]           [O]   \n",
       "298618            1         4           :                 [O]           [O]   \n",
       "\n",
       "       tag_pers_o_predicted         _merge  \n",
       "604541                    O  true negative  \n",
       "604542                    O  true negative  \n",
       "604543                    O  true negative  \n",
       "298617                    O  true negative  \n",
       "298618                    O  true negative  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename `sentence` column `token`\n",
    "pred_perso = pred_perso.rename(columns={\"sentence\":\"token\"}).sort_values(by=\"token_id\")\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data, adding the annotation IDs (`ann_id` column) to the prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [\"sentence_id\", \"token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso.join(df_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag\"])  # duplicate of tag_expected\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_expected\"].isna()].shape[0] == 0\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.explode([\"tag_pers_o_expected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the BIO tags to label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "pred_labels = list(pred_perso_ann[\"tag_{}_predicted\".format(category)])\n",
    "pred_labels = [label if label == \"O\" else label[2:] for label in pred_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_predicted\".format(category), pred_labels)\n",
    "# Get the lists of expected labels\n",
    "exp_labels = list(pred_perso_ann[\"tag_{}_expected\".format(category)])\n",
    "exp_labels = [label if label == \"O\" else label[2:] for label in exp_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_expected\".format(category), exp_labels)\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_ann = utils.implodeDataFrame(pred_perso_ann, [\"sentence_id\", \"ann_id\"])\n",
    "pred_perso_ann = pred_perso_ann.reset_index()\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreements and disagreements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_types_perso, agmt_labels_perso = utils1.getAnnotationAgreement(pred_perso_ann, \"label_pers_o_predicted\", \"label_pers_o_expected\")\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"annotation_agreement\", agmt_types_perso)\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"agreement_label\", agmt_labels_perso)\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>false negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>false positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>10080</td>\n",
       "      <td>15546</td>\n",
       "      <td>5003</td>\n",
       "      <td>0.756533</td>\n",
       "      <td>0.606649</td>\n",
       "      <td>0.673351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person Name</td>\n",
       "      <td>8551</td>\n",
       "      <td>13603</td>\n",
       "      <td>4324</td>\n",
       "      <td>0.758800</td>\n",
       "      <td>0.614020</td>\n",
       "      <td>0.678775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>3749</td>\n",
       "      <td>8755</td>\n",
       "      <td>2649</td>\n",
       "      <td>0.767713</td>\n",
       "      <td>0.700176</td>\n",
       "      <td>0.732391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>817</td>\n",
       "      <td>1636</td>\n",
       "      <td>593</td>\n",
       "      <td>0.733961</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.698847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>3985</td>\n",
       "      <td>3212</td>\n",
       "      <td>1082</td>\n",
       "      <td>0.748020</td>\n",
       "      <td>0.446297</td>\n",
       "      <td>0.559046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>1529</td>\n",
       "      <td>1943</td>\n",
       "      <td>679</td>\n",
       "      <td>0.741037</td>\n",
       "      <td>0.559620</td>\n",
       "      <td>0.637676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels  false negative  true positive  false positive  precision  \\\n",
       "0          all           10080          15546            5003   0.756533   \n",
       "0  Person Name            8551          13603            4324   0.758800   \n",
       "0      Unknown            3749           8755            2649   0.767713   \n",
       "0     Feminine             817           1636             593   0.733961   \n",
       "0    Masculine            3985           3212            1082   0.748020   \n",
       "0   Occupation            1529           1943             679   0.741037   \n",
       "\n",
       "     recall       f_1  \n",
       "0  0.606649  0.673351  \n",
       "0  0.614020  0.678775  \n",
       "0  0.700176  0.732391  \n",
       "0  0.666938  0.698847  \n",
       "0  0.446297  0.559046  \n",
       "0  0.559620  0.637676  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_perso_all = utils1.getAnnotationAgreementMetrics(pred_perso_ann, \"all\")\n",
    "metrics_perso_pn = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[~(pred_perso_ann.agreement_label.isin([\"Occupation\",\"O\"]))], \"Person Name\")\n",
    "metrics_perso_unk = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Unknown\"], \"Unknown\")\n",
    "metrics_perso_fem = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Feminine\"], \"Feminine\")\n",
    "metrics_perso_mas = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Masculine\"], \"Masculine\")\n",
    "metrics_perso_occ = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Occupation\"], \"Occupation\")\n",
    "metrics_perso = pd.concat([metrics_perso_all, metrics_perso_pn, metrics_perso_unk, metrics_perso_fem, metrics_perso_mas, metrics_perso_occ])\n",
    "metrics_perso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_perso.to_csv(\n",
    "    config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_annot_agmt.csv\".format(a=a, d=d, c=category)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Loose, Each Label\n",
    "\n",
    "Generalize the tokens' BIO tags to the labels and calculate agreement scores for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels = pred_perso.drop(columns=[\"_merge\"])\n",
    "tag_exp = list(pred_perso_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_perso_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_pers_labels.loc[pred_pers_labels.label_personname_predicted == \"Feminine\"].head()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the agreement metrics at the label level for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>6017</td>\n",
       "      <td>5487</td>\n",
       "      <td>13030</td>\n",
       "      <td>0.703678</td>\n",
       "      <td>0.684097</td>\n",
       "      <td>0.693749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>422</td>\n",
       "      <td>874</td>\n",
       "      <td>2356</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.848092</td>\n",
       "      <td>0.784288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2159</td>\n",
       "      <td>1942</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.657677</td>\n",
       "      <td>0.633447</td>\n",
       "      <td>0.645334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>2200</td>\n",
       "      <td>1381</td>\n",
       "      <td>3612</td>\n",
       "      <td>0.723413</td>\n",
       "      <td>0.621473</td>\n",
       "      <td>0.668579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown            6017            5487          13030   0.703678   \n",
       "0    Feminine             422             874           2356   0.729412   \n",
       "0   Masculine            2159            1942           3731   0.657677   \n",
       "0  Occupation            2200            1381           3612   0.723413   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.684097  0.693749  \n",
       "0  0.848092  0.784288  \n",
       "0  0.633447  0.645334  \n",
       "0  0.621473  0.668579  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['Unknown', 'Feminine', 'Masculine', 'Occupation']\n",
    "pred_perso_labels = utils.isPredictedInExpected(pred_perso_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine and save the performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_stats.to_csv(\n",
    "    config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_loose_agmt.csv\".format(a=a, d=d, c=category)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Past error analysis on Person Name document classifiers would suggest that there's a mix-up in the manual annotations between 'Masculine' and 'Unknown,' with many Masculine-labeled names actually needing an 'Unknown' label based on the annotation descriptions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

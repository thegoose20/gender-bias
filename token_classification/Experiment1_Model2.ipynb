{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1, Model 2\n",
    "\n",
    "#### Model Setup\n",
    "\n",
    "Run models in the following order, using their output labels as features for the next model:\n",
    "\n",
    "1. Multilabel Linguistic Classifier\n",
    "2. Multiclass Person Name + Occupation Sequence Classifier\n",
    "3. Multilabel Stereotype + Omission Document Classifier\n",
    "\n",
    "***\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/experiment_input/`\n",
    "    * Prediction Data: Data: under directory `../data/token_clf_data/model_output/experiment1/`\n",
    "* Word Embeddings\n",
    "    * Custom fastText (word2vec with subwords) embeddings of 100 dimensions trained on the CRC Archives catalog's descriptive metadata (harvested October 2020)\n",
    "    \n",
    "***\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I.](#i) Person Name + Occupation Classifier\n",
    "* [Preprocessing](#prep)\n",
    "* [Training & Prediction](#tp)\n",
    "* [Evaluation](#eval)\n",
    "\n",
    "[II.](#ii) Predict Over All Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load programming resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, utils1, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For classification\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define resources for the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(config.experiment_input_path).mkdir(parents=True, exist_ok=True)    # For train, devtest, and blind test data\n",
    "predictions_dir = config.experiment1_path+\"5fold/output/\"\n",
    "Path(predictions_dir).mkdir(parents=True, exist_ok=True)  # For predictions\n",
    "agreement_dir = config.experiment1_path+\"5fold/agreement/\"\n",
    "Path(agreement_dir).mkdir(parents=True, exist_ok=True)    # For agreement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_o_label_subset = [\n",
    "    \"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \n",
    "    \"B-Masculine\", \"I-Masculine\", \"B-Occupation\", \"I-Occupation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_label_tags = {\n",
    "    \"Gendered-Pronoun\": [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"], \"Gendered-Role\": [\"B-Gendered-Role\", \"I-Gendered-Role\"],\"Generalization\": [\"B-Generalization\", \"I-Generalization\"]\n",
    "    }\n",
    "pers_o_label_tags = {\n",
    "    \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "     \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100                   # dimensions of word embeddings (should match utils1.py) for file names\n",
    "target_labels = \"pers_o\"  # for file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"i\"></a>\n",
    "## I. Person Name + Occupation Labels\n",
    "\n",
    "Train a multiclass sequence classifier, using Conditional Random Field with Adaptive Regularization of Weight Vectors (AROW), on the Person Name and Occupation labels, **passing in the Linguistic labels (not specific BIO label-tag pair) from the previous model's predictions as features to this model.**\n",
    "\n",
    "Multiclass is a suitable setup for these labels because they are mutually exclusive (no one token should have more than one of these labels).  The sequence classifier with AROW was the highest performing for past algorithm experiments with sequence classifiers for Person Name and Occupation labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Linguistic features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"rf\"\n",
    "# ling_filename = config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_ALLDATA.csv\".format(a=a,d=d)\n",
    "ling_filename = predictions_dir+\"cc-{a}_ling_baseline_fastText{d}_evaluation_loose.csv\".format(a=a, d=d)\n",
    "ling_eval_df = pd.read_csv(ling_filename, usecols=[\"token_id\", \"predicted_tag\"])\n",
    "# Replace tags with labels\n",
    "for label,tags in ling_label_tags.items():\n",
    "    for tag in tags:\n",
    "        ling_eval_df[\"predicted_tag\"] = ling_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "ling_features = ling_eval_df.rename(columns={\"predicted_tag\":\"pred_ling_tag\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(779270, 10) (779270, 10)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(config.tokc_path+\"experiment_input/token_train.csv\", index_col=0)\n",
    "# dev_df =  pd.read_csv(config.tokc_path+\"experiment_input/token_validate.csv\", index_col=0)\n",
    "# test_df = pd.read_csv(config.tokc_path+\"experiment_input/token_test.csv\", index_col=0)\n",
    "df = pd.read_csv(config.tokc_path+\"experiment_input/token_5fold.csv\", index_col=0)\n",
    "perso_df = utils1.selectDataForLabels(df, \"tag\", pers_o_label_subset)\n",
    "# perso_train = utils1.selectDataForLabels(train_df, \"tag\", pers_o_label_subset)\n",
    "# perso_dev = utils1.selectDataForLabels(dev_df, \"tag\", pers_o_label_subset)\n",
    "# perso_test = utils1.selectDataForLabels(test_df, \"tag\", pers_o_label_subset)\n",
    "# print(perso_train.shape, perso_dev.shape, perso_test.shape)\n",
    "print(df.shape, perso_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the label associated with each annotation for future evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ann_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[58341, 58342, 58343, 58344]</td>\n",
       "      <td>[B-Feminine, I-Feminine, I-Feminine, I-Feminine]</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[19836, 19837, 19838, 19839]</td>\n",
       "      <td>[B-Unknown, I-Unknown, I-Unknown, I-Unknown]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[28713, 28714]</td>\n",
       "      <td>[B-Unknown, I-Unknown]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[28738, 28739, 28740, 28741]</td>\n",
       "      <td>[B-Unknown, I-Unknown, I-Unknown, I-Unknown]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[28790, 28791, 28792]</td>\n",
       "      <td>[B-Unknown, I-Unknown, I-Unknown]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            token_id  \\\n",
       "ann_id                                 \n",
       "7       [58341, 58342, 58343, 58344]   \n",
       "14      [19836, 19837, 19838, 19839]   \n",
       "15                    [28713, 28714]   \n",
       "16      [28738, 28739, 28740, 28741]   \n",
       "17             [28790, 28791, 28792]   \n",
       "\n",
       "                                                     tag expected_label  \n",
       "ann_id                                                                   \n",
       "7       [B-Feminine, I-Feminine, I-Feminine, I-Feminine]       Feminine  \n",
       "14          [B-Unknown, I-Unknown, I-Unknown, I-Unknown]        Unknown  \n",
       "15                                [B-Unknown, I-Unknown]        Unknown  \n",
       "16          [B-Unknown, I-Unknown, I-Unknown, I-Unknown]        Unknown  \n",
       "17                     [B-Unknown, I-Unknown, I-Unknown]        Unknown  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_ann = pd.read_csv(config.tokc_path+\"experiment_input/token_5fold.csv\", usecols=[\"ann_id\", \"token_id\", \"tag\"])\n",
    "df_by_ann = df_by_ann.drop_duplicates()\n",
    "df_by_ann = utils.implodeDataFrame(df_by_ann, [\"ann_id\"])\n",
    "tags_col = list(df_by_ann.tag)\n",
    "labels = [[tag[2:] if tag != \"O\" else tag for tag in tags] for tags in tags_col]\n",
    "labels = [label_list[0] for label_list in labels]\n",
    "df_by_ann.insert(len(df_by_ann.columns), \"expected_label\", labels)\n",
    "perso_labels = list(pers_o_label_tags.keys())\n",
    "df_by_ann = df_by_ann.loc[df_by_ann.expected_label.isin(perso_labels)]\n",
    "df_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the linguistic labels (features) to the model input and evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                   768238\n",
       "Gendered-Role         5638\n",
       "Gendered-Pronoun      4919\n",
       "Generalization         592\n",
       "Name: pred_ling_tag, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perso_train = perso_train.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "# assert perso_train.loc[perso_train.tag.isna()].shape[0] == 0\n",
    "# # perso_train.head()\n",
    "# perso_dev = perso_dev.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "# assert perso_dev.loc[perso_dev.tag.isna()].shape[0] == 0\n",
    "# perso_test = perso_test.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "# assert perso_test.loc[perso_test.tag.isna()].shape[0] == 0\n",
    "# ---------------------------------------------\n",
    "perso_df = perso_df.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "assert perso_df.loc[perso_df.tag.isna()].shape[0] == 0\n",
    "perso_df.pred_ling_tag = perso_df.pred_ling_tag.fillna(\"O\")\n",
    "perso_df.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# perso_train.pred_ling_tag = perso_train.pred_ling_tag.fillna(\"O\")\n",
    "# perso_train.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# perso_dev.pred_ling_tag = perso_dev.pred_ling_tag.fillna(\"O\")\n",
    "# perso_dev.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# perso_test.pred_ling_tag = perso_test.pred_ling_tag.fillna(\"O\")\n",
    "# perso_test.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prep\"></a>\n",
    "### Preprocessing\n",
    "\n",
    "Group data by token and then by sentence, so each sentence is a list of tokens and each token has a list of tags associated with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = perso_train.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# dev_df = perso_dev.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# test_df = perso_test.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# ---------------------------------------------\n",
    "perso_data = perso_df.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "perso_token_groups = utils.implodeDataFrame(perso_data, [\"token_id\", \"sentence_id\", \"token\", \"fold\"]).reset_index()\n",
    "# perso_token_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42030, 6)\n"
     ]
    }
   ],
   "source": [
    "perso_grouped = utils.implodeDataFrame(perso_token_groups, [\"sentence_id\", \"fold\"]).reset_index()\n",
    "perso_grouped = perso_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# perso_grouped.head()\n",
    "print(perso_grouped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the five groups of training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['split0' 'split1' 'split2' 'split3' 'split4']\n"
     ]
    }
   ],
   "source": [
    "split_col = \"fold\"\n",
    "splits = perso_grouped[split_col].unique()\n",
    "splits.sort()\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0, test0 = list(splits[:4]), splits[4]\n",
    "train1, test1 = list(splits[1:]), splits[0]\n",
    "train2, test2 = list(splits[2:])+[splits[0]], splits[1]\n",
    "train3, test3 = list(splits[3:])+list(splits[:2]), splits[2]\n",
    "train4, test4 = [splits[4]]+list(splits[:3]), splits[3]\n",
    "runs = [(train0, test0), (train1, test1), (train2, test2), (train3, test3), (train4, test4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_train_token_groups = utils.implodeDataFrame(train_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_train_token_groups = df_train_token_groups.reset_index()\n",
    "# # df_train_token_groups.head()\n",
    "# df_train_grouped = utils.implodeDataFrame(df_train_token_groups, ['sentence_id'])\n",
    "# df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_train_grouped = df_train_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_dev_token_groups = utils.implodeDataFrame(dev_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_dev_token_groups = df_dev_token_groups.reset_index()\n",
    "# df_dev_grouped = utils.implodeDataFrame(df_dev_token_groups, ['sentence_id'])\n",
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_dev_grouped = df_dev_grouped.reset_index()\n",
    "# # df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_test_token_groups = utils.implodeDataFrame(test_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_test_token_groups = df_test_token_groups.reset_index()\n",
    "# df_test_grouped = utils.implodeDataFrame(df_test_token_groups, ['sentence_id'])\n",
    "# df_test_grouped = df_test_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_test_grouped = df_test_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the linguistic label and BIO tags together with the tokens so each sentence item is a tuple: `(TOKEN, LING_LABEL, TAG_LIST)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_sentences_perso = utils1.zip2FeaturesAndTarget(df_train_grouped, \"tag\")\n",
    "# print(train_sentences_perso[2][:3])\n",
    "# dev_sentences_perso = utils1.zip2FeaturesAndTarget(df_dev_grouped, \"tag\")\n",
    "# print(dev_sentences_perso[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_sentences = train_sentences_perso\n",
    "# dev_sentences = dev_sentences_perso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Features\n",
    "# X_train = [utils1.extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "# X_dev = [utils1.extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# # Target\n",
    "# y_train = [utils1.extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "# y_dev = [utils1.extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tp\"></a>\n",
    "### Training & Prediction\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Person Name** category of tags.  We'll set the max iterations to 100 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: ['split0', 'split1', 'split2', 'split3']\n",
      "Predicting on: split4\n",
      "Predictions for split4 saved!\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "\n",
    "# Specify the run one at a time (with for loop, kernel dies)\n",
    "run = runs[0]  # 1, 2, 3, 4\n",
    "\n",
    "# Get the train (80%) and test (20%) subsets of data\n",
    "# with Person Name tags as targets and Linguistic labels as features\n",
    "train_splits, test_split = run[0], run[1]\n",
    "print(\"Training on:\", train_splits)\n",
    "train_df = perso_grouped.loc[perso_grouped[split_col].isin(train_splits)]\n",
    "dev_df = perso_grouped.loc[perso_grouped[split_col] == test_split]\n",
    "\n",
    "# Zip the linguistic label and BIO tags together with the tokens so each \n",
    "# sentence item is a tuple: `(TOKEN, LING_LABEL, TAG_LIST)`\n",
    "train_sentences = utils1.zip2FeaturesAndTarget(train_df, \"tag\")\n",
    "dev_sentences = utils1.zip2FeaturesAndTarget(dev_df, \"tag\")\n",
    "\n",
    "# Extract features\n",
    "X_train = [utils1.extractSentenceFeatures(s) for s in train_sentences] \n",
    "X_dev = [utils1.extractSentenceFeatures(s) for s in dev_sentences]\n",
    "\n",
    "# Extract targets\n",
    "y_train = [utils1.extractSentenceTargets(s) for s in train_sentences]\n",
    "y_dev = [utils1.extractSentenceTargets(s) for s in dev_sentences]\n",
    "\n",
    "# Train a classification model\n",
    "a = \"arow\"\n",
    "clf = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict with the trained model\n",
    "print(\"Predicting on:\", test_split)\n",
    "predictions = clf.predict(X_dev)\n",
    "\n",
    "dev_df = dev_df.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "dev_df.insert(len(dev_df.columns), \"tag_pers_o_predicted\", predictions)\n",
    "dev_df = dev_df.set_index([\"sentence_id\", \"fold\"])\n",
    "dev_df_exploded = dev_df.explode(list(dev_df.columns))\n",
    "\n",
    "if pred_df.shape[0] > 0:\n",
    "    pred_df = pd.concat([pred_df, dev_df_exploded])\n",
    "else:\n",
    "    pred_df = dev_df_exploded\n",
    "\n",
    "assert pred_df.loc[pred_df[\"tag_pers_o_expected\"].isna()].shape[0] == 0, \"Any NaN values should be replaced with 'O'\"\n",
    "\n",
    "filename = \"crf_{a}_{t}_baseline_fastText{d}_predictions_{s}.csv\".format(a=a, t=target_labels, d=d, s=test_split)\n",
    "pred_df.to_csv(predictions_dir+filename)\n",
    "\n",
    "print(\"Predictions for {} saved!\".format(test_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(753521, 6)\n"
     ]
    }
   ],
   "source": [
    "pred_df0 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split0.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df1 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split1.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df2 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split2.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df3 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split3.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df4 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split4.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_perso = pd.concat([pred_df0, pred_df1, pred_df2, pred_df3, pred_df4])\n",
    "print(pred_perso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[B-Masculine]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>B-Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[I-Masculine]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>I-Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id sentence tag_pers_o_expected pred_ling_tag  \\\n",
       "0            8  split0       233    James       [B-Masculine]           [O]   \n",
       "1            8  split0       234    Whyte       [I-Masculine]           [O]   \n",
       "2            8  split0       235      was                 [O]           [O]   \n",
       "3            8  split0       236   called                 [O]           [O]   \n",
       "4            8  split0       237     upon                 [O]           [O]   \n",
       "\n",
       "  tag_pers_o_predicted  \n",
       "0            B-Unknown  \n",
       "1            I-Unknown  \n",
       "2                    O  \n",
       "3                    O  \n",
       "4                    O  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso = pred_perso.reset_index()\n",
    "pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_pers_o_expected\")\n",
    "pred_perso = utils.getColumnValuesAsLists(pred_perso, \"pred_ling_tag\")\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pred_perso.shape[0] == len(pred_perso.token_id.unique()), \"There should be one row per token.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# a = \"arow\"\n",
    "# clf_perso = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "# try:\n",
    "#     clf_perso.fit(X_train, y_train)\n",
    "# except AttributeError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# targets = list(clf_perso.classes_)\n",
    "# targets.remove('O')\n",
    "# print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# predictions = clf_perso.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(\"  - F1:\", metrics.flat_f1_score(y_dev, predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Prec:\", metrics.flat_precision_score(y_dev, predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Rec\", metrics.flat_recall_score(y_dev, predictions, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "# df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_pers_o_predicted\", predictions)\n",
    "# # df_dev_grouped.head()\n",
    "# df_dev_grouped = df_dev_grouped.set_index(\"sentence_id\")\n",
    "# df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns))\n",
    "# # df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# filename = \"crf_{a}_{t}_baseline_fastText{d}_predictions.csv\".format(a=a, t=target_labels, d=d)\n",
    "# df_dev_exploded.to_csv(config.experiment1_output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval\"></a>\n",
    "### Evaluation\n",
    "#### Evaluate: Strict, Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# filename = \"crf_{a}_{c}_baseline_fastText{d}_predictions.csv\".format(a=a, c=category, d=d)\n",
    "# pred_perso = pd.read_csv(config.experiment1_output_path+filename)\n",
    "# pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_{}_expected\".format(category))\n",
    "# # pred_pers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics for each category of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso = pred_perso.fillna(\"O\")\n",
    "pred_perso = utils.isPredictedInExpected(pred_perso, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the combined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_{c}_baseline_fastText{d}_predictions.csv\".format(a=a, c=category, d=d)\n",
    "pred_perso.to_csv(predictions_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>2336</td>\n",
       "      <td>2628</td>\n",
       "      <td>3408</td>\n",
       "      <td>0.564612</td>\n",
       "      <td>0.593315</td>\n",
       "      <td>0.578608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>5082</td>\n",
       "      <td>3833</td>\n",
       "      <td>5900</td>\n",
       "      <td>0.606185</td>\n",
       "      <td>0.537243</td>\n",
       "      <td>0.569636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>101</td>\n",
       "      <td>464</td>\n",
       "      <td>676</td>\n",
       "      <td>0.592982</td>\n",
       "      <td>0.870013</td>\n",
       "      <td>0.705269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>534</td>\n",
       "      <td>956</td>\n",
       "      <td>1440</td>\n",
       "      <td>0.601002</td>\n",
       "      <td>0.729483</td>\n",
       "      <td>0.659039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>793</td>\n",
       "      <td>1144</td>\n",
       "      <td>1494</td>\n",
       "      <td>0.566338</td>\n",
       "      <td>0.653258</td>\n",
       "      <td>0.606701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>1632</td>\n",
       "      <td>2641</td>\n",
       "      <td>2089</td>\n",
       "      <td>0.441649</td>\n",
       "      <td>0.561408</td>\n",
       "      <td>0.494379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>1033</td>\n",
       "      <td>862</td>\n",
       "      <td>1491</td>\n",
       "      <td>0.633659</td>\n",
       "      <td>0.590729</td>\n",
       "      <td>0.611441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>1682</td>\n",
       "      <td>1225</td>\n",
       "      <td>1445</td>\n",
       "      <td>0.541199</td>\n",
       "      <td>0.462104</td>\n",
       "      <td>0.498534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     B-Unknown            2336            2628           3408   0.564612   \n",
       "0     I-Unknown            5082            3833           5900   0.606185   \n",
       "0    B-Feminine             101             464            676   0.592982   \n",
       "0    I-Feminine             534             956           1440   0.601002   \n",
       "0   B-Masculine             793            1144           1494   0.566338   \n",
       "0   I-Masculine            1632            2641           2089   0.441649   \n",
       "0  B-Occupation            1033             862           1491   0.633659   \n",
       "0  I-Occupation            1682            1225           1445   0.541199   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.593315  0.578608  \n",
       "0  0.537243  0.569636  \n",
       "0  0.870013  0.705269  \n",
       "0  0.729483  0.659039  \n",
       "0  0.653258  0.606701  \n",
       "0  0.561408  0.494379  \n",
       "0  0.590729  0.611441  \n",
       "0  0.462104  0.498534  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso, \"_merge\", pers_o_label_subset[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(pers_o_label_subset)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso, \"_merge\", pers_o_label_subset[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_strict_agmt.csv\".format(a=a, c=category, d=d)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_{c}_baseline_fastText{d}_strict_agmt.csv\".format(a=a, c=category, d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Annotation Agreement\n",
    "\n",
    "Calculate agreement at the annotation level, so if the model labels any word correctly from a manually annotated text span, that annotation is recorded as being correctly labeled (`true positive`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the annotation data by token:\n",
    "\n",
    "*Note: `ann_id` of `9999` indicates no annotation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>58341</td>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>58342</td>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>58343</td>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>58344</td>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>19836</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id token_id expected_tag expected_label\n",
       "0       7    58341   B-Feminine       Feminine\n",
       "1       7    58342   I-Feminine       Feminine\n",
       "2       7    58343   I-Feminine       Feminine\n",
       "3       7    58344   I-Feminine       Feminine\n",
       "4      14    19836    B-Unknown        Unknown"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_ann = df_by_ann.explode([\"token_id\", \"tag\"])\n",
    "df_by_ann = df_by_ann.rename(columns={\"tag\": \"expected_tag\"})\n",
    "df_by_ann = df_by_ann.reset_index()\n",
    "df_by_ann.head()\n",
    "# # df_by_ann.expected_label.value_counts()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the columns of the dev and prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename `sentence` column `token`\n",
    "pred_perso = pred_perso.rename(columns={\"sentence\":\"token\"})\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data, adding the annotation IDs (`ann_id` column) to the prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [\"token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[O]</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "      <td>14387.0</td>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[O]</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "      <td>14387.0</td>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id   token pred_ling_tag tag_pers_o_predicted  \\\n",
       "0            8  split0       233   James           [O]            B-Unknown   \n",
       "1            8  split0       234   Whyte           [O]            I-Unknown   \n",
       "2            8  split0       235     was           [O]                    O   \n",
       "3            8  split0       236  called           [O]                    O   \n",
       "4            8  split0       237    upon           [O]                    O   \n",
       "\n",
       "           _merge   ann_id tag_pers_o_expected expected_label  \n",
       "0  false positive  14387.0         B-Masculine      Masculine  \n",
       "1  false positive  14387.0         I-Masculine      Masculine  \n",
       "2   true negative  99999.0                   O              O  \n",
       "3   true negative  99999.0                   O              O  \n",
       "4   true negative  99999.0                   O              O  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann = pred_perso.join(df_by_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category)])  # duplicate of expected_tag\n",
    "pred_perso_ann = pred_perso_ann.rename(columns={\"expected_tag\":\"tag_{}_expected\".format(category)})\n",
    "pred_perso_ann[\"ann_id\"] = pred_perso_ann[\"ann_id\"].fillna(99999)\n",
    "pred_perso_ann[\"tag_pers_o_expected\"] = pred_perso_ann[\"tag_pers_o_expected\"].fillna(\"O\")\n",
    "pred_perso_ann[\"expected_label\"] = pred_perso_ann[\"expected_label\"].fillna(\"O\")\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_expected\"].isna()].shape[0] == 0\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the predicted BIO tags to label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "pred_labels = list(pred_perso_ann[\"tag_{}_predicted\".format(category)])\n",
    "pred_labels = [label if label == \"O\" else label[2:] for label in pred_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_predicted\".format(category), pred_labels)\n",
    "# Rename the expected labels column name\n",
    "pred_perso_ann = pred_perso_ann.rename(columns={\"expected_label\":\"label_{}_expected\".format(category)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>Feminine</td>\n",
       "      <td>[2590, 2590, 2590, 2590]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>[58341, 58342, 58343, 58344]</td>\n",
       "      <td>[Mrs, Norman, Macleod, ,]</td>\n",
       "      <td>[[Gendered-Role, Gendered-Role, Gendered-Role]...</td>\n",
       "      <td>[true negative, true positive, true positive, ...</td>\n",
       "      <td>[O, Feminine, Feminine, Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[1097, 1097, 1097, 1097]</td>\n",
       "      <td>[split4, split4, split4, split4]</td>\n",
       "      <td>[19836, 19837, 19838, 19839]</td>\n",
       "      <td>[Dr., Nelly, Renee, Deme]</td>\n",
       "      <td>[[O], [O], [O], [O]]</td>\n",
       "      <td>[true positive, false positive, false positive...</td>\n",
       "      <td>[Unknown, Feminine, Feminine, Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[1485, 1485]</td>\n",
       "      <td>[split4, split4]</td>\n",
       "      <td>[28713, 28714]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser]</td>\n",
       "      <td>[[O, O], [O, O]]</td>\n",
       "      <td>[true positive, true positive]</td>\n",
       "      <td>[Feminine, Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[1486, 1486, 1486, 1486]</td>\n",
       "      <td>[split3, split3, split3, split3]</td>\n",
       "      <td>[28738, 28739, 28740, 28741]</td>\n",
       "      <td>[Marjory, Kennedy, Fraser, ,]</td>\n",
       "      <td>[[O, O], [O, O], [O, O], [O, O]]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Feminine, Feminine, Feminine, Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[1487, 1487, 1487]</td>\n",
       "      <td>[split0, split0, split0]</td>\n",
       "      <td>[28790, 28791, 28792]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser, ,]</td>\n",
       "      <td>[[O, O], [O, O], [O, O]]</td>\n",
       "      <td>[true positive, true positive, false negative]</td>\n",
       "      <td>[Feminine, Feminine, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id label_pers_o_expected               sentence_id  \\\n",
       "0     7.0              Feminine  [2590, 2590, 2590, 2590]   \n",
       "1    14.0               Unknown  [1097, 1097, 1097, 1097]   \n",
       "2    15.0               Unknown              [1485, 1485]   \n",
       "3    16.0               Unknown  [1486, 1486, 1486, 1486]   \n",
       "4    17.0               Unknown        [1487, 1487, 1487]   \n",
       "\n",
       "                               fold                      token_id  \\\n",
       "0  [split2, split2, split2, split2]  [58341, 58342, 58343, 58344]   \n",
       "1  [split4, split4, split4, split4]  [19836, 19837, 19838, 19839]   \n",
       "2                  [split4, split4]                [28713, 28714]   \n",
       "3  [split3, split3, split3, split3]  [28738, 28739, 28740, 28741]   \n",
       "4          [split0, split0, split0]         [28790, 28791, 28792]   \n",
       "\n",
       "                           token  \\\n",
       "0      [Mrs, Norman, Macleod, ,]   \n",
       "1      [Dr., Nelly, Renee, Deme]   \n",
       "2      [Marjory, Kennedy-Fraser]   \n",
       "3  [Marjory, Kennedy, Fraser, ,]   \n",
       "4   [Marjory, Kennedy-Fraser, ,]   \n",
       "\n",
       "                                       pred_ling_tag  \\\n",
       "0  [[Gendered-Role, Gendered-Role, Gendered-Role]...   \n",
       "1                               [[O], [O], [O], [O]]   \n",
       "2                                   [[O, O], [O, O]]   \n",
       "3                   [[O, O], [O, O], [O, O], [O, O]]   \n",
       "4                           [[O, O], [O, O], [O, O]]   \n",
       "\n",
       "                                              _merge  \\\n",
       "0  [true negative, true positive, true positive, ...   \n",
       "1  [true positive, false positive, false positive...   \n",
       "2                     [true positive, true positive]   \n",
       "3  [true positive, true positive, true positive, ...   \n",
       "4     [true positive, true positive, false negative]   \n",
       "\n",
       "                     label_pers_o_predicted  \n",
       "0         [O, Feminine, Feminine, Feminine]  \n",
       "1   [Unknown, Feminine, Feminine, Feminine]  \n",
       "2                      [Feminine, Feminine]  \n",
       "3  [Feminine, Feminine, Feminine, Feminine]  \n",
       "4                   [Feminine, Feminine, O]  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_ann = utils.implodeDataFrame(pred_perso_ann, [\"ann_id\", \"label_{}_expected\".format(category)])\n",
    "pred_perso_ann = pred_perso_ann.reset_index()\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out the expected unannotated data (`ann_id` of `99999`) from the expected annotated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712167, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>238</td>\n",
       "      <td>to</td>\n",
       "      <td>[O]</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>239</td>\n",
       "      <td>preach</td>\n",
       "      <td>[O]</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ann_id label_pers_o_expected sentence_id    fold token_id   token  \\\n",
       "20710  99999.0                     O           8  split0      235     was   \n",
       "20710  99999.0                     O           8  split0      236  called   \n",
       "20710  99999.0                     O           8  split0      237    upon   \n",
       "20710  99999.0                     O           8  split0      238      to   \n",
       "20710  99999.0                     O           8  split0      239  preach   \n",
       "\n",
       "      pred_ling_tag         _merge label_pers_o_predicted  \n",
       "20710           [O]  true negative                      O  \n",
       "20710           [O]  true negative                      O  \n",
       "20710           [O]  true negative                      O  \n",
       "20710           [O]  true negative                      O  \n",
       "20710           [O]  true negative                      O  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_df = pred_perso_ann.loc[pred_perso_ann.ann_id == 99999]\n",
    "fp_df = fp_df.explode([\"token_id\", \"sentence_id\", \"fold\", \"token\", \"pred_ling_tag\", \"_merge\", \"label_{}_predicted\".format(category)])\n",
    "print(fp_df.shape)\n",
    "fp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.loc[pred_perso_ann.ann_id != 99999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann = pd.concat([pred_perso_ann, fp_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get unique lists of predicted labels per row, removing `O` values from lists with Linguistic labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueLabels(label_list):\n",
    "    final_labels = []\n",
    "    for labels in label_list:\n",
    "        if (len(labels) > 1) and (\"O\" in labels):\n",
    "            labels.remove(\"O\")\n",
    "        final_labels += [labels]\n",
    "    assert len(final_labels) == len(label_list), \"There should be the same number of sub-lists in final_labels and label_list.\"\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>[2590, 2590, 2590, 2590]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>[58341, 58342, 58343, 58344]</td>\n",
       "      <td>[Mrs, Norman, Macleod, ,]</td>\n",
       "      <td>[[Gendered-Role, Gendered-Role, Gendered-Role]...</td>\n",
       "      <td>[true negative, true positive, true positive, ...</td>\n",
       "      <td>Feminine</td>\n",
       "      <td>[Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>[1097, 1097, 1097, 1097]</td>\n",
       "      <td>[split4, split4, split4, split4]</td>\n",
       "      <td>[19836, 19837, 19838, 19839]</td>\n",
       "      <td>[Dr., Nelly, Renee, Deme]</td>\n",
       "      <td>[[O], [O], [O], [O]]</td>\n",
       "      <td>[true positive, false positive, false positive...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine, Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>[1485, 1485]</td>\n",
       "      <td>[split4, split4]</td>\n",
       "      <td>[28713, 28714]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser]</td>\n",
       "      <td>[[O, O], [O, O]]</td>\n",
       "      <td>[true positive, true positive]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>[1486, 1486, 1486, 1486]</td>\n",
       "      <td>[split3, split3, split3, split3]</td>\n",
       "      <td>[28738, 28739, 28740, 28741]</td>\n",
       "      <td>[Marjory, Kennedy, Fraser, ,]</td>\n",
       "      <td>[[O, O], [O, O], [O, O], [O, O]]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>[1487, 1487, 1487]</td>\n",
       "      <td>[split0, split0, split0]</td>\n",
       "      <td>[28790, 28791, 28792]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser, ,]</td>\n",
       "      <td>[[O, O], [O, O], [O, O]]</td>\n",
       "      <td>[true positive, true positive, false negative]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id               sentence_id                              fold  \\\n",
       "0     7.0  [2590, 2590, 2590, 2590]  [split2, split2, split2, split2]   \n",
       "1    14.0  [1097, 1097, 1097, 1097]  [split4, split4, split4, split4]   \n",
       "2    15.0              [1485, 1485]                  [split4, split4]   \n",
       "3    16.0  [1486, 1486, 1486, 1486]  [split3, split3, split3, split3]   \n",
       "4    17.0        [1487, 1487, 1487]          [split0, split0, split0]   \n",
       "\n",
       "                       token_id                          token  \\\n",
       "0  [58341, 58342, 58343, 58344]      [Mrs, Norman, Macleod, ,]   \n",
       "1  [19836, 19837, 19838, 19839]      [Dr., Nelly, Renee, Deme]   \n",
       "2                [28713, 28714]      [Marjory, Kennedy-Fraser]   \n",
       "3  [28738, 28739, 28740, 28741]  [Marjory, Kennedy, Fraser, ,]   \n",
       "4         [28790, 28791, 28792]   [Marjory, Kennedy-Fraser, ,]   \n",
       "\n",
       "                                       pred_ling_tag  \\\n",
       "0  [[Gendered-Role, Gendered-Role, Gendered-Role]...   \n",
       "1                               [[O], [O], [O], [O]]   \n",
       "2                                   [[O, O], [O, O]]   \n",
       "3                   [[O, O], [O, O], [O, O], [O, O]]   \n",
       "4                           [[O, O], [O, O], [O, O]]   \n",
       "\n",
       "                                              _merge label_pers_o_expected  \\\n",
       "0  [true negative, true positive, true positive, ...              Feminine   \n",
       "1  [true positive, false positive, false positive...               Unknown   \n",
       "2                     [true positive, true positive]               Unknown   \n",
       "3  [true positive, true positive, true positive, ...               Unknown   \n",
       "4     [true positive, true positive, false negative]               Unknown   \n",
       "\n",
       "  label_pers_o_predicted  \n",
       "0             [Feminine]  \n",
       "1    [Feminine, Unknown]  \n",
       "2             [Feminine]  \n",
       "3             [Feminine]  \n",
       "4             [Feminine]  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = list(eval_by_ann[\"label_{}_predicted\".format(category)])\n",
    "predicted_labels = [list(set(predictions)) for predictions in predicted_labels]\n",
    "predicted_unique = getUniqueLabels(predicted_labels)\n",
    "# predicted_unique[:5]  # Looks good\n",
    "# for pred in predicted_unique:\n",
    "#     if len(pred) > 1:\n",
    "#         print(\"Multi-item lists exist\")\n",
    "#         break\n",
    "eval_by_ann[\"label_{}_predicted\".format(category)] = predicted_unique\n",
    "# Reorder columns:\n",
    "eval_by_ann = eval_by_ann[\n",
    "    [\"ann_id\", \"sentence_id\", \"fold\", \"token_id\", \"token\", \"pred_ling_tag\", \"_merge\", \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category)]\n",
    "]\n",
    "eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreements and disagreements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "      <th>annotation_agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>[2590, 2590, 2590, 2590]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>[58341, 58342, 58343, 58344]</td>\n",
       "      <td>[Mrs, Norman, Macleod, ,]</td>\n",
       "      <td>[[Gendered-Role, Gendered-Role, Gendered-Role]...</td>\n",
       "      <td>[true negative, true positive, true positive, ...</td>\n",
       "      <td>Feminine</td>\n",
       "      <td>[Feminine]</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>[1097, 1097, 1097, 1097]</td>\n",
       "      <td>[split4, split4, split4, split4]</td>\n",
       "      <td>[19836, 19837, 19838, 19839]</td>\n",
       "      <td>[Dr., Nelly, Renee, Deme]</td>\n",
       "      <td>[[O], [O], [O], [O]]</td>\n",
       "      <td>[true positive, false positive, false positive...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine, Unknown]</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>[1485, 1485]</td>\n",
       "      <td>[split4, split4]</td>\n",
       "      <td>[28713, 28714]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser]</td>\n",
       "      <td>[[O, O], [O, O]]</td>\n",
       "      <td>[true positive, true positive]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>[1486, 1486, 1486, 1486]</td>\n",
       "      <td>[split3, split3, split3, split3]</td>\n",
       "      <td>[28738, 28739, 28740, 28741]</td>\n",
       "      <td>[Marjory, Kennedy, Fraser, ,]</td>\n",
       "      <td>[[O, O], [O, O], [O, O], [O, O]]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>[1487, 1487, 1487]</td>\n",
       "      <td>[split0, split0, split0]</td>\n",
       "      <td>[28790, 28791, 28792]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser, ,]</td>\n",
       "      <td>[[O, O], [O, O], [O, O]]</td>\n",
       "      <td>[true positive, true positive, false negative]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id               sentence_id                              fold  \\\n",
       "0     7.0  [2590, 2590, 2590, 2590]  [split2, split2, split2, split2]   \n",
       "1    14.0  [1097, 1097, 1097, 1097]  [split4, split4, split4, split4]   \n",
       "2    15.0              [1485, 1485]                  [split4, split4]   \n",
       "3    16.0  [1486, 1486, 1486, 1486]  [split3, split3, split3, split3]   \n",
       "4    17.0        [1487, 1487, 1487]          [split0, split0, split0]   \n",
       "\n",
       "                       token_id                          token  \\\n",
       "0  [58341, 58342, 58343, 58344]      [Mrs, Norman, Macleod, ,]   \n",
       "1  [19836, 19837, 19838, 19839]      [Dr., Nelly, Renee, Deme]   \n",
       "2                [28713, 28714]      [Marjory, Kennedy-Fraser]   \n",
       "3  [28738, 28739, 28740, 28741]  [Marjory, Kennedy, Fraser, ,]   \n",
       "4         [28790, 28791, 28792]   [Marjory, Kennedy-Fraser, ,]   \n",
       "\n",
       "                                       pred_ling_tag  \\\n",
       "0  [[Gendered-Role, Gendered-Role, Gendered-Role]...   \n",
       "1                               [[O], [O], [O], [O]]   \n",
       "2                                   [[O, O], [O, O]]   \n",
       "3                   [[O, O], [O, O], [O, O], [O, O]]   \n",
       "4                           [[O, O], [O, O], [O, O]]   \n",
       "\n",
       "                                              _merge label_pers_o_expected  \\\n",
       "0  [true negative, true positive, true positive, ...              Feminine   \n",
       "1  [true positive, false positive, false positive...               Unknown   \n",
       "2                     [true positive, true positive]               Unknown   \n",
       "3  [true positive, true positive, true positive, ...               Unknown   \n",
       "4     [true positive, true positive, false negative]               Unknown   \n",
       "\n",
       "  label_pers_o_predicted annotation_agreement  \n",
       "0             [Feminine]        true positive  \n",
       "1    [Feminine, Unknown]        true positive  \n",
       "2             [Feminine]       false positive  \n",
       "3             [Feminine]       false positive  \n",
       "4             [Feminine]       false positive  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_labels = list(eval_by_ann[\"label_{}_expected\".format(category)])\n",
    "predicted_labels = list(eval_by_ann[\"label_{}_predicted\".format(category)])\n",
    "ann_agmts = []\n",
    "perso_labels = list(pers_o_label_tags.keys())\n",
    "for i,exp in enumerate(expected_labels):\n",
    "    pred = predicted_labels[i]\n",
    "    if (exp == \"O\"):\n",
    "        # If `O` was predicted as expected\n",
    "        if 'O' in pred:\n",
    "            ann_agmts += [\"true negative\"]\n",
    "        # If a label was predicted when `O` was expected\n",
    "        else:\n",
    "            ann_agmts += [\"false positive\"]\n",
    "    else:\n",
    "        # If the correct label was predicted\n",
    "        if exp in pred:\n",
    "            ann_agmts += [\"true positive\"]\n",
    "        # If there's a label mismatch\n",
    "        elif (exp != \"O\") and (not \"O\" in pred) and (not exp in pred):\n",
    "            ann_agmts += [\"false positive\"]\n",
    "        # If `O` was predicted when there was an expected label\n",
    "        else:\n",
    "            ann_agmts += [\"false negative\"]\n",
    "assert len(ann_agmts) == eval_by_ann.shape[0]\n",
    "\n",
    "# Insert the annotation agreement column recording TP, FP, TN, and FN\n",
    "eval_by_ann.insert(len(eval_by_ann.columns), \"annotation_agreement\", ann_agmts)\n",
    "eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann.to_csv(predictions_dir+\"cc-{a}_{c}_baseline_fastText{d}_annot_evaluation.csv\".format(a=a, c=category, d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# metrics_perso_all = utils1.getAnnotationAgreementMetrics(pred_perso_ann, \"all\")\n",
    "# metrics_perso_pn = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[~(pred_perso_ann.agreement_label.isin([\"Occupation\",\"O\"]))], \"Person Name\")\n",
    "# metrics_perso_unk = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Unknown\"], \"Unknown\")\n",
    "# metrics_perso_fem = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Feminine\"], \"Feminine\")\n",
    "# metrics_perso_mas = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Masculine\"], \"Masculine\")\n",
    "# metrics_perso_occ = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Occupation\"], \"Occupation\")\n",
    "# metrics_perso = pd.concat([metrics_perso_all, metrics_perso_pn, metrics_perso_unk, metrics_perso_fem, metrics_perso_mas, metrics_perso_occ])\n",
    "# metrics_perso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate annotation agreement metrics for each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_agmt = pd.DataFrame.from_dict({\n",
    "        \"label\":[], \"false negative\":[], \"false positive\":[],\n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>4926.0</td>\n",
       "      <td>0.709798</td>\n",
       "      <td>0.579734</td>\n",
       "      <td>0.638207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>274.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>0.841419</td>\n",
       "      <td>0.809192</td>\n",
       "      <td>0.824991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>2345.0</td>\n",
       "      <td>0.658523</td>\n",
       "      <td>0.536613</td>\n",
       "      <td>0.591350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>0.963554</td>\n",
       "      <td>0.584658</td>\n",
       "      <td>0.727742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown          3571.0          2014.0         4926.0   0.709798   \n",
       "0    Feminine           274.0           219.0         1162.0   0.841419   \n",
       "0   Masculine          2025.0          1216.0         2345.0   0.658523   \n",
       "0  Occupation          1202.0            64.0         1692.0   0.963554   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.579734  0.638207  \n",
       "0  0.809192  0.824991  \n",
       "0  0.536613  0.591350  \n",
       "0  0.584658  0.727742  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in perso_labels:\n",
    "    agmt_df = eval_by_ann.loc[eval_by_ann[\"label_{}_expected\".format(category)] == label]\n",
    "    tp = agmt_df.loc[agmt_df.annotation_agreement == \"true positive\"].shape[0]\n",
    "    fp = agmt_df.loc[agmt_df.annotation_agreement == \"false positive\"].shape[0]\n",
    "    fn = agmt_df.loc[agmt_df.annotation_agreement == \"false negative\"].shape[0]\n",
    "    prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "    label_agmt = pd.DataFrame.from_dict({\n",
    "            \"label\":[label], \"false negative\":[fn], \"false positive\":[fp],\n",
    "             \"true positive\":[tp], \"precision\":[prec], \"recall\":[rec], \"f1\":[f1]\n",
    "        })\n",
    "    annot_agmt = pd.concat([annot_agmt, label_agmt])\n",
    "annot_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_perso.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_annot_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "annot_agmt.to_csv(agreement_dir+\"crf_{a}_{c}_baseline_fastText{d}_annot_agmt.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Loose, Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the tokens' BIO tags to the label, and calculate agreement scores for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels = pred_perso.drop(columns=[\"_merge\"])\n",
    "tag_exp = list(pred_perso_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_perso_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_pers_labels.loc[pred_pers_labels.label_personname_predicted == \"Feminine\"].head()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the agreement metrics at the label level for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>7400</td>\n",
       "      <td>5639</td>\n",
       "      <td>10130</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>0.577867</td>\n",
       "      <td>0.608427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>635</td>\n",
       "      <td>1174</td>\n",
       "      <td>2362</td>\n",
       "      <td>0.667986</td>\n",
       "      <td>0.788121</td>\n",
       "      <td>0.723098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2417</td>\n",
       "      <td>3300</td>\n",
       "      <td>4068</td>\n",
       "      <td>0.552117</td>\n",
       "      <td>0.627294</td>\n",
       "      <td>0.587310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>2715</td>\n",
       "      <td>1890</td>\n",
       "      <td>3133</td>\n",
       "      <td>0.623731</td>\n",
       "      <td>0.535739</td>\n",
       "      <td>0.576396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown            7400            5639          10130   0.642400   \n",
       "0    Feminine             635            1174           2362   0.667986   \n",
       "0   Masculine            2417            3300           4068   0.552117   \n",
       "0  Occupation            2715            1890           3133   0.623731   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.577867  0.608427  \n",
       "0  0.788121  0.723098  \n",
       "0  0.627294  0.587310  \n",
       "0  0.535739  0.576396  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['Unknown', 'Feminine', 'Masculine', 'Occupation']\n",
    "pred_perso_labels = utils.isPredictedInExpected(pred_perso_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine and save the performance measures and loose predictions (labels, not BIO tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_loose_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_{c}_baseline_fastText{d}_loose_agmt.csv\".format(a=a, d=d, c=category))\n",
    "pred_perso_labels.to_csv(predictions_dir+\"crf_{a}_{c}_baseline_fastText{d}_loose_evaluation.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### *For train-dev-test (i.e., 40-40-20) approach*\n",
    "\n",
    "<a id=\"ii\"></a>\n",
    "## II. Predict Over All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = utils1.zip2FeaturesAndTarget(df_test_grouped, \"tag\")\n",
    "X_test = [utils1.extractSentenceFeatures(sentence) for sentence in test_sentences]  # Features\n",
    "y_test = [utils1.extractSentenceTargets(sentence) for sentence in test_sentences]   # Target\n",
    "# Combine all data subsets' features and targets\n",
    "X_all = X_train+X_dev+X_test\n",
    "y_all = y_train+y_dev+y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = clf_perso.predict(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.5211899648086513\n",
      "  - Prec: 0.5561442804328083\n",
      "  - Rec 0.49419097345616575\n"
     ]
    }
   ],
   "source": [
    "print(\"  - F1:\", metrics.flat_f1_score(y_all, all_predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_all, all_predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_all, all_predictions, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_grouped = df_train_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "df_train_grouped = df_train_grouped.reset_index()\n",
    "df_dev_grouped = df_dev_grouped.drop(columns=\"tag_{}_predicted\".format(target_labels))\n",
    "df_dev_grouped = df_dev_grouped.reset_index()\n",
    "df_test_grouped = df_test_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "df_all_grouped = pd.concat([df_train_grouped, df_dev_grouped, df_test_grouped])\n",
    "# df_all_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Scope</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>and</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>Contents</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>Sermons</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token_id  sentence tag_pers_o_expected pred_ling_tag  \\\n",
       "sentence_id                                                        \n",
       "2                 16     Scope                 [O]           [O]   \n",
       "2                 17       and                 [O]           [O]   \n",
       "2                 18  Contents                 [O]           [O]   \n",
       "2                 19         :                 [O]           [O]   \n",
       "2                 20   Sermons                 [O]           [O]   \n",
       "\n",
       "            tag_pers_o_predicted  \n",
       "sentence_id                       \n",
       "2                              O  \n",
       "2                              O  \n",
       "2                              O  \n",
       "2                              O  \n",
       "2                              O  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_grouped.insert(len(df_all_grouped.columns), \"tag_pers_o_predicted\", all_predictions)\n",
    "df_all_grouped = df_all_grouped.set_index(\"sentence_id\")\n",
    "df_all_exploded = df_all_grouped.explode(list(df_all_grouped.columns))\n",
    "df_all_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_exploded = df_all_exploded.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_{t}_baseline_fastText{d}_predictions_ALLDATA.csv\".format(a=a, t=target_labels, d=d)\n",
    "df_all_exploded.to_csv(config.experiment1_output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the manual annotations, the predicted labels will be deemed incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics for each category of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso = df_all_exploded.copy()\n",
    "pred_perso = pred_perso.fillna(\"O\")\n",
    "pred_perso = utils.isPredictedInExpected(pred_perso, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>2082</td>\n",
       "      <td>2433</td>\n",
       "      <td>4549</td>\n",
       "      <td>0.651533</td>\n",
       "      <td>0.686020</td>\n",
       "      <td>0.668332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>3959</td>\n",
       "      <td>3797</td>\n",
       "      <td>7738</td>\n",
       "      <td>0.670828</td>\n",
       "      <td>0.661537</td>\n",
       "      <td>0.666150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>88</td>\n",
       "      <td>183</td>\n",
       "      <td>392</td>\n",
       "      <td>0.681739</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.743128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>337</td>\n",
       "      <td>1035</td>\n",
       "      <td>1620</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.827798</td>\n",
       "      <td>0.702515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>678</td>\n",
       "      <td>875</td>\n",
       "      <td>1356</td>\n",
       "      <td>0.607799</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.635873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>1491</td>\n",
       "      <td>1339</td>\n",
       "      <td>2103</td>\n",
       "      <td>0.610982</td>\n",
       "      <td>0.585142</td>\n",
       "      <td>0.597783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>896</td>\n",
       "      <td>623</td>\n",
       "      <td>1634</td>\n",
       "      <td>0.723970</td>\n",
       "      <td>0.645850</td>\n",
       "      <td>0.682682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>1304</td>\n",
       "      <td>892</td>\n",
       "      <td>1844</td>\n",
       "      <td>0.673977</td>\n",
       "      <td>0.585769</td>\n",
       "      <td>0.626785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     B-Unknown            2082            2433           4549   0.651533   \n",
       "0     I-Unknown            3959            3797           7738   0.670828   \n",
       "0    B-Feminine              88             183            392   0.681739   \n",
       "0    I-Feminine             337            1035           1620   0.610169   \n",
       "0   B-Masculine             678             875           1356   0.607799   \n",
       "0   I-Masculine            1491            1339           2103   0.610982   \n",
       "0  B-Occupation             896             623           1634   0.723970   \n",
       "0  I-Occupation            1304             892           1844   0.673977   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.686020  0.668332  \n",
       "0  0.661537  0.666150  \n",
       "0  0.816667  0.743128  \n",
       "0  0.827798  0.702515  \n",
       "0  0.666667  0.635873  \n",
       "0  0.585142  0.597783  \n",
       "0  0.645850  0.682682  \n",
       "0  0.585769  0.626785  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso, \"_merge\", pers_o_label_subset[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(pers_o_label_subset)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso, \"_merge\", pers_o_label_subset[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_stats.to_csv(\n",
    "    config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_strict_agmt_ALLDATA.csv\".format(a=a, c=category, d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Annotation Agreement\n",
    "\n",
    "Calculate agreement at the annotation level, so if the model labels any word correctly from a manually annotated text span, that annotation is recorded as being correctly labeled (`true positive`).  Note whether the models' labels are an `exact_match`, `label_match`, `category_match` or `mismatch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the annotation data:\n",
    "\n",
    "*Note: `ann_id` of `9999` indicates no annotation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the annotation data by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "perso_all = pd.concat([perso_train, perso_dev, perso_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778803, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>[B-Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>8</td>\n",
       "      <td>[I-Unknown]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id  token_id          tag\n",
       "0            0   99999         0          [O]\n",
       "1            0   99999         1          [O]\n",
       "2            0   99999         2          [O]\n",
       "3            1   14384         7  [B-Unknown]\n",
       "4            1   14384         8  [I-Unknown]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ann = perso_all[[\"sentence_id\", \"ann_id\", \"token_id\", \"tag\"]]\n",
    "df_ann = utils.implodeDataFrame(df_ann, [\"sentence_id\", \"ann_id\", \"token_id\"])\n",
    "df_ann = df_ann.reset_index()\n",
    "print(df_ann.shape)\n",
    "df_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the columns of the annotation and prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>604541</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604542</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604543</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298617</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298618</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id       token tag_pers_o_expected pred_ling_tag  \\\n",
       "604541            0         0  Identifier                 [O]           [O]   \n",
       "604542            0         1           :                 [O]           [O]   \n",
       "604543            0         2         AA5                 [O]           [O]   \n",
       "298617            1         3       Title                 [O]           [O]   \n",
       "298618            1         4           :                 [O]           [O]   \n",
       "\n",
       "       tag_pers_o_predicted         _merge  \n",
       "604541                    O  true negative  \n",
       "604542                    O  true negative  \n",
       "604543                    O  true negative  \n",
       "298617                    O  true negative  \n",
       "298618                    O  true negative  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename `sentence` column `token`\n",
    "pred_perso = pred_perso.rename(columns={\"sentence\":\"token\"}).sort_values(by=\"token_id\")\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data, adding the annotation IDs (`ann_id` column) to the prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [\"sentence_id\", \"token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso.join(df_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag\"])  # duplicate of tag_expected\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_expected\"].isna()].shape[0] == 0\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.explode([\"tag_pers_o_expected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the BIO tags to label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "pred_labels = list(pred_perso_ann[\"tag_{}_predicted\".format(category)])\n",
    "pred_labels = [label if label == \"O\" else label[2:] for label in pred_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_predicted\".format(category), pred_labels)\n",
    "# Get the lists of expected labels\n",
    "exp_labels = list(pred_perso_ann[\"tag_{}_expected\".format(category)])\n",
    "exp_labels = [label if label == \"O\" else label[2:] for label in exp_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_expected\".format(category), exp_labels)\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_ann = utils.implodeDataFrame(pred_perso_ann, [\"sentence_id\", \"ann_id\"])\n",
    "pred_perso_ann = pred_perso_ann.reset_index()\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreements and disagreements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_types_perso, agmt_labels_perso = utils1.getAnnotationAgreement(pred_perso_ann, \"label_pers_o_predicted\", \"label_pers_o_expected\")\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"annotation_agreement\", agmt_types_perso)\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"agreement_label\", agmt_labels_perso)\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>false negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>false positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>10080</td>\n",
       "      <td>15546</td>\n",
       "      <td>5003</td>\n",
       "      <td>0.756533</td>\n",
       "      <td>0.606649</td>\n",
       "      <td>0.673351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person Name</td>\n",
       "      <td>8551</td>\n",
       "      <td>13603</td>\n",
       "      <td>4324</td>\n",
       "      <td>0.758800</td>\n",
       "      <td>0.614020</td>\n",
       "      <td>0.678775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>3749</td>\n",
       "      <td>8755</td>\n",
       "      <td>2649</td>\n",
       "      <td>0.767713</td>\n",
       "      <td>0.700176</td>\n",
       "      <td>0.732391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>817</td>\n",
       "      <td>1636</td>\n",
       "      <td>593</td>\n",
       "      <td>0.733961</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.698847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>3985</td>\n",
       "      <td>3212</td>\n",
       "      <td>1082</td>\n",
       "      <td>0.748020</td>\n",
       "      <td>0.446297</td>\n",
       "      <td>0.559046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>1529</td>\n",
       "      <td>1943</td>\n",
       "      <td>679</td>\n",
       "      <td>0.741037</td>\n",
       "      <td>0.559620</td>\n",
       "      <td>0.637676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels  false negative  true positive  false positive  precision  \\\n",
       "0          all           10080          15546            5003   0.756533   \n",
       "0  Person Name            8551          13603            4324   0.758800   \n",
       "0      Unknown            3749           8755            2649   0.767713   \n",
       "0     Feminine             817           1636             593   0.733961   \n",
       "0    Masculine            3985           3212            1082   0.748020   \n",
       "0   Occupation            1529           1943             679   0.741037   \n",
       "\n",
       "     recall       f_1  \n",
       "0  0.606649  0.673351  \n",
       "0  0.614020  0.678775  \n",
       "0  0.700176  0.732391  \n",
       "0  0.666938  0.698847  \n",
       "0  0.446297  0.559046  \n",
       "0  0.559620  0.637676  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_perso_all = utils1.getAnnotationAgreementMetrics(pred_perso_ann, \"all\")\n",
    "metrics_perso_pn = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[~(pred_perso_ann.agreement_label.isin([\"Occupation\",\"O\"]))], \"Person Name\")\n",
    "metrics_perso_unk = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Unknown\"], \"Unknown\")\n",
    "metrics_perso_fem = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Feminine\"], \"Feminine\")\n",
    "metrics_perso_mas = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Masculine\"], \"Masculine\")\n",
    "metrics_perso_occ = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Occupation\"], \"Occupation\")\n",
    "metrics_perso = pd.concat([metrics_perso_all, metrics_perso_pn, metrics_perso_unk, metrics_perso_fem, metrics_perso_mas, metrics_perso_occ])\n",
    "metrics_perso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_perso.to_csv(\n",
    "    config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_annot_agmt.csv\".format(a=a, d=d, c=category)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Loose, Each Label\n",
    "\n",
    "Generalize the tokens' BIO tags to the labels and calculate agreement scores for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels = pred_perso.drop(columns=[\"_merge\"])\n",
    "tag_exp = list(pred_perso_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_perso_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_pers_labels.loc[pred_pers_labels.label_personname_predicted == \"Feminine\"].head()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the agreement metrics at the label level for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>6017</td>\n",
       "      <td>5487</td>\n",
       "      <td>13030</td>\n",
       "      <td>0.703678</td>\n",
       "      <td>0.684097</td>\n",
       "      <td>0.693749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>422</td>\n",
       "      <td>874</td>\n",
       "      <td>2356</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.848092</td>\n",
       "      <td>0.784288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2159</td>\n",
       "      <td>1942</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.657677</td>\n",
       "      <td>0.633447</td>\n",
       "      <td>0.645334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>2200</td>\n",
       "      <td>1381</td>\n",
       "      <td>3612</td>\n",
       "      <td>0.723413</td>\n",
       "      <td>0.621473</td>\n",
       "      <td>0.668579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown            6017            5487          13030   0.703678   \n",
       "0    Feminine             422             874           2356   0.729412   \n",
       "0   Masculine            2159            1942           3731   0.657677   \n",
       "0  Occupation            2200            1381           3612   0.723413   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.684097  0.693749  \n",
       "0  0.848092  0.784288  \n",
       "0  0.633447  0.645334  \n",
       "0  0.621473  0.668579  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['Unknown', 'Feminine', 'Masculine', 'Occupation']\n",
    "pred_perso_labels = utils.isPredictedInExpected(pred_perso_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine and save the performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_stats.to_csv(\n",
    "    config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_loose_agmt.csv\".format(a=a, d=d, c=category)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Past error analysis on Person Name document classifiers would suggest that there's a mix-up in the manual annotations between 'Masculine' and 'Unknown,' with many Masculine-labeled names actually needing an 'Unknown' label based on the annotation descriptions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

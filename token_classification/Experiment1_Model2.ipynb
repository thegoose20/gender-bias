{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1, Model 2\n",
    "\n",
    "#### Model Setup\n",
    "\n",
    "Run models in the following order, using their output labels as features for the next model:\n",
    "\n",
    "1. Multilabel Linguistic Classifier\n",
    "2. Multiclass Person Name + Occupation Sequence Classifier\n",
    "3. Multilabel Stereotype + Omission Document Classifier\n",
    "\n",
    "***\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/experiment_input/`\n",
    "    * Prediction Data: Data: under directory `../data/token_clf_data/model_output/experiment1/`\n",
    "* Word Embeddings\n",
    "    * Custom fastText (word2vec with subwords) embeddings of 100 dimensions trained on the CRC Archives catalog's descriptive metadata (harvested October 2020)\n",
    "    \n",
    "***\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I.](#i) Person Name + Occupation Classifier\n",
    "* [Preprocessing](#prep)\n",
    "* [Training & Prediction](#tp)\n",
    "* [Evaluation](#eval)\n",
    "\n",
    "[II.](#ii) Predict Over All Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load programming resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, utils1, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For classification\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define resources for the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(config.experiment_input_path).mkdir(parents=True, exist_ok=True)    # For train, devtest, and blind test data\n",
    "predictions_dir = config.experiment1_path+\"5fold/output/\"\n",
    "Path(predictions_dir).mkdir(parents=True, exist_ok=True)  # For predictions\n",
    "agreement_dir = config.experiment1_path+\"5fold/agreement/\"\n",
    "Path(agreement_dir).mkdir(parents=True, exist_ok=True)    # For agreement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_o_label_subset = [\n",
    "    \"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \n",
    "    \"B-Masculine\", \"I-Masculine\", \"B-Occupation\", \"I-Occupation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_label_tags = {\n",
    "    \"Gendered-Pronoun\": [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"], \"Gendered-Role\": [\"B-Gendered-Role\", \"I-Gendered-Role\"],\"Generalization\": [\"B-Generalization\", \"I-Generalization\"]\n",
    "    }\n",
    "pers_o_label_tags = {\n",
    "    \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "     \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100                   # dimensions of word embeddings (should match utils1.py) for file names\n",
    "target_labels = \"pers_o\"  # for file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"i\"></a>\n",
    "## I. Person Name + Occupation Labels\n",
    "\n",
    "Train a multiclass sequence classifier, using Conditional Random Field with Adaptive Regularization of Weight Vectors (AROW), on the Person Name and Occupation labels, **passing in the Linguistic labels (not specific BIO label-tag pair) from the previous model's predictions as features to this model.**\n",
    "\n",
    "Multiclass is a suitable setup for these labels because they are mutually exclusive (no one token should have more than one of these labels).  The sequence classifier with AROW was the highest performing for past algorithm experiments with sequence classifiers for Person Name and Occupation labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Linguistic features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"rf\"\n",
    "# ling_filename = config.experiment1_agmt_path+\"cc-{a}_ling_baseline_fastText{d}_evaluation_ALLDATA.csv\".format(a=a,d=d)\n",
    "ling_filename = predictions_dir+\"cc-{a}_ling_baseline_fastText{d}_evaluation_loose.csv\".format(a=a, d=d)\n",
    "ling_eval_df = pd.read_csv(ling_filename, usecols=[\"token_id\", \"predicted_tag\"])\n",
    "# Replace tags with labels\n",
    "for label,tags in ling_label_tags.items():\n",
    "    for tag in tags:\n",
    "        ling_eval_df[\"predicted_tag\"] = ling_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "ling_features = ling_eval_df.rename(columns={\"predicted_tag\":\"pred_ling_tag\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(779270, 10) (779270, 10)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(config.tokc_path+\"experiment_input/token_train.csv\", index_col=0)\n",
    "# dev_df =  pd.read_csv(config.tokc_path+\"experiment_input/token_validate.csv\", index_col=0)\n",
    "# test_df = pd.read_csv(config.tokc_path+\"experiment_input/token_test.csv\", index_col=0)\n",
    "df = pd.read_csv(config.tokc_path+\"experiment_input/token_5fold.csv\", index_col=0)\n",
    "perso_df = utils1.selectDataForLabels(df, \"tag\", pers_o_label_subset)\n",
    "# perso_train = utils1.selectDataForLabels(train_df, \"tag\", pers_o_label_subset)\n",
    "# perso_dev = utils1.selectDataForLabels(dev_df, \"tag\", pers_o_label_subset)\n",
    "# perso_test = utils1.selectDataForLabels(test_df, \"tag\", pers_o_label_subset)\n",
    "# print(perso_train.shape, perso_dev.shape, perso_test.shape)\n",
    "print(df.shape, perso_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the linguistic labels (features) to the model input and evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                   769249\n",
       "Gendered-Pronoun      4809\n",
       "Gendered-Role         4736\n",
       "Generalization         530\n",
       "Name: pred_ling_tag, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perso_train = perso_train.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "# assert perso_train.loc[perso_train.tag.isna()].shape[0] == 0\n",
    "# # perso_train.head()\n",
    "# perso_dev = perso_dev.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "# assert perso_dev.loc[perso_dev.tag.isna()].shape[0] == 0\n",
    "# perso_test = perso_test.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "# assert perso_test.loc[perso_test.tag.isna()].shape[0] == 0\n",
    "perso_df = perso_df.join(ling_features.set_index(\"token_id\"), on=\"token_id\", how=\"left\")\n",
    "assert perso_df.loc[perso_df.tag.isna()].shape[0] == 0\n",
    "perso_df.pred_ling_tag = perso_df.pred_ling_tag.fillna(\"O\")\n",
    "perso_df.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# perso_train.pred_ling_tag = perso_train.pred_ling_tag.fillna(\"O\")\n",
    "# perso_train.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# perso_dev.pred_ling_tag = perso_dev.pred_ling_tag.fillna(\"O\")\n",
    "# perso_dev.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# perso_test.pred_ling_tag = perso_test.pred_ling_tag.fillna(\"O\")\n",
    "# perso_test.pred_ling_tag.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prep\"></a>\n",
    "### Preprocessing\n",
    "\n",
    "Group data by token and then by sentence, so each sentence is a list of tokens and each token has a list of tags associated with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = perso_train.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# dev_df = perso_dev.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# test_df = perso_test.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "perso_data = perso_df.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "perso_token_groups = utils.implodeDataFrame(perso_data, [\"token_id\", \"sentence_id\", \"token\", \"fold\"]).reset_index()\n",
    "# perso_token_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42030, 6)\n"
     ]
    }
   ],
   "source": [
    "perso_grouped = utils.implodeDataFrame(perso_token_groups, [\"sentence_id\", \"fold\"]).reset_index()\n",
    "perso_grouped = perso_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# perso_grouped.head()\n",
    "print(perso_grouped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the five groups of training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['split0' 'split1' 'split2' 'split3' 'split4']\n"
     ]
    }
   ],
   "source": [
    "split_col = \"fold\"\n",
    "splits = perso_grouped[split_col].unique()\n",
    "splits.sort()\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0, test0 = list(splits[:4]), splits[4]\n",
    "train1, test1 = list(splits[1:]), splits[0]\n",
    "train2, test2 = list(splits[2:])+[splits[0]], splits[1]\n",
    "train3, test3 = list(splits[3:])+list(splits[:2]), splits[2]\n",
    "train4, test4 = [splits[4]]+list(splits[:3]), splits[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['split0', 'split1', 'split2', 'split3'], 'split4')\n",
      "(['split1', 'split2', 'split3', 'split4'], 'split0')\n",
      "(['split2', 'split3', 'split4', 'split0'], 'split1')\n",
      "(['split3', 'split4', 'split0', 'split1'], 'split2')\n",
      "(['split4', 'split0', 'split1', 'split2'], 'split3')\n"
     ]
    }
   ],
   "source": [
    "runs = [(train0, test0), (train1, test1), (train2, test2), (train3, test3), (train4, test4)]\n",
    "for run in runs:\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_train_token_groups = utils.implodeDataFrame(train_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_train_token_groups = df_train_token_groups.reset_index()\n",
    "# # df_train_token_groups.head()\n",
    "# df_train_grouped = utils.implodeDataFrame(df_train_token_groups, ['sentence_id'])\n",
    "# df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_train_grouped = df_train_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_dev_token_groups = utils.implodeDataFrame(dev_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_dev_token_groups = df_dev_token_groups.reset_index()\n",
    "# df_dev_grouped = utils.implodeDataFrame(df_dev_token_groups, ['sentence_id'])\n",
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_dev_grouped = df_dev_grouped.reset_index()\n",
    "# # df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_test_token_groups = utils.implodeDataFrame(test_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_test_token_groups = df_test_token_groups.reset_index()\n",
    "# df_test_grouped = utils.implodeDataFrame(df_test_token_groups, ['sentence_id'])\n",
    "# df_test_grouped = df_test_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_test_grouped = df_test_grouped.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the linguistic label and BIO tags together with the tokens so each sentence item is a tuple: `(TOKEN, LING_LABEL, TAG_LIST)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_sentences_perso = utils1.zip2FeaturesAndTarget(df_train_grouped, \"tag\")\n",
    "# print(train_sentences_perso[2][:3])\n",
    "# dev_sentences_perso = utils1.zip2FeaturesAndTarget(df_dev_grouped, \"tag\")\n",
    "# print(dev_sentences_perso[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_sentences = train_sentences_perso\n",
    "# dev_sentences = dev_sentences_perso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Features\n",
    "# X_train = [utils1.extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "# X_dev = [utils1.extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# # Target\n",
    "# y_train = [utils1.extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "# y_dev = [utils1.extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tp\"></a>\n",
    "### Training & Prediction\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Person Name** category of tags.  We'll set the max iterations to 100 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: ['split4', 'split0', 'split1', 'split2']\n",
      "Predicting on: split3\n",
      "Predictions for split3 saved!\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "\n",
    "# Specify the run one at a time (with for loop, kernel dies)\n",
    "run = runs[4]  # 0, 1, 2, 3\n",
    "\n",
    "# Get the train (80%) and test (20%) subsets of data\n",
    "# with Person Name tags as targets and Linguistic labels as features\n",
    "train_splits, test_split = run[0], run[1]\n",
    "print(\"Training on:\", train_splits)\n",
    "train_df = perso_grouped.loc[perso_grouped[split_col].isin(train_splits)]\n",
    "dev_df = perso_grouped.loc[perso_grouped[split_col] == test_split]\n",
    "\n",
    "# Zip the linguistic label and BIO tags together with the tokens so each \n",
    "# sentence item is a tuple: `(TOKEN, LING_LABEL, TAG_LIST)`\n",
    "train_sentences = utils1.zip2FeaturesAndTarget(train_df, \"tag\")\n",
    "dev_sentences = utils1.zip2FeaturesAndTarget(dev_df, \"tag\")\n",
    "\n",
    "# Extract features\n",
    "X_train = [utils1.extractSentenceFeatures(s) for s in train_sentences] \n",
    "X_dev = [utils1.extractSentenceFeatures(s) for s in dev_sentences]\n",
    "\n",
    "# Extract targets\n",
    "y_train = [utils1.extractSentenceTargets(s) for s in train_sentences]\n",
    "y_dev = [utils1.extractSentenceTargets(s) for s in dev_sentences]\n",
    "\n",
    "# Train a classification model\n",
    "a = \"arow\"\n",
    "clf = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict with the trained model\n",
    "print(\"Predicting on:\", test_split)\n",
    "predictions = clf.predict(X_dev)\n",
    "\n",
    "dev_df = dev_df.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "dev_df.insert(len(dev_df.columns), \"tag_pers_o_predicted\", predictions)\n",
    "dev_df = dev_df.set_index([\"sentence_id\", \"fold\"])\n",
    "dev_df_exploded = dev_df.explode(list(dev_df.columns))\n",
    "\n",
    "if pred_df.shape[0] > 0:\n",
    "    pred_df = pd.concat([pred_df, dev_df_exploded])\n",
    "else:\n",
    "    pred_df = dev_df_exploded\n",
    "\n",
    "assert pred_df.loc[pred_df[\"tag_pers_o_expected\"].isna()].shape[0] == 0, \"Any NaN values should be replaced with 'O'\"\n",
    "\n",
    "filename = \"crf_{a}_{t}_baseline_fastText{d}_predictions_{s}.csv\".format(a=a, t=target_labels, d=d, s=test_split)\n",
    "pred_df.to_csv(predictions_dir+filename)\n",
    "\n",
    "print(\"Predictions for {} saved!\".format(test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(753521, 6)\n"
     ]
    }
   ],
   "source": [
    "pred_df0 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split0.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df1 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split1.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df2 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split2.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df3 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split3.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df4 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_predictions_split4.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_perso = pd.concat([pred_df0, pred_df1, pred_df2, pred_df3, pred_df4])\n",
    "print(pred_perso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[B-Masculine]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>B-Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[I-Masculine]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>I-Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id sentence tag_pers_o_expected pred_ling_tag  \\\n",
       "0            8  split0       233    James       [B-Masculine]           [O]   \n",
       "1            8  split0       234    Whyte       [I-Masculine]           [O]   \n",
       "2            8  split0       235      was                 [O]           [O]   \n",
       "3            8  split0       236   called                 [O]           [O]   \n",
       "4            8  split0       237     upon                 [O]           [O]   \n",
       "\n",
       "  tag_pers_o_predicted  \n",
       "0            B-Unknown  \n",
       "1            I-Unknown  \n",
       "2                    O  \n",
       "3                    O  \n",
       "4                    O  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso = pred_perso.reset_index()\n",
    "pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_pers_o_expected\")\n",
    "pred_perso = utils.getColumnValuesAsLists(pred_perso, \"pred_ling_tag\")\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pred_perso.shape[0] == len(pred_perso.token_id.unique()), \"There should be one row per token.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# a = \"arow\"\n",
    "# clf_perso = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "# try:\n",
    "#     clf_perso.fit(X_train, y_train)\n",
    "# except AttributeError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# targets = list(clf_perso.classes_)\n",
    "# targets.remove('O')\n",
    "# print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# predictions = clf_perso.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(\"  - F1:\", metrics.flat_f1_score(y_dev, predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Prec:\", metrics.flat_precision_score(y_dev, predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Rec\", metrics.flat_recall_score(y_dev, predictions, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "# df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_pers_o_predicted\", predictions)\n",
    "# # df_dev_grouped.head()\n",
    "# df_dev_grouped = df_dev_grouped.set_index(\"sentence_id\")\n",
    "# df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns))\n",
    "# # df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# filename = \"crf_{a}_{t}_baseline_fastText{d}_predictions.csv\".format(a=a, t=target_labels, d=d)\n",
    "# df_dev_exploded.to_csv(config.experiment1_output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval\"></a>\n",
    "### Evaluation\n",
    "#### Evaluate: Strict, Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# filename = \"crf_{a}_{c}_baseline_fastText{d}_predictions.csv\".format(a=a, c=category, d=d)\n",
    "# pred_perso = pd.read_csv(config.experiment1_output_path+filename)\n",
    "# pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_{}_expected\".format(category))\n",
    "# # pred_pers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics for each category of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso = pred_perso.fillna(\"O\")\n",
    "pred_perso = utils.isPredictedInExpected(pred_perso, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the combined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_{c}_baseline_fastText{d}_predictions.csv\".format(a=a, c=category, d=d)\n",
    "pred_perso.to_csv(predictions_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>2149</td>\n",
       "      <td>2933</td>\n",
       "      <td>3788</td>\n",
       "      <td>0.563607</td>\n",
       "      <td>0.638033</td>\n",
       "      <td>0.598515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>4394</td>\n",
       "      <td>4529</td>\n",
       "      <td>6589</td>\n",
       "      <td>0.592643</td>\n",
       "      <td>0.599927</td>\n",
       "      <td>0.596263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>113</td>\n",
       "      <td>394</td>\n",
       "      <td>637</td>\n",
       "      <td>0.617847</td>\n",
       "      <td>0.849333</td>\n",
       "      <td>0.715328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>490</td>\n",
       "      <td>970</td>\n",
       "      <td>1434</td>\n",
       "      <td>0.596506</td>\n",
       "      <td>0.745322</td>\n",
       "      <td>0.662662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>644</td>\n",
       "      <td>1465</td>\n",
       "      <td>1839</td>\n",
       "      <td>0.556598</td>\n",
       "      <td>0.740636</td>\n",
       "      <td>0.635562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>1425</td>\n",
       "      <td>3058</td>\n",
       "      <td>2181</td>\n",
       "      <td>0.416301</td>\n",
       "      <td>0.604825</td>\n",
       "      <td>0.493160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>1021</td>\n",
       "      <td>922</td>\n",
       "      <td>1490</td>\n",
       "      <td>0.617745</td>\n",
       "      <td>0.593389</td>\n",
       "      <td>0.605322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>1621</td>\n",
       "      <td>1526</td>\n",
       "      <td>1488</td>\n",
       "      <td>0.493696</td>\n",
       "      <td>0.478610</td>\n",
       "      <td>0.486036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     B-Unknown            2149            2933           3788   0.563607   \n",
       "0     I-Unknown            4394            4529           6589   0.592643   \n",
       "0    B-Feminine             113             394            637   0.617847   \n",
       "0    I-Feminine             490             970           1434   0.596506   \n",
       "0   B-Masculine             644            1465           1839   0.556598   \n",
       "0   I-Masculine            1425            3058           2181   0.416301   \n",
       "0  B-Occupation            1021             922           1490   0.617745   \n",
       "0  I-Occupation            1621            1526           1488   0.493696   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.638033  0.598515  \n",
       "0  0.599927  0.596263  \n",
       "0  0.849333  0.715328  \n",
       "0  0.745322  0.662662  \n",
       "0  0.740636  0.635562  \n",
       "0  0.604825  0.493160  \n",
       "0  0.593389  0.605322  \n",
       "0  0.478610  0.486036  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso, \"_merge\", pers_o_label_subset[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(pers_o_label_subset)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso, \"_merge\", pers_o_label_subset[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_strict_agmt.csv\".format(a=a, c=category, d=d)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_{c}_baseline_fastText{d}_strict_agmt.csv\".format(a=a, c=category, d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Annotation Agreement\n",
    "\n",
    "Calculate agreement at the annotation level, so if the model labels any word correctly from a manually annotated text span, that annotation is recorded as being correctly labeled (`true positive`).  Note whether the models' labels are an `exact_match`, `label_match`, `category_match` or `mismatch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the annotation data:\n",
    "\n",
    "*Note: `ann_id` of `9999` indicates no annotation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the annotation data by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778803, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>[B-Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>8</td>\n",
       "      <td>[I-Unknown]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id  token_id          tag\n",
       "0            0   99999         0          [O]\n",
       "1            0   99999         1          [O]\n",
       "2            0   99999         2          [O]\n",
       "3            1   14384         7  [B-Unknown]\n",
       "4            1   14384         8  [I-Unknown]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ann = perso_df[[\"sentence_id\", \"ann_id\", \"token_id\", \"tag\"]]  #perso_dev[[\"sentence_id\", \"ann_id\", \"token_id\", \"tag\"]]\n",
    "df_ann = utils.implodeDataFrame(df_ann, [\"sentence_id\", \"ann_id\", \"token_id\"])\n",
    "df_ann = df_ann.reset_index()\n",
    "print(df_ann.shape)\n",
    "df_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the columns of the dev and prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename `sentence` column `token`\n",
    "pred_perso = pred_perso.rename(columns={\"sentence\":\"token\"})\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data, adding the annotation IDs (`ann_id` column) to the prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [\"sentence_id\", \"token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[B-Masculine]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "      <td>14387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[I-Masculine]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "      <td>14387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id   token tag_pers_o_expected pred_ling_tag  \\\n",
       "0            8  split0       233   James       [B-Masculine]           [O]   \n",
       "1            8  split0       234   Whyte       [I-Masculine]           [O]   \n",
       "2            8  split0       235     was                 [O]           [O]   \n",
       "3            8  split0       236  called                 [O]           [O]   \n",
       "4            8  split0       237    upon                 [O]           [O]   \n",
       "\n",
       "  tag_pers_o_predicted          _merge  ann_id  \n",
       "0            B-Unknown  false positive   14387  \n",
       "1            I-Unknown  false positive   14387  \n",
       "2                    O   true negative   99999  \n",
       "3                    O   true negative   99999  \n",
       "4                    O   true negative   99999  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann = pred_perso.join(df_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag\"])  # duplicate of tag_expected\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_expected\"].isna()].shape[0] == 0\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>[O]</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "      <td>14387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>[O]</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "      <td>14387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>O</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>O</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id   token tag_pers_o_expected pred_ling_tag  \\\n",
       "0            8  split0       233   James         B-Masculine           [O]   \n",
       "1            8  split0       234   Whyte         I-Masculine           [O]   \n",
       "2            8  split0       235     was                   O           [O]   \n",
       "3            8  split0       236  called                   O           [O]   \n",
       "4            8  split0       237    upon                   O           [O]   \n",
       "\n",
       "  tag_pers_o_predicted          _merge  ann_id  \n",
       "0            B-Unknown  false positive   14387  \n",
       "1            I-Unknown  false positive   14387  \n",
       "2                    O   true negative   99999  \n",
       "3                    O   true negative   99999  \n",
       "4                    O   true negative   99999  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann = pred_perso_ann.explode([\"tag_pers_o_expected\"])\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the BIO tags to label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "pred_labels = list(pred_perso_ann[\"tag_{}_predicted\".format(category)])\n",
    "pred_labels = [label if label == \"O\" else label[2:] for label in pred_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_predicted\".format(category), pred_labels)\n",
    "# Get the lists of expected labels\n",
    "exp_labels = list(pred_perso_ann[\"tag_{}_expected\".format(category)])\n",
    "exp_labels = [label if label == \"O\" else label[2:] for label in exp_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_expected\".format(category), exp_labels)\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_ann = utils.implodeDataFrame(pred_perso_ann, [\"sentence_id\", \"ann_id\", \"fold\"])\n",
    "pred_perso_ann = pred_perso_ann.reset_index()\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreements and disagreements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>annotation_agreement</th>\n",
       "      <th>agreement_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>split4</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Identifier, :, AA5]</td>\n",
       "      <td>[[O], [O], [O]]</td>\n",
       "      <td>[true negative, true negative, true negative]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>split2</td>\n",
       "      <td>[7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...</td>\n",
       "      <td>[The, The, The, Very, Very, Very, Rev, Rev, Re...</td>\n",
       "      <td>[[O, O, O], [O, O, O], [O, O, O], [O, O, O], [...</td>\n",
       "      <td>[true negative, true negative, true negative, ...</td>\n",
       "      <td>[O, O, O, Unknown, Unknown, Unknown, Unknown, ...</td>\n",
       "      <td>[O, Unknown, Masculine, Unknown, Masculine, O,...</td>\n",
       "      <td>true positive</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24275</td>\n",
       "      <td>split2</td>\n",
       "      <td>[7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...</td>\n",
       "      <td>[The, The, The, Very, Very, Very, Rev, Rev, Re...</td>\n",
       "      <td>[[O, O, O], [O, O, O], [O, O, O], [O, O, O], [...</td>\n",
       "      <td>[true negative, true negative, true negative, ...</td>\n",
       "      <td>[O, O, O, Unknown, Unknown, Unknown, Unknown, ...</td>\n",
       "      <td>[O, Unknown, Masculine, Unknown, Masculine, O,...</td>\n",
       "      <td>true positive</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26233</td>\n",
       "      <td>split2</td>\n",
       "      <td>[9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 1...</td>\n",
       "      <td>[Rev, Rev, Rev, Rev, Prof, Prof, Prof, Prof, J...</td>\n",
       "      <td>[[O, O, O, O], [O, O, O, O], [O, O, O, O], [O,...</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Unknown, Unknown, Unknown, Unknown, Unknown, ...</td>\n",
       "      <td>[Unknown, Masculine, Unknown, O, Unknown, O, M...</td>\n",
       "      <td>true positive</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>52952</td>\n",
       "      <td>split2</td>\n",
       "      <td>[7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...</td>\n",
       "      <td>[The, The, The, Very, Very, Very, Rev, Rev, Re...</td>\n",
       "      <td>[[O, O, O], [O, O, O], [O, O, O], [O, O, O], [...</td>\n",
       "      <td>[true negative, true negative, true negative, ...</td>\n",
       "      <td>[O, O, O, Unknown, Unknown, Unknown, Unknown, ...</td>\n",
       "      <td>[O, Unknown, Masculine, Unknown, Masculine, O,...</td>\n",
       "      <td>true positive</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id    fold  \\\n",
       "0            0   99999  split4   \n",
       "1            1   14384  split2   \n",
       "2            1   24275  split2   \n",
       "3            1   26233  split2   \n",
       "4            1   52952  split2   \n",
       "\n",
       "                                            token_id  \\\n",
       "0                                          [0, 1, 2]   \n",
       "1  [7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...   \n",
       "2  [7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...   \n",
       "3  [9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 1...   \n",
       "4  [7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...   \n",
       "\n",
       "                                               token  \\\n",
       "0                               [Identifier, :, AA5]   \n",
       "1  [The, The, The, Very, Very, Very, Rev, Rev, Re...   \n",
       "2  [The, The, The, Very, Very, Very, Rev, Rev, Re...   \n",
       "3  [Rev, Rev, Rev, Rev, Prof, Prof, Prof, Prof, J...   \n",
       "4  [The, The, The, Very, Very, Very, Rev, Rev, Re...   \n",
       "\n",
       "                                       pred_ling_tag  \\\n",
       "0                                    [[O], [O], [O]]   \n",
       "1  [[O, O, O], [O, O, O], [O, O, O], [O, O, O], [...   \n",
       "2  [[O, O, O], [O, O, O], [O, O, O], [O, O, O], [...   \n",
       "3  [[O, O, O, O], [O, O, O, O], [O, O, O, O], [O,...   \n",
       "4  [[O, O, O], [O, O, O], [O, O, O], [O, O, O], [...   \n",
       "\n",
       "                                              _merge  \\\n",
       "0      [true negative, true negative, true negative]   \n",
       "1  [true negative, true negative, true negative, ...   \n",
       "2  [true negative, true negative, true negative, ...   \n",
       "3  [true positive, true positive, true positive, ...   \n",
       "4  [true negative, true negative, true negative, ...   \n",
       "\n",
       "                              label_pers_o_predicted  \\\n",
       "0                                          [O, O, O]   \n",
       "1  [O, O, O, Unknown, Unknown, Unknown, Unknown, ...   \n",
       "2  [O, O, O, Unknown, Unknown, Unknown, Unknown, ...   \n",
       "3  [Unknown, Unknown, Unknown, Unknown, Unknown, ...   \n",
       "4  [O, O, O, Unknown, Unknown, Unknown, Unknown, ...   \n",
       "\n",
       "                               label_pers_o_expected annotation_agreement  \\\n",
       "0                                          [O, O, O]        true negative   \n",
       "1  [O, Unknown, Masculine, Unknown, Masculine, O,...        true positive   \n",
       "2  [O, Unknown, Masculine, Unknown, Masculine, O,...        true positive   \n",
       "3  [Unknown, Masculine, Unknown, O, Unknown, O, M...        true positive   \n",
       "4  [O, Unknown, Masculine, Unknown, Masculine, O,...        true positive   \n",
       "\n",
       "  agreement_label  \n",
       "0               O  \n",
       "1       Masculine  \n",
       "2       Masculine  \n",
       "3       Masculine  \n",
       "4       Masculine  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agmt_types_perso, agmt_labels_perso = utils1.getAnnotationAgreement(pred_perso_ann, \"label_pers_o_predicted\", \"label_pers_o_expected\")\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"annotation_agreement\", agmt_types_perso)\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"agreement_label\", agmt_labels_perso)\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>false negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>false positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>9168</td>\n",
       "      <td>15783</td>\n",
       "      <td>6809</td>\n",
       "      <td>0.698610</td>\n",
       "      <td>0.632560</td>\n",
       "      <td>0.663946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person Name</td>\n",
       "      <td>7626</td>\n",
       "      <td>13910</td>\n",
       "      <td>5871</td>\n",
       "      <td>0.703200</td>\n",
       "      <td>0.645895</td>\n",
       "      <td>0.673331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>3722</td>\n",
       "      <td>7100</td>\n",
       "      <td>2534</td>\n",
       "      <td>0.736973</td>\n",
       "      <td>0.656071</td>\n",
       "      <td>0.694173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>721</td>\n",
       "      <td>1793</td>\n",
       "      <td>821</td>\n",
       "      <td>0.685922</td>\n",
       "      <td>0.713206</td>\n",
       "      <td>0.699298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>3183</td>\n",
       "      <td>5017</td>\n",
       "      <td>2516</td>\n",
       "      <td>0.666003</td>\n",
       "      <td>0.611829</td>\n",
       "      <td>0.637768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>1542</td>\n",
       "      <td>1873</td>\n",
       "      <td>938</td>\n",
       "      <td>0.666311</td>\n",
       "      <td>0.548463</td>\n",
       "      <td>0.601670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels  false negative  true positive  false positive  precision  \\\n",
       "0          all            9168          15783            6809   0.698610   \n",
       "0  Person Name            7626          13910            5871   0.703200   \n",
       "0      Unknown            3722           7100            2534   0.736973   \n",
       "0     Feminine             721           1793             821   0.685922   \n",
       "0    Masculine            3183           5017            2516   0.666003   \n",
       "0   Occupation            1542           1873             938   0.666311   \n",
       "\n",
       "     recall       f_1  \n",
       "0  0.632560  0.663946  \n",
       "0  0.645895  0.673331  \n",
       "0  0.656071  0.694173  \n",
       "0  0.713206  0.699298  \n",
       "0  0.611829  0.637768  \n",
       "0  0.548463  0.601670  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_perso_all = utils1.getAnnotationAgreementMetrics(pred_perso_ann, \"all\")\n",
    "metrics_perso_pn = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[~(pred_perso_ann.agreement_label.isin([\"Occupation\",\"O\"]))], \"Person Name\")\n",
    "metrics_perso_unk = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Unknown\"], \"Unknown\")\n",
    "metrics_perso_fem = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Feminine\"], \"Feminine\")\n",
    "metrics_perso_mas = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Masculine\"], \"Masculine\")\n",
    "metrics_perso_occ = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Occupation\"], \"Occupation\")\n",
    "metrics_perso = pd.concat([metrics_perso_all, metrics_perso_pn, metrics_perso_unk, metrics_perso_fem, metrics_perso_mas, metrics_perso_occ])\n",
    "metrics_perso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics and annotation-level evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_perso.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_annot_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "metrics_perso.to_csv(agreement_dir+\"crf_{a}_{c}_baseline_fastText{d}_annot_agmt.csv\".format(a=a, d=d, c=category))\n",
    "pred_perso_ann[[\"sentence_id\", \"ann_id\", \"fold\", \"token_id\", \"annotation_agreement\", \"agreement_label\"]].to_csv(\n",
    "    predictions_dir+\"crf_{a}_{c}_baseline_fastText{d}_annot_evaluation.csv\".format(a=a, d=d, c=category)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Loose, Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the tokens' BIO tags to the label, and calculate agreement scores for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels = pred_perso.drop(columns=[\"_merge\"])\n",
    "tag_exp = list(pred_perso_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_perso_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_pers_labels.loc[pred_pers_labels.label_personname_predicted == \"Feminine\"].head()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the agreement metrics at the label level for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>6526</td>\n",
       "      <td>6649</td>\n",
       "      <td>11190</td>\n",
       "      <td>0.627277</td>\n",
       "      <td>0.631632</td>\n",
       "      <td>0.629447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>602</td>\n",
       "      <td>1125</td>\n",
       "      <td>2310</td>\n",
       "      <td>0.672489</td>\n",
       "      <td>0.793269</td>\n",
       "      <td>0.727903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2062</td>\n",
       "      <td>4080</td>\n",
       "      <td>4463</td>\n",
       "      <td>0.522416</td>\n",
       "      <td>0.683985</td>\n",
       "      <td>0.592381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>2642</td>\n",
       "      <td>2239</td>\n",
       "      <td>3187</td>\n",
       "      <td>0.587357</td>\n",
       "      <td>0.546749</td>\n",
       "      <td>0.566326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown            6526            6649          11190   0.627277   \n",
       "0    Feminine             602            1125           2310   0.672489   \n",
       "0   Masculine            2062            4080           4463   0.522416   \n",
       "0  Occupation            2642            2239           3187   0.587357   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.631632  0.629447  \n",
       "0  0.793269  0.727903  \n",
       "0  0.683985  0.592381  \n",
       "0  0.546749  0.566326  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['Unknown', 'Feminine', 'Masculine', 'Occupation']\n",
    "pred_perso_labels = utils.isPredictedInExpected(pred_perso_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine and save the performance measures and loose predictions (labels, not BIO tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_loose_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_{c}_baseline_fastText{d}_loose_agmt.csv\".format(a=a, d=d, c=category))\n",
    "pred_perso_labels.to_csv(predictions_dir+\"crf_{a}_{c}_baseline_fastText{d}_loose_evaluation.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### *For train-dev-test (i.e., 40-40-20) approach*\n",
    "\n",
    "<a id=\"ii\"></a>\n",
    "## II. Predict Over All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = utils1.zip2FeaturesAndTarget(df_test_grouped, \"tag\")\n",
    "X_test = [utils1.extractSentenceFeatures(sentence) for sentence in test_sentences]  # Features\n",
    "y_test = [utils1.extractSentenceTargets(sentence) for sentence in test_sentences]   # Target\n",
    "# Combine all data subsets' features and targets\n",
    "X_all = X_train+X_dev+X_test\n",
    "y_all = y_train+y_dev+y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = clf_perso.predict(X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.5211899648086513\n",
      "  - Prec: 0.5561442804328083\n",
      "  - Rec 0.49419097345616575\n"
     ]
    }
   ],
   "source": [
    "print(\"  - F1:\", metrics.flat_f1_score(y_all, all_predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_all, all_predictions, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_all, all_predictions, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_grouped = df_train_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "df_train_grouped = df_train_grouped.reset_index()\n",
    "df_dev_grouped = df_dev_grouped.drop(columns=\"tag_{}_predicted\".format(target_labels))\n",
    "df_dev_grouped = df_dev_grouped.reset_index()\n",
    "df_test_grouped = df_test_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "df_all_grouped = pd.concat([df_train_grouped, df_dev_grouped, df_test_grouped])\n",
    "# df_all_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Scope</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>and</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>Contents</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>Sermons</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token_id  sentence tag_pers_o_expected pred_ling_tag  \\\n",
       "sentence_id                                                        \n",
       "2                 16     Scope                 [O]           [O]   \n",
       "2                 17       and                 [O]           [O]   \n",
       "2                 18  Contents                 [O]           [O]   \n",
       "2                 19         :                 [O]           [O]   \n",
       "2                 20   Sermons                 [O]           [O]   \n",
       "\n",
       "            tag_pers_o_predicted  \n",
       "sentence_id                       \n",
       "2                              O  \n",
       "2                              O  \n",
       "2                              O  \n",
       "2                              O  \n",
       "2                              O  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_grouped.insert(len(df_all_grouped.columns), \"tag_pers_o_predicted\", all_predictions)\n",
    "df_all_grouped = df_all_grouped.set_index(\"sentence_id\")\n",
    "df_all_exploded = df_all_grouped.explode(list(df_all_grouped.columns))\n",
    "df_all_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_exploded = df_all_exploded.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_{t}_baseline_fastText{d}_predictions_ALLDATA.csv\".format(a=a, t=target_labels, d=d)\n",
    "df_all_exploded.to_csv(config.experiment1_output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the manual annotations, the predicted labels will be deemed incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics for each category of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso = df_all_exploded.copy()\n",
    "pred_perso = pred_perso.fillna(\"O\")\n",
    "pred_perso = utils.isPredictedInExpected(pred_perso, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>2082</td>\n",
       "      <td>2433</td>\n",
       "      <td>4549</td>\n",
       "      <td>0.651533</td>\n",
       "      <td>0.686020</td>\n",
       "      <td>0.668332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>3959</td>\n",
       "      <td>3797</td>\n",
       "      <td>7738</td>\n",
       "      <td>0.670828</td>\n",
       "      <td>0.661537</td>\n",
       "      <td>0.666150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>88</td>\n",
       "      <td>183</td>\n",
       "      <td>392</td>\n",
       "      <td>0.681739</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.743128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>337</td>\n",
       "      <td>1035</td>\n",
       "      <td>1620</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.827798</td>\n",
       "      <td>0.702515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>678</td>\n",
       "      <td>875</td>\n",
       "      <td>1356</td>\n",
       "      <td>0.607799</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.635873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>1491</td>\n",
       "      <td>1339</td>\n",
       "      <td>2103</td>\n",
       "      <td>0.610982</td>\n",
       "      <td>0.585142</td>\n",
       "      <td>0.597783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>896</td>\n",
       "      <td>623</td>\n",
       "      <td>1634</td>\n",
       "      <td>0.723970</td>\n",
       "      <td>0.645850</td>\n",
       "      <td>0.682682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>1304</td>\n",
       "      <td>892</td>\n",
       "      <td>1844</td>\n",
       "      <td>0.673977</td>\n",
       "      <td>0.585769</td>\n",
       "      <td>0.626785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     B-Unknown            2082            2433           4549   0.651533   \n",
       "0     I-Unknown            3959            3797           7738   0.670828   \n",
       "0    B-Feminine              88             183            392   0.681739   \n",
       "0    I-Feminine             337            1035           1620   0.610169   \n",
       "0   B-Masculine             678             875           1356   0.607799   \n",
       "0   I-Masculine            1491            1339           2103   0.610982   \n",
       "0  B-Occupation             896             623           1634   0.723970   \n",
       "0  I-Occupation            1304             892           1844   0.673977   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.686020  0.668332  \n",
       "0  0.661537  0.666150  \n",
       "0  0.816667  0.743128  \n",
       "0  0.827798  0.702515  \n",
       "0  0.666667  0.635873  \n",
       "0  0.585142  0.597783  \n",
       "0  0.645850  0.682682  \n",
       "0  0.585769  0.626785  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso, \"_merge\", pers_o_label_subset[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(pers_o_label_subset)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso, \"_merge\", pers_o_label_subset[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_stats.to_csv(\n",
    "    config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_strict_agmt_ALLDATA.csv\".format(a=a, c=category, d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Annotation Agreement\n",
    "\n",
    "Calculate agreement at the annotation level, so if the model labels any word correctly from a manually annotated text span, that annotation is recorded as being correctly labeled (`true positive`).  Note whether the models' labels are an `exact_match`, `label_match`, `category_match` or `mismatch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the annotation data:\n",
    "\n",
    "*Note: `ann_id` of `9999` indicates no annotation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the annotation data by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "perso_all = pd.concat([perso_train, perso_dev, perso_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778803, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>2</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>[B-Unknown]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>8</td>\n",
       "      <td>[I-Unknown]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id  token_id          tag\n",
       "0            0   99999         0          [O]\n",
       "1            0   99999         1          [O]\n",
       "2            0   99999         2          [O]\n",
       "3            1   14384         7  [B-Unknown]\n",
       "4            1   14384         8  [I-Unknown]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ann = perso_all[[\"sentence_id\", \"ann_id\", \"token_id\", \"tag\"]]\n",
    "df_ann = utils.implodeDataFrame(df_ann, [\"sentence_id\", \"ann_id\", \"token_id\"])\n",
    "df_ann = df_ann.reset_index()\n",
    "print(df_ann.shape)\n",
    "df_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the columns of the annotation and prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>pred_ling_tag</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>604541</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604542</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604543</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298617</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298618</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id       token tag_pers_o_expected pred_ling_tag  \\\n",
       "604541            0         0  Identifier                 [O]           [O]   \n",
       "604542            0         1           :                 [O]           [O]   \n",
       "604543            0         2         AA5                 [O]           [O]   \n",
       "298617            1         3       Title                 [O]           [O]   \n",
       "298618            1         4           :                 [O]           [O]   \n",
       "\n",
       "       tag_pers_o_predicted         _merge  \n",
       "604541                    O  true negative  \n",
       "604542                    O  true negative  \n",
       "604543                    O  true negative  \n",
       "298617                    O  true negative  \n",
       "298618                    O  true negative  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename `sentence` column `token`\n",
    "pred_perso = pred_perso.rename(columns={\"sentence\":\"token\"}).sort_values(by=\"token_id\")\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data, adding the annotation IDs (`ann_id` column) to the prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [\"sentence_id\", \"token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso.join(df_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag\"])  # duplicate of tag_expected\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_expected\"].isna()].shape[0] == 0\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.explode([\"tag_pers_o_expected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the BIO tags to label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "pred_labels = list(pred_perso_ann[\"tag_{}_predicted\".format(category)])\n",
    "pred_labels = [label if label == \"O\" else label[2:] for label in pred_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_predicted\".format(category), pred_labels)\n",
    "# Get the lists of expected labels\n",
    "exp_labels = list(pred_perso_ann[\"tag_{}_expected\".format(category)])\n",
    "exp_labels = [label if label == \"O\" else label[2:] for label in exp_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_expected\".format(category), exp_labels)\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_ann = utils.implodeDataFrame(pred_perso_ann, [\"sentence_id\", \"ann_id\"])\n",
    "pred_perso_ann = pred_perso_ann.reset_index()\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreements and disagreements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_types_perso, agmt_labels_perso = utils1.getAnnotationAgreement(pred_perso_ann, \"label_pers_o_predicted\", \"label_pers_o_expected\")\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"annotation_agreement\", agmt_types_perso)\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"agreement_label\", agmt_labels_perso)\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>false negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>false positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>10080</td>\n",
       "      <td>15546</td>\n",
       "      <td>5003</td>\n",
       "      <td>0.756533</td>\n",
       "      <td>0.606649</td>\n",
       "      <td>0.673351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person Name</td>\n",
       "      <td>8551</td>\n",
       "      <td>13603</td>\n",
       "      <td>4324</td>\n",
       "      <td>0.758800</td>\n",
       "      <td>0.614020</td>\n",
       "      <td>0.678775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>3749</td>\n",
       "      <td>8755</td>\n",
       "      <td>2649</td>\n",
       "      <td>0.767713</td>\n",
       "      <td>0.700176</td>\n",
       "      <td>0.732391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>817</td>\n",
       "      <td>1636</td>\n",
       "      <td>593</td>\n",
       "      <td>0.733961</td>\n",
       "      <td>0.666938</td>\n",
       "      <td>0.698847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>3985</td>\n",
       "      <td>3212</td>\n",
       "      <td>1082</td>\n",
       "      <td>0.748020</td>\n",
       "      <td>0.446297</td>\n",
       "      <td>0.559046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>1529</td>\n",
       "      <td>1943</td>\n",
       "      <td>679</td>\n",
       "      <td>0.741037</td>\n",
       "      <td>0.559620</td>\n",
       "      <td>0.637676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels  false negative  true positive  false positive  precision  \\\n",
       "0          all           10080          15546            5003   0.756533   \n",
       "0  Person Name            8551          13603            4324   0.758800   \n",
       "0      Unknown            3749           8755            2649   0.767713   \n",
       "0     Feminine             817           1636             593   0.733961   \n",
       "0    Masculine            3985           3212            1082   0.748020   \n",
       "0   Occupation            1529           1943             679   0.741037   \n",
       "\n",
       "     recall       f_1  \n",
       "0  0.606649  0.673351  \n",
       "0  0.614020  0.678775  \n",
       "0  0.700176  0.732391  \n",
       "0  0.666938  0.698847  \n",
       "0  0.446297  0.559046  \n",
       "0  0.559620  0.637676  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_perso_all = utils1.getAnnotationAgreementMetrics(pred_perso_ann, \"all\")\n",
    "metrics_perso_pn = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[~(pred_perso_ann.agreement_label.isin([\"Occupation\",\"O\"]))], \"Person Name\")\n",
    "metrics_perso_unk = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Unknown\"], \"Unknown\")\n",
    "metrics_perso_fem = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Feminine\"], \"Feminine\")\n",
    "metrics_perso_mas = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Masculine\"], \"Masculine\")\n",
    "metrics_perso_occ = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Occupation\"], \"Occupation\")\n",
    "metrics_perso = pd.concat([metrics_perso_all, metrics_perso_pn, metrics_perso_unk, metrics_perso_fem, metrics_perso_mas, metrics_perso_occ])\n",
    "metrics_perso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_perso.to_csv(\n",
    "    config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_annot_agmt.csv\".format(a=a, d=d, c=category)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: Loose, Each Label\n",
    "\n",
    "Generalize the tokens' BIO tags to the labels and calculate agreement scores for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels = pred_perso.drop(columns=[\"_merge\"])\n",
    "tag_exp = list(pred_perso_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_perso_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_pers_labels.loc[pred_pers_labels.label_personname_predicted == \"Feminine\"].head()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the agreement metrics at the label level for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>6017</td>\n",
       "      <td>5487</td>\n",
       "      <td>13030</td>\n",
       "      <td>0.703678</td>\n",
       "      <td>0.684097</td>\n",
       "      <td>0.693749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>422</td>\n",
       "      <td>874</td>\n",
       "      <td>2356</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.848092</td>\n",
       "      <td>0.784288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2159</td>\n",
       "      <td>1942</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.657677</td>\n",
       "      <td>0.633447</td>\n",
       "      <td>0.645334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>2200</td>\n",
       "      <td>1381</td>\n",
       "      <td>3612</td>\n",
       "      <td>0.723413</td>\n",
       "      <td>0.621473</td>\n",
       "      <td>0.668579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown            6017            5487          13030   0.703678   \n",
       "0    Feminine             422             874           2356   0.729412   \n",
       "0   Masculine            2159            1942           3731   0.657677   \n",
       "0  Occupation            2200            1381           3612   0.723413   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.684097  0.693749  \n",
       "0  0.848092  0.784288  \n",
       "0  0.633447  0.645334  \n",
       "0  0.621473  0.668579  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['Unknown', 'Feminine', 'Masculine', 'Occupation']\n",
    "pred_perso_labels = utils.isPredictedInExpected(pred_perso_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine and save the performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_stats.to_csv(\n",
    "    config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_loose_agmt.csv\".format(a=a, d=d, c=category)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Past error analysis on Person Name document classifiers would suggest that there's a mix-up in the manual annotations between 'Masculine' and 'Unknown,' with many Masculine-labeled names actually needing an 'Unknown' label based on the annotation descriptions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

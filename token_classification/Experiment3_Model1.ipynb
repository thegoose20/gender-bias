{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3, Model 1\n",
    "\n",
    "#### Model Setup\n",
    "\n",
    "Run models in the following order, using their output labels as features for the next model:\n",
    "\n",
    "1. Multiclass Person Name + Occupation Sequence Classifier\n",
    "\n",
    "2. Multilabel Stereotype + Omission Document Classifier\n",
    "\n",
    "***\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/experiment_input/`\n",
    "    * Prediction Data: Data: under directory `../data/token_clf_data/model_output/experiment3/`\n",
    "* Word Embeddings\n",
    "    * Custom fastText (word2vec with subwords) embeddings of 100 dimensions trained on the CRC Archives catalog's descriptive metadata (harvested October 2020)\n",
    "\n",
    "***\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I.](#i) Person Name + Occupation Sequence Classifier\n",
    "* [Preprocessing](#prep)\n",
    "* [Training & Prediction](#tp)\n",
    "* [Evaluation](#eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, utils1, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For multilabel token classification\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "# For multiclass sequence classification\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define resources for the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.experiment_input_path).mkdir(parents=True, exist_ok=True)    # For train, devtest, and blind test data\n",
    "# Path(config.experiment1_output_path).mkdir(parents=True, exist_ok=True)  # For predictions\n",
    "# Path(config.experiment1_agmt_path).mkdir(parents=True, exist_ok=True)    # For agreement metrics\n",
    "\n",
    "predictions_dir = config.experiment3_path+\"5fold/output/\"     # For predictions\n",
    "Path(predictions_dir).mkdir(parents=True, exist_ok=True)\n",
    "agreement_dir = config.experiment3_path+\"5fold/agreement/\"    # For agreement metrics\n",
    "Path(agreement_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2:\n",
    "pers_o_label_subset = [\"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \"B-Masculine\", \"I-Masculine\", \"B-Occupation\", \"I-Occupation\"]\n",
    "# Model 2.1:\n",
    "# pers_label_subset = [\"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \"B-Masculine\", \"I-Masculine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_o_label_tags = {\n",
    "    \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "     \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100  # dimensions of word embeddings (should match utils1.py)\n",
    "target_labels = \"pers_o\"  # for file names\n",
    "# ---------------------\n",
    "### Model 2.2: binary classification with Person-Name vs. O\n",
    "# target_labels = \"pers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Person Name + Occupation Labels\n",
    "\n",
    "Train a multiclass sequence classifier, using Conditional Random Field with Adaptive Regularization of Weight Vectors (AROW), on the Person Name and Occupation labels.\n",
    "\n",
    "Multiclass is a suitable setup for these labels because they are mutually exclusive (no one token should have more than one of these labels).  The sequence classifier with AROW was the highest performing for past algorithm experiments with sequence classifiers for Person Name and Occupation labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The devtest data subset from the model in step 1 will be the train data subset in this step, with the predicted Linguistic labels as features passed into this second model.  The train data subset from the first model will be the devtest data subset for this second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>split2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>split2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id       token token_offsets  \\\n",
       "0               0            0   99999         0  Identifier       (0, 10)   \n",
       "1               0            0   99999         1           :      (10, 11)   \n",
       "2               0            0   99999         2         AA5      (12, 15)   \n",
       "3               1            1   99999         3       Title      (17, 22)   \n",
       "4               1            1   99999         4           :      (22, 23)   \n",
       "\n",
       "  pos tag       field    fold  \n",
       "0  NN   O  Identifier  split4  \n",
       "1   :   O  Identifier  split4  \n",
       "2  NN   O  Identifier  split4  \n",
       "3  NN   O       Title  split2  \n",
       "4   :   O       Title  split2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### For 40-40-20 data split\n",
    "# train_df = pd.read_csv(config.tokc_path+\"experiment_input/token_validate.csv\", index_col=0)\n",
    "# dev_df =  pd.read_csv(config.tokc_path+\"experiment_input/token_train.csv\", index_col=0)\n",
    "# perso_train, perso_dev = utils.selectDataForLabels(train_df, dev_df, \"tag\", pers_o_label_subset)\n",
    "# print(perso_train.shape, perso_dev.shape)\n",
    "# ------------------------\n",
    "### For this experiment, we'll repeatedly train models on different 80% selections of \n",
    "### data and predict on the remaining 20% split, for a modified 5-fold cross-validation approach.\n",
    "perso_df = pd.read_csv(config.tokc_path+\"experiment_input/token_5fold.csv\", index_col=0)\n",
    "# Make sure only Person Name and Occupation tags are considered\n",
    "perso_df = utils1.selectDataForLabels(perso_df, \"tag\", pers_o_label_subset) #pers_label_subset)\n",
    "perso_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'B-Unknown' 'B-Masculine' 'I-Unknown' 'I-Masculine' 'B-Occupation'\n",
      " 'I-Occupation' 'B-Feminine' 'I-Feminine']\n"
     ]
    }
   ],
   "source": [
    "print(perso_df.tag.unique())  # Looks good\n",
    "# -----------------------------\n",
    "# # Model 2.2: for every Person Name BIO tag, replace it with the category name (simply, Person Name)\n",
    "# tags = list(perso_df[\"tag\"])\n",
    "# new_tags = [tag if tag == \"O\" else \"Person Name\" for tag in tags]\n",
    "# perso_df[\"tag\"] = new_tags\n",
    "# print(perso_df.tag.unique())  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the label associated with each annotation for future evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ann_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[58341, 58342, 58343, 58344]</td>\n",
       "      <td>[B-Feminine, I-Feminine, I-Feminine, I-Feminine]</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[19836, 19837, 19838, 19839]</td>\n",
       "      <td>[B-Unknown, I-Unknown, I-Unknown, I-Unknown]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[28713, 28714]</td>\n",
       "      <td>[B-Unknown, I-Unknown]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[28738, 28739, 28740, 28741]</td>\n",
       "      <td>[B-Unknown, I-Unknown, I-Unknown, I-Unknown]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[28790, 28791, 28792]</td>\n",
       "      <td>[B-Unknown, I-Unknown, I-Unknown]</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            token_id  \\\n",
       "ann_id                                 \n",
       "7       [58341, 58342, 58343, 58344]   \n",
       "14      [19836, 19837, 19838, 19839]   \n",
       "15                    [28713, 28714]   \n",
       "16      [28738, 28739, 28740, 28741]   \n",
       "17             [28790, 28791, 28792]   \n",
       "\n",
       "                                                     tag expected_label  \n",
       "ann_id                                                                   \n",
       "7       [B-Feminine, I-Feminine, I-Feminine, I-Feminine]       Feminine  \n",
       "14          [B-Unknown, I-Unknown, I-Unknown, I-Unknown]        Unknown  \n",
       "15                                [B-Unknown, I-Unknown]        Unknown  \n",
       "16          [B-Unknown, I-Unknown, I-Unknown, I-Unknown]        Unknown  \n",
       "17                     [B-Unknown, I-Unknown, I-Unknown]        Unknown  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_ann = pd.read_csv(config.tokc_path+\"experiment_input/token_5fold.csv\", usecols=[\"ann_id\", \"token_id\", \"tag\"])\n",
    "df_by_ann = df_by_ann.drop_duplicates()\n",
    "df_by_ann = utils.implodeDataFrame(df_by_ann, [\"ann_id\"])\n",
    "tags_col = list(df_by_ann.tag)\n",
    "labels = [[tag[2:] if tag != \"O\" else tag for tag in tags] for tags in tags_col]\n",
    "labels = [label_list[0] for label_list in labels]\n",
    "df_by_ann.insert(len(df_by_ann.columns), \"expected_label\", labels)\n",
    "perso_labels = list(pers_o_label_tags.keys())\n",
    "df_by_ann = df_by_ann.loc[df_by_ann.expected_label.isin(perso_labels)]\n",
    "df_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the five groups of training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = \"fold\"\n",
    "splits = perso_df[split_col].unique()\n",
    "splits.sort()\n",
    "train0, test0 = list(splits[:4]), splits[4]\n",
    "train1, test1 = list(splits[1:]), splits[0]\n",
    "train2, test2 = list(splits[2:])+[splits[0]], splits[1]\n",
    "train3, test3 = list(splits[3:])+list(splits[:2]), splits[2]\n",
    "train4, test4 = [splits[4]]+list(splits[:3]), splits[3]\n",
    "runs = [(train0, test0), (train1, test1), (train2, test2), (train3, test3), (train4, test4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prep\"></a>\n",
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = perso_train.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# dev_df = perso_dev.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# ------------------------\n",
    "df = perso_df.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_train_token_groups = utils.implodeDataFrame(train_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_train_token_groups = df_train_token_groups.reset_index()\n",
    "# # df_train_token_groups.head()\n",
    "# df_dev_token_groups = utils.implodeDataFrame(dev_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_dev_token_groups = df_dev_token_groups.reset_index()\n",
    "# # df_dev_token_groups.head()\n",
    "# df_train_grouped = utils.implodeDataFrame(df_train_token_groups, ['sentence_id'])\n",
    "# df_dev_grouped = utils.implodeDataFrame(df_dev_token_groups, ['sentence_id'])\n",
    "# df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_train_grouped.head()\n",
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_groups = utils.implodeDataFrame(df, ['token_id', 'sentence_id', 'token', 'fold'])\n",
    "df_token_groups = df_token_groups.reset_index()\n",
    "# df_token_groups.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that every row's tags are not duplicated, and that if a row has a `B-` or `I-` tag (or category name), it doesn't also have an `O` tag, and sort the order of the tags so that any `B` tag will be selected as the expected tag for training before an \"I\" tag.  Additionally, if multiple labels are present and one is `Unknown`, put the `Unknown` tag first so that it will be selected as the expected tag for training (as the data sample output above illustrates, and as Error Analysis of document classifiers for Person Names showed, people's names should have been annotated as `Unknown` more than they actually were)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(df_token_groups[\"tag\"])\n",
    "new_tags = []\n",
    "for tag_list in tags:\n",
    "    unique_tags = list(set(tag_list))\n",
    "    if (len(unique_tags) > 1) and (\"O\" in unique_tags):\n",
    "        unique_tags.remove(\"O\")\n",
    "    unique_tags.sort()\n",
    "    # Put any Unknown tags at the start of the list, so they'll be selected\n",
    "    # as a feature for training over Masculine or Feminine tags\n",
    "    if len(unique_tags) > 1:\n",
    "        if \"I-Unknown\" in unique_tags:\n",
    "            unique_tags.remove(\"I-Unknown\")\n",
    "            unique_tags = [\"I-Unknown\"] + unique_tags\n",
    "        if \"B-Unknown\" in unique_tags:\n",
    "            unique_tags.remove(\"B-Unknown\")\n",
    "            unique_tags = [\"B-Unknown\"] + unique_tags\n",
    "    new_tags += [unique_tags]\n",
    "df_token_groups[\"tag\"] = new_tags\n",
    "# df_token_groups.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>split4</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Identifier, :, AA5]</td>\n",
       "      <td>[[O], [O], [O]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>split2</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n",
       "      <td>[Title, :, Papers, of, The, Very, Rev, Prof, J...</td>\n",
       "      <td>[[O], [O], [O], [O], [B-Unknown, B-Masculine],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>split1</td>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>split2</td>\n",
       "      <td>[109, 110, 111, 112, 113, 114, 115, 116, 117, ...</td>\n",
       "      <td>[Biographical, /, Historical, :, Professor, Ja...</td>\n",
       "      <td>[[O], [O], [O], [O], [B-Masculine], [I-Masculi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>split4</td>\n",
       "      <td>[134, 135, 136, 137, 138, 139, 140, 141, 142, ...</td>\n",
       "      <td>[He, was, educated, at, Daniel, Stewart, 's, C...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold                                           token_id  \\\n",
       "0            0  split4                                          [0, 1, 2]   \n",
       "1            1  split2      [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]   \n",
       "2            2  split1  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...   \n",
       "3            3  split2  [109, 110, 111, 112, 113, 114, 115, 116, 117, ...   \n",
       "4            4  split4  [134, 135, 136, 137, 138, 139, 140, 141, 142, ...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0                               [Identifier, :, AA5]   \n",
       "1  [Title, :, Papers, of, The, Very, Rev, Prof, J...   \n",
       "2  [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "3  [Biographical, /, Historical, :, Professor, Ja...   \n",
       "4  [He, was, educated, at, Daniel, Stewart, 's, C...   \n",
       "\n",
       "                                                 tag  \n",
       "0                                    [[O], [O], [O]]  \n",
       "1  [[O], [O], [O], [O], [B-Unknown, B-Masculine],...  \n",
       "2  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  \n",
       "3  [[O], [O], [O], [O], [B-Masculine], [I-Masculi...  \n",
       "4  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped = utils.implodeDataFrame(df_token_groups, ['sentence_id', 'fold'])\n",
    "df_grouped = df_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_grouped = df_grouped.reset_index()\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Zip the linguistic label and BIO tags together with the tokens so each sentence item is a tuple: `(TOKEN, TAG_LIST)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_train_grouped = df_train_grouped.reset_index()\n",
    "# df_dev_grouped = df_dev_grouped.reset_index()\n",
    "# train_sentences_pers = utils1.zip1Feature1AndTarget(df_train_grouped, \"tag\")  # Dev because not using additional feature col for linguistic labels\n",
    "# print(train_sentences_pers[2][:3])\n",
    "# dev_sentences_pers = utils1.zip1FeatureAndTarget(df_dev_grouped, \"tag\")\n",
    "# print(dev_sentences_pers[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_sentences = train_sentences_pers\n",
    "# dev_sentences = dev_sentences_pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Features\n",
    "# X_train = [utils1.extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "# X_dev = [utils1.extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# # Target\n",
    "# y_train = [utils1.extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "# y_dev = [utils1.extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tp\"></a>\n",
    "#### Training & Prediction\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Person Name** category of tags.  We'll increase the max iterations to 100 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"arow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# clf_pers = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "# # https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "# try:\n",
    "#     clf_pers.fit(X_train, y_train)\n",
    "# except AttributeError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "\n",
    "# Specify the run one at a time (with for loop, kernel dies; also, \n",
    "# crf_suite for sklearn's models will keep learning from previous runs if not restarted)\n",
    "run = runs[4]  # 0, 1, 2, 3\n",
    "\n",
    "# Get the train (80%) and test (20%) subsets of data\n",
    "train_splits, test_split = run[0], run[1]\n",
    "print(\"Training on:\", train_splits)\n",
    "train_df = df_grouped.loc[df_grouped[split_col].isin(train_splits)]\n",
    "dev_df = df_grouped.loc[df_grouped[split_col] == test_split]\n",
    "\n",
    "# Zip feature and target columns together so each \n",
    "# sentence item is a tuple: `(TOKEN, TAG_LIST)`\n",
    "train_sentences = utils1.zip1FeatureAndTarget(train_df, \"tag\")\n",
    "dev_sentences = utils1.zip1FeatureAndTarget(dev_df, \"tag\")\n",
    "# Extract features\n",
    "X_train = [utils1.extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [utils1.extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# Extract targets\n",
    "y_train = [utils1.extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [utils1.extractSentenceTargets(sentence) for sentence in dev_sentences]\n",
    "\n",
    "# Train a classification model\n",
    "clf_pers = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf_pers.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict with the trained model\n",
    "print(\"Predicting on:\", test_split)\n",
    "predictions = clf_pers.predict(X_dev)\n",
    "dev_df = dev_df.rename(columns={\"tag\":\"tag_{}_expected\".format(target_labels)})\n",
    "dev_df.insert(len(dev_df.columns), \"tag_{}_predicted\".format(target_labels), predictions)\n",
    "dev_df = dev_df.set_index([\"sentence_id\", \"fold\"])\n",
    "dev_df_exploded = dev_df.explode(list(dev_df.columns))\n",
    "\n",
    "if pred_df.shape[0] > 0:\n",
    "    pred_df = pd.concat([pred_df, dev_df_exploded])\n",
    "else:\n",
    "    pred_df = dev_df_exploded\n",
    "\n",
    "assert pred_df.loc[pred_df[\"tag_{}_predicted\".format(target_labels)].isna()].shape[0] == 0, \"Any NaN values should be replaced with 'O'\"\n",
    "\n",
    "filename = \"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_{s}.csv\".format(a=a, t=target_labels, d=d, s=test_split)\n",
    "pred_df.to_csv(predictions_dir+filename)\n",
    "\n",
    "print(\"Predictions for {} saved!\".format(test_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(753521, 5)\n"
     ]
    }
   ],
   "source": [
    "pred_df0 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split0.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df1 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split1.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df2 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split2.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df3 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split3.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df4 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split4.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_perso = pd.concat([pred_df0, pred_df1, pred_df2, pred_df3, pred_df4])\n",
    "print(pred_perso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[B-Masculine]</td>\n",
       "      <td>B-Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[I-Masculine]</td>\n",
       "      <td>I-Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id sentence tag_pers_o_expected  \\\n",
       "0            8  split0       233    James       [B-Masculine]   \n",
       "1            8  split0       234    Whyte       [I-Masculine]   \n",
       "2            8  split0       235      was                 [O]   \n",
       "3            8  split0       236   called                 [O]   \n",
       "4            8  split0       237     upon                 [O]   \n",
       "\n",
       "  tag_pers_o_predicted  \n",
       "0            B-Unknown  \n",
       "1            I-Unknown  \n",
       "2                    O  \n",
       "3                    O  \n",
       "4                    O  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso = pred_perso.reset_index()\n",
    "pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_{}_expected\".format(target_labels))\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Unknown', 'I-Unknown', 'B-Masculine', 'I-Masculine', 'B-Occupation', 'I-Occupation', 'B-Feminine', 'I-Feminine']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf_pers.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# y_pred = clf_pers.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.42825014312210974\n",
      "  - Prec: 0.4496106745338277\n",
      "  - Rec 0.41771448419590135\n"
     ]
    }
   ],
   "source": [
    "# print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "# df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_pers_o_predicted\", y_pred)\n",
    "# # df_dev_grouped.head()\n",
    "# df_dev_grouped = df_dev_grouped.set_index(\"sentence_id\")\n",
    "# df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns))\n",
    "# df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# filename = \"crf_{a}_pers_o_baseline_fastText{d}_nolingfeatures_predictions.csv\".format(a=a, d=d)\n",
    "# df_dev_exploded.to_csv(config.experiment1_output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval\"></a>\n",
    "### Evaluation\n",
    "#### Evaluate: Strict, Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# a = \"arow\"\n",
    "# category = \"pers_o\"\n",
    "# filename = \"crf_{a}_{c}_baseline_fastText{d}_nolingfeatures_predictions.csv\".format(a=a, c=category, d=d)\n",
    "# pred_perso = pd.read_csv(config.experiment1_output_path+filename)\n",
    "# pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_{}_expected\".format(category))\n",
    "# # pred_pers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics for each category of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[B-Masculine]</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[I-Masculine]</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id sentence tag_pers_o_expected  \\\n",
       "0            8  split0       233    James       [B-Masculine]   \n",
       "1            8  split0       234    Whyte       [I-Masculine]   \n",
       "2            8  split0       235      was                 [O]   \n",
       "3            8  split0       236   called                 [O]   \n",
       "4            8  split0       237     upon                 [O]   \n",
       "\n",
       "  tag_pers_o_predicted          _merge  \n",
       "0            B-Unknown  false positive  \n",
       "1            I-Unknown  false positive  \n",
       "2                    O   true negative  \n",
       "3                    O   true negative  \n",
       "4                    O   true negative  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso = utils.isPredictedInExpected(pred_perso, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the combined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_evaluation.csv\".format(a=a, t=target_labels, d=d)\n",
    "pred_perso.to_csv(predictions_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>2485</td>\n",
       "      <td>3361</td>\n",
       "      <td>5446</td>\n",
       "      <td>0.618372</td>\n",
       "      <td>0.686673</td>\n",
       "      <td>0.650735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>4215</td>\n",
       "      <td>4940</td>\n",
       "      <td>8649</td>\n",
       "      <td>0.636471</td>\n",
       "      <td>0.672341</td>\n",
       "      <td>0.653914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>192</td>\n",
       "      <td>330</td>\n",
       "      <td>664</td>\n",
       "      <td>0.668008</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.717838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>466</td>\n",
       "      <td>646</td>\n",
       "      <td>1485</td>\n",
       "      <td>0.696856</td>\n",
       "      <td>0.761148</td>\n",
       "      <td>0.727585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>892</td>\n",
       "      <td>1181</td>\n",
       "      <td>1648</td>\n",
       "      <td>0.582538</td>\n",
       "      <td>0.648819</td>\n",
       "      <td>0.613895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>1400</td>\n",
       "      <td>2000</td>\n",
       "      <td>2076</td>\n",
       "      <td>0.509323</td>\n",
       "      <td>0.597238</td>\n",
       "      <td>0.549788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>1274</td>\n",
       "      <td>836</td>\n",
       "      <td>1562</td>\n",
       "      <td>0.651376</td>\n",
       "      <td>0.550776</td>\n",
       "      <td>0.596867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>1843</td>\n",
       "      <td>942</td>\n",
       "      <td>1456</td>\n",
       "      <td>0.607173</td>\n",
       "      <td>0.441346</td>\n",
       "      <td>0.511146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     B-Unknown            2485            3361           5446   0.618372   \n",
       "0     I-Unknown            4215            4940           8649   0.636471   \n",
       "0    B-Feminine             192             330            664   0.668008   \n",
       "0    I-Feminine             466             646           1485   0.696856   \n",
       "0   B-Masculine             892            1181           1648   0.582538   \n",
       "0   I-Masculine            1400            2000           2076   0.509323   \n",
       "0  B-Occupation            1274             836           1562   0.651376   \n",
       "0  I-Occupation            1843             942           1456   0.607173   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.686673  0.650735  \n",
       "0  0.672341  0.653914  \n",
       "0  0.775701  0.717838  \n",
       "0  0.761148  0.727585  \n",
       "0  0.648819  0.613895  \n",
       "0  0.597238  0.549788  \n",
       "0  0.550776  0.596867  \n",
       "0  0.441346  0.511146  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso, \"_merge\", pers_o_label_subset[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(pers_o_label_subset)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso, \"_merge\", pers_o_label_subset[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "# ----------------------\n",
    "# Model 2.2:\n",
    "# pred_perso_stats = utils.getScoresByCatTags(\n",
    "#     pred_perso, \"_merge\", \"Person Name\", \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "# )\n",
    "# ----------------------\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_strict_agmt.csv\".format(a=a, c=category, d=d)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_strict_agmt.csv\".format(a=a, c=category, d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation Agreement\n",
    "\n",
    "Calculate agreement at the annotation level, so if the model labels any word correctly from a manually annotated text span, that annotation is recorded as being correctly labeled (`true positive`).  Note whether the models' labels are an `exact_match`, `label_match`, `category_match` or `mismatch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: `ann_id` of `9999` indicates no annotation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the annotation data by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>expected_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>58341</td>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>58342</td>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>58343</td>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>58344</td>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>19836</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id token_id expected_tag expected_label\n",
       "0       7    58341   B-Feminine       Feminine\n",
       "1       7    58342   I-Feminine       Feminine\n",
       "2       7    58343   I-Feminine       Feminine\n",
       "3       7    58344   I-Feminine       Feminine\n",
       "4      14    19836    B-Unknown        Unknown"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_ann = df_by_ann.explode([\"token_id\", \"tag\"])\n",
    "df_by_ann = df_by_ann.rename(columns={\"tag\": \"expected_tag\"})\n",
    "df_by_ann = df_by_ann.reset_index()\n",
    "df_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the columns of the dev and prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename `sentence` column `token`\n",
    "pred_perso = pred_perso.rename(columns={\"sentence\":\"token\"})\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data, adding the annotation IDs (`ann_id` column) to the prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [\"token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso.join(df_by_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category)])  # duplicate of expected_tag\n",
    "pred_perso_ann = pred_perso_ann.rename(columns={\"expected_tag\":\"tag_{}_expected\".format(category)})\n",
    "pred_perso_ann[\"ann_id\"] = pred_perso_ann[\"ann_id\"].fillna(99999)\n",
    "pred_perso_ann[\"tag_pers_o_expected\"] = pred_perso_ann[\"tag_pers_o_expected\"].fillna(\"O\")\n",
    "pred_perso_ann[\"expected_label\"] = pred_perso_ann[\"expected_label\"].fillna(\"O\")\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_expected\"].isna()].shape[0] == 0\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.explode([\"tag_pers_o_expected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the predicted BIO tags to label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "pred_labels = list(pred_perso_ann[\"tag_{}_predicted\".format(category)])\n",
    "pred_labels = [label if label == \"O\" else label[2:] for label in pred_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_predicted\".format(category), pred_labels)\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>expected_label</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>Feminine</td>\n",
       "      <td>[2590, 2590, 2590, 2590]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>[58341, 58342, 58343, 58344]</td>\n",
       "      <td>[Mrs, Norman, Macleod, ,]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Feminine, Feminine, Feminine, Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[1097, 1097, 1097, 1097]</td>\n",
       "      <td>[split4, split4, split4, split4]</td>\n",
       "      <td>[19836, 19837, 19838, 19839]</td>\n",
       "      <td>[Dr., Nelly, Renee, Deme]</td>\n",
       "      <td>[false positive, false positive, false positiv...</td>\n",
       "      <td>[Feminine, Feminine, Feminine, Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[1485, 1485]</td>\n",
       "      <td>[split4, split4]</td>\n",
       "      <td>[28713, 28714]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser]</td>\n",
       "      <td>[true positive, true positive]</td>\n",
       "      <td>[Feminine, Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[1486, 1486, 1486, 1486]</td>\n",
       "      <td>[split3, split3, split3, split3]</td>\n",
       "      <td>[28738, 28739, 28740, 28741]</td>\n",
       "      <td>[Marjory, Kennedy, Fraser, ,]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Feminine, Feminine, Feminine, Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[1487, 1487, 1487]</td>\n",
       "      <td>[split0, split0, split0]</td>\n",
       "      <td>[28790, 28791, 28792]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser, ,]</td>\n",
       "      <td>[true positive, true positive, true positive]</td>\n",
       "      <td>[Feminine, Feminine, Feminine]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id expected_label               sentence_id  \\\n",
       "0     7.0       Feminine  [2590, 2590, 2590, 2590]   \n",
       "1    14.0        Unknown  [1097, 1097, 1097, 1097]   \n",
       "2    15.0        Unknown              [1485, 1485]   \n",
       "3    16.0        Unknown  [1486, 1486, 1486, 1486]   \n",
       "4    17.0        Unknown        [1487, 1487, 1487]   \n",
       "\n",
       "                               fold                      token_id  \\\n",
       "0  [split2, split2, split2, split2]  [58341, 58342, 58343, 58344]   \n",
       "1  [split4, split4, split4, split4]  [19836, 19837, 19838, 19839]   \n",
       "2                  [split4, split4]                [28713, 28714]   \n",
       "3  [split3, split3, split3, split3]  [28738, 28739, 28740, 28741]   \n",
       "4          [split0, split0, split0]         [28790, 28791, 28792]   \n",
       "\n",
       "                        sentence  \\\n",
       "0      [Mrs, Norman, Macleod, ,]   \n",
       "1      [Dr., Nelly, Renee, Deme]   \n",
       "2      [Marjory, Kennedy-Fraser]   \n",
       "3  [Marjory, Kennedy, Fraser, ,]   \n",
       "4   [Marjory, Kennedy-Fraser, ,]   \n",
       "\n",
       "                                              _merge  \\\n",
       "0  [true positive, true positive, true positive, ...   \n",
       "1  [false positive, false positive, false positiv...   \n",
       "2                     [true positive, true positive]   \n",
       "3  [true positive, true positive, true positive, ...   \n",
       "4      [true positive, true positive, true positive]   \n",
       "\n",
       "                     label_pers_o_predicted  \n",
       "0  [Feminine, Feminine, Feminine, Feminine]  \n",
       "1  [Feminine, Feminine, Feminine, Feminine]  \n",
       "2                      [Feminine, Feminine]  \n",
       "3  [Feminine, Feminine, Feminine, Feminine]  \n",
       "4            [Feminine, Feminine, Feminine]  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_ann = utils.implodeDataFrame(pred_perso_ann, [\"ann_id\", \"expected_label\"])\n",
    "pred_perso_ann = pred_perso_ann.reset_index()\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out the expected unannotated data (`ann_id` of `99999`) from the expected annotated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712167, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>238</td>\n",
       "      <td>to</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>239</td>\n",
       "      <td>preach</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ann_id label_pers_o_expected sentence_id    fold token_id   token  \\\n",
       "20710  99999.0                     O           8  split0      235     was   \n",
       "20710  99999.0                     O           8  split0      236  called   \n",
       "20710  99999.0                     O           8  split0      237    upon   \n",
       "20710  99999.0                     O           8  split0      238      to   \n",
       "20710  99999.0                     O           8  split0      239  preach   \n",
       "\n",
       "              _merge label_pers_o_predicted  \n",
       "20710  true negative                      O  \n",
       "20710  true negative                      O  \n",
       "20710  true negative                      O  \n",
       "20710  true negative                      O  \n",
       "20710  true negative                      O  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann = pred_perso_ann.rename(columns={\"sentence\":\"token\", \"expected_label\":\"label_{}_expected\".format(category)})\n",
    "fp_df = pred_perso_ann.loc[pred_perso_ann.ann_id == 99999]\n",
    "fp_df = fp_df.explode([\"token_id\", \"sentence_id\", \"fold\", \"token\", \"_merge\", \"label_{}_predicted\".format(category)])\n",
    "print(fp_df.shape)\n",
    "fp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.loc[pred_perso_ann.ann_id != 99999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann = pd.concat([pred_perso_ann, fp_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get unique lists of predicted labels per row, removing `O` values from lists with Linguistic labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueLabels(label_list):\n",
    "    final_labels = []\n",
    "    for labels in label_list:\n",
    "        if (len(labels) > 1) and (\"O\" in labels):\n",
    "            labels.remove(\"O\")\n",
    "        final_labels += [labels]\n",
    "    assert len(final_labels) == len(label_list), \"There should be the same number of sub-lists in final_labels and label_list.\"\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>[2590, 2590, 2590, 2590]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>[58341, 58342, 58343, 58344]</td>\n",
       "      <td>[Mrs, Norman, Macleod, ,]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Feminine</td>\n",
       "      <td>[Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>[1097, 1097, 1097, 1097]</td>\n",
       "      <td>[split4, split4, split4, split4]</td>\n",
       "      <td>[19836, 19837, 19838, 19839]</td>\n",
       "      <td>[Dr., Nelly, Renee, Deme]</td>\n",
       "      <td>[false positive, false positive, false positiv...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>[1485, 1485]</td>\n",
       "      <td>[split4, split4]</td>\n",
       "      <td>[28713, 28714]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser]</td>\n",
       "      <td>[true positive, true positive]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>[1486, 1486, 1486, 1486]</td>\n",
       "      <td>[split3, split3, split3, split3]</td>\n",
       "      <td>[28738, 28739, 28740, 28741]</td>\n",
       "      <td>[Marjory, Kennedy, Fraser, ,]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>[1487, 1487, 1487]</td>\n",
       "      <td>[split0, split0, split0]</td>\n",
       "      <td>[28790, 28791, 28792]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser, ,]</td>\n",
       "      <td>[true positive, true positive, true positive]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id               sentence_id                              fold  \\\n",
       "0     7.0  [2590, 2590, 2590, 2590]  [split2, split2, split2, split2]   \n",
       "1    14.0  [1097, 1097, 1097, 1097]  [split4, split4, split4, split4]   \n",
       "2    15.0              [1485, 1485]                  [split4, split4]   \n",
       "3    16.0  [1486, 1486, 1486, 1486]  [split3, split3, split3, split3]   \n",
       "4    17.0        [1487, 1487, 1487]          [split0, split0, split0]   \n",
       "\n",
       "                       token_id                          token  \\\n",
       "0  [58341, 58342, 58343, 58344]      [Mrs, Norman, Macleod, ,]   \n",
       "1  [19836, 19837, 19838, 19839]      [Dr., Nelly, Renee, Deme]   \n",
       "2                [28713, 28714]      [Marjory, Kennedy-Fraser]   \n",
       "3  [28738, 28739, 28740, 28741]  [Marjory, Kennedy, Fraser, ,]   \n",
       "4         [28790, 28791, 28792]   [Marjory, Kennedy-Fraser, ,]   \n",
       "\n",
       "                                              _merge label_pers_o_expected  \\\n",
       "0  [true positive, true positive, true positive, ...              Feminine   \n",
       "1  [false positive, false positive, false positiv...               Unknown   \n",
       "2                     [true positive, true positive]               Unknown   \n",
       "3  [true positive, true positive, true positive, ...               Unknown   \n",
       "4      [true positive, true positive, true positive]               Unknown   \n",
       "\n",
       "  label_pers_o_predicted  \n",
       "0             [Feminine]  \n",
       "1             [Feminine]  \n",
       "2             [Feminine]  \n",
       "3             [Feminine]  \n",
       "4             [Feminine]  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = list(eval_by_ann[\"label_{}_predicted\".format(category)])\n",
    "predicted_labels = [list(set(predictions)) for predictions in predicted_labels]\n",
    "predicted_unique = getUniqueLabels(predicted_labels)\n",
    "# predicted_unique[:5]  # Looks good\n",
    "# for pred in predicted_unique:\n",
    "#     if len(pred) > 1:\n",
    "#         print(\"Multi-item lists exist\")\n",
    "#         break\n",
    "eval_by_ann[\"label_{}_predicted\".format(category)] = predicted_unique\n",
    "# Reorder columns:\n",
    "eval_by_ann = eval_by_ann[\n",
    "    [\"ann_id\", \"sentence_id\", \"fold\", \"token_id\", \"token\", \"_merge\", \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category)]\n",
    "]\n",
    "eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreements and disagreements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "      <th>annotation_agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>[2590, 2590, 2590, 2590]</td>\n",
       "      <td>[split2, split2, split2, split2]</td>\n",
       "      <td>[58341, 58342, 58343, 58344]</td>\n",
       "      <td>[Mrs, Norman, Macleod, ,]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Feminine</td>\n",
       "      <td>[Feminine]</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>[1097, 1097, 1097, 1097]</td>\n",
       "      <td>[split4, split4, split4, split4]</td>\n",
       "      <td>[19836, 19837, 19838, 19839]</td>\n",
       "      <td>[Dr., Nelly, Renee, Deme]</td>\n",
       "      <td>[false positive, false positive, false positiv...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>[1485, 1485]</td>\n",
       "      <td>[split4, split4]</td>\n",
       "      <td>[28713, 28714]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser]</td>\n",
       "      <td>[true positive, true positive]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>[1486, 1486, 1486, 1486]</td>\n",
       "      <td>[split3, split3, split3, split3]</td>\n",
       "      <td>[28738, 28739, 28740, 28741]</td>\n",
       "      <td>[Marjory, Kennedy, Fraser, ,]</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>[1487, 1487, 1487]</td>\n",
       "      <td>[split0, split0, split0]</td>\n",
       "      <td>[28790, 28791, 28792]</td>\n",
       "      <td>[Marjory, Kennedy-Fraser, ,]</td>\n",
       "      <td>[true positive, true positive, true positive]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[Feminine]</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id               sentence_id                              fold  \\\n",
       "0     7.0  [2590, 2590, 2590, 2590]  [split2, split2, split2, split2]   \n",
       "1    14.0  [1097, 1097, 1097, 1097]  [split4, split4, split4, split4]   \n",
       "2    15.0              [1485, 1485]                  [split4, split4]   \n",
       "3    16.0  [1486, 1486, 1486, 1486]  [split3, split3, split3, split3]   \n",
       "4    17.0        [1487, 1487, 1487]          [split0, split0, split0]   \n",
       "\n",
       "                       token_id                          token  \\\n",
       "0  [58341, 58342, 58343, 58344]      [Mrs, Norman, Macleod, ,]   \n",
       "1  [19836, 19837, 19838, 19839]      [Dr., Nelly, Renee, Deme]   \n",
       "2                [28713, 28714]      [Marjory, Kennedy-Fraser]   \n",
       "3  [28738, 28739, 28740, 28741]  [Marjory, Kennedy, Fraser, ,]   \n",
       "4         [28790, 28791, 28792]   [Marjory, Kennedy-Fraser, ,]   \n",
       "\n",
       "                                              _merge label_pers_o_expected  \\\n",
       "0  [true positive, true positive, true positive, ...              Feminine   \n",
       "1  [false positive, false positive, false positiv...               Unknown   \n",
       "2                     [true positive, true positive]               Unknown   \n",
       "3  [true positive, true positive, true positive, ...               Unknown   \n",
       "4      [true positive, true positive, true positive]               Unknown   \n",
       "\n",
       "  label_pers_o_predicted annotation_agreement  \n",
       "0             [Feminine]        true positive  \n",
       "1             [Feminine]       false positive  \n",
       "2             [Feminine]       false positive  \n",
       "3             [Feminine]       false positive  \n",
       "4             [Feminine]       false positive  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_labels = list(eval_by_ann[\"label_{}_expected\".format(category)])\n",
    "predicted_labels = list(eval_by_ann[\"label_{}_predicted\".format(category)])\n",
    "ann_agmts = []\n",
    "perso_labels = list(pers_o_label_tags.keys())\n",
    "for i,exp in enumerate(expected_labels):\n",
    "    pred = predicted_labels[i]\n",
    "    if (exp == \"O\"):\n",
    "        # If `O` was predicted as expected\n",
    "        if 'O' in pred:\n",
    "            ann_agmts += [\"true negative\"]\n",
    "        # If a label was predicted when `O` was expected\n",
    "        else:\n",
    "            ann_agmts += [\"false positive\"]\n",
    "    else:\n",
    "        # If the correct label was predicted\n",
    "        if exp in pred:\n",
    "            ann_agmts += [\"true positive\"]\n",
    "        # If there's a label mismatch\n",
    "        elif (exp != \"O\") and (not \"O\" in pred) and (not exp in pred):\n",
    "            ann_agmts += [\"false positive\"]\n",
    "        # If `O` was predicted when there was an expected label\n",
    "        else:\n",
    "            ann_agmts += [\"false negative\"]\n",
    "assert len(ann_agmts) == eval_by_ann.shape[0]\n",
    "\n",
    "# Insert the annotation agreement column recording TP, FP, TN, and FN\n",
    "eval_by_ann.insert(len(eval_by_ann.columns), \"annotation_agreement\", ann_agmts)\n",
    "eval_by_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_ann.to_csv(predictions_dir+\"cc-{a}_{c}_baseline_fastText{d}_annot_evaluation.csv\".format(a=a, c=category, d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate annotation agreement metrics for each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_agmt = pd.DataFrame.from_dict({\n",
    "        \"label\":[], \"false negative\":[], \"false positive\":[],\n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>2345.0</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>6840.0</td>\n",
       "      <td>0.837619</td>\n",
       "      <td>0.744692</td>\n",
       "      <td>0.788427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>161.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>974.0</td>\n",
       "      <td>0.651941</td>\n",
       "      <td>0.858150</td>\n",
       "      <td>0.740966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>780.0</td>\n",
       "      <td>2805.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>0.416355</td>\n",
       "      <td>0.719525</td>\n",
       "      <td>0.527481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1738.0</td>\n",
       "      <td>0.959691</td>\n",
       "      <td>0.602426</td>\n",
       "      <td>0.740204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown          2345.0          1326.0         6840.0   0.837619   \n",
       "0    Feminine           161.0           520.0          974.0   0.651941   \n",
       "0   Masculine           780.0          2805.0         2001.0   0.416355   \n",
       "0  Occupation          1147.0            73.0         1738.0   0.959691   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.744692  0.788427  \n",
       "0  0.858150  0.740966  \n",
       "0  0.719525  0.527481  \n",
       "0  0.602426  0.740204  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in perso_labels:\n",
    "    agmt_df = eval_by_ann.loc[eval_by_ann[\"label_{}_expected\".format(category)] == label]\n",
    "    tp = agmt_df.loc[agmt_df.annotation_agreement == \"true positive\"].shape[0]\n",
    "    fp = agmt_df.loc[agmt_df.annotation_agreement == \"false positive\"].shape[0]\n",
    "    fn = agmt_df.loc[agmt_df.annotation_agreement == \"false negative\"].shape[0]\n",
    "    prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "    label_agmt = pd.DataFrame.from_dict({\n",
    "            \"label\":[label], \"false negative\":[fn], \"false positive\":[fp],\n",
    "             \"true positive\":[tp], \"precision\":[prec], \"recall\":[rec], \"f1\":[f1]\n",
    "        })\n",
    "    annot_agmt = pd.concat([annot_agmt, label_agmt])\n",
    "annot_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics and annotation-level data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_perso.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_annot_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "annot_agmt.to_csv(agreement_dir+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_annot_agmt.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loose Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the manual annotation evaluation, we want to evaluate the predictions more loosely, considering overlapping text spans in addition to exactly matching text spans.\n",
    "\n",
    "#### Token Agreement\n",
    "\n",
    "First, generalize the tokens' IOB tags to the label, and calculate agreement scores for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels = pred_perso.copy()\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"_merge\"])\n",
    "tag_exp = list(pred_perso_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_perso_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_pers_labels.loc[pred_pers_labels.label_personname_predicted == \"Feminine\"].head()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the agreement metrics at the label level for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>6683</td>\n",
       "      <td>7489</td>\n",
       "      <td>14907</td>\n",
       "      <td>0.665610</td>\n",
       "      <td>0.690459</td>\n",
       "      <td>0.677807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>651</td>\n",
       "      <td>856</td>\n",
       "      <td>2269</td>\n",
       "      <td>0.726080</td>\n",
       "      <td>0.777055</td>\n",
       "      <td>0.750703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2276</td>\n",
       "      <td>2986</td>\n",
       "      <td>3919</td>\n",
       "      <td>0.567560</td>\n",
       "      <td>0.632607</td>\n",
       "      <td>0.598321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>3117</td>\n",
       "      <td>1598</td>\n",
       "      <td>3198</td>\n",
       "      <td>0.666806</td>\n",
       "      <td>0.506413</td>\n",
       "      <td>0.575646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown            6683            7489          14907   0.665610   \n",
       "0    Feminine             651             856           2269   0.726080   \n",
       "0   Masculine            2276            2986           3919   0.567560   \n",
       "0  Occupation            3117            1598           3198   0.666806   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.690459  0.677807  \n",
       "0  0.777055  0.750703  \n",
       "0  0.632607  0.598321  \n",
       "0  0.506413  0.575646  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['Unknown', 'Feminine', 'Masculine', 'Occupation']\n",
    "pred_perso_labels = utils.isPredictedInExpected(pred_perso_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine and save the performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_loose_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_loose_agmt.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[Masculine]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[Masculine]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id   token label_pers_o_expected  \\\n",
       "0            8  split0       233   James           [Masculine]   \n",
       "1            8  split0       234   Whyte           [Masculine]   \n",
       "2            8  split0       235     was                   [O]   \n",
       "3            8  split0       236  called                   [O]   \n",
       "4            8  split0       237    upon                   [O]   \n",
       "\n",
       "  label_pers_o_predicted          _merge  \n",
       "0                Unknown  false positive  \n",
       "1                Unknown  false positive  \n",
       "2                      O   true negative  \n",
       "3                      O   true negative  \n",
       "4                      O   true negative  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the loose predictions (with labels instead of BIO tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels.to_csv(predictions_dir+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_loose_evaluation.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

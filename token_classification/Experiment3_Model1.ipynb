{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3, Model 1\n",
    "\n",
    "#### Model Setup\n",
    "\n",
    "Run models in the following order, using their output labels as features for the next model:\n",
    "\n",
    "1. Multiclass Person Name + Occupation Sequence Classifier\n",
    "\n",
    "2. Multilabel Stereotype + Omission Document Classifier\n",
    "\n",
    "***\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/experiment_input/`\n",
    "    * Prediction Data: Data: under directory `../data/token_clf_data/model_output/experiment3/`\n",
    "* Word Embeddings\n",
    "    * Custom fastText (word2vec with subwords) embeddings of 100 dimensions trained on the CRC Archives catalog's descriptive metadata (harvested October 2020)\n",
    "\n",
    "***\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I.](#i) Person Name + Occupation Sequence Classifier\n",
    "* [Preprocessing](#prep)\n",
    "* [Training & Prediction](#tp)\n",
    "* [Evaluation](#eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, utils1, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For multilabel token classification\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "# For multiclass sequence classification\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define resources for the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.experiment_input_path).mkdir(parents=True, exist_ok=True)    # For train, devtest, and blind test data\n",
    "# Path(config.experiment1_output_path).mkdir(parents=True, exist_ok=True)  # For predictions\n",
    "# Path(config.experiment1_agmt_path).mkdir(parents=True, exist_ok=True)    # For agreement metrics\n",
    "\n",
    "predictions_dir = config.experiment3_path+\"5fold/output\"     # For predictions\n",
    "Path(predictions_dir).mkdir(parents=True, exist_ok=True)\n",
    "agreement_dir = config.experiment3_path+\"5fold/agreement\"    # For agreement metrics\n",
    "Path(agreement_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2:\n",
    "pers_o_label_subset = [\"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \"B-Masculine\", \"I-Masculine\", \"B-Occupation\", \"I-Occupation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_o_label_tags = {\n",
    "    \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "     \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100  # dimensions of word embeddings (should match utils1.py)\n",
    "target_labels = \"pers_o\"  # for file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Person Name + Occupation Labels\n",
    "\n",
    "Train a multiclass sequence classifier, using Conditional Random Field with Adaptive Regularization of Weight Vectors (AROW), on the Person Name and Occupation labels.\n",
    "\n",
    "Multiclass is a suitable setup for these labels because they are mutually exclusive (no one token should have more than one of these labels).  The sequence classifier with AROW was the highest performing for past algorithm experiments with sequence classifiers for Person Name and Occupation labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The devtest data subset from the model in step 1 will be the train data subset in this step, with the predicted Linguistic labels as features passed into this second model.  The train data subset from the first model will be the devtest data subset for this second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>(0, 10)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>:</td>\n",
       "      <td>(10, 11)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>2</td>\n",
       "      <td>AA5</td>\n",
       "      <td>(12, 15)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Identifier</td>\n",
       "      <td>split4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>split2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>split2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id       token token_offsets  \\\n",
       "0               0            0   99999         0  Identifier       (0, 10)   \n",
       "1               0            0   99999         1           :      (10, 11)   \n",
       "2               0            0   99999         2         AA5      (12, 15)   \n",
       "3               1            1   99999         3       Title      (17, 22)   \n",
       "4               1            1   99999         4           :      (22, 23)   \n",
       "\n",
       "  pos tag       field    fold  \n",
       "0  NN   O  Identifier  split4  \n",
       "1   :   O  Identifier  split4  \n",
       "2  NN   O  Identifier  split4  \n",
       "3  NN   O       Title  split2  \n",
       "4   :   O       Title  split2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### For 40-40-20 data split\n",
    "# train_df = pd.read_csv(config.tokc_path+\"experiment_input/token_validate.csv\", index_col=0)\n",
    "# dev_df =  pd.read_csv(config.tokc_path+\"experiment_input/token_train.csv\", index_col=0)\n",
    "# perso_train, perso_dev = utils.selectDataForLabels(train_df, dev_df, \"tag\", pers_o_label_subset)\n",
    "# print(perso_train.shape, perso_dev.shape)\n",
    "# ------------------------\n",
    "### For this experiment, we'll repeatedly train models on different 80% selections of \n",
    "### data and predict on the remaining 20% split, for a modified 5-fold cross-validation approach.\n",
    "perso_df = pd.read_csv(config.tokc_path+\"experiment_input/token_5fold.csv\", index_col=0)\n",
    "# Make sure only Person Name and Occupation tags are considered\n",
    "perso_df = utils1.selectDataForLabels(perso_df, \"tag\", pers_o_label_subset)\n",
    "perso_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B-Unknown', 'B-Masculine', 'I-Unknown', 'I-Masculine',\n",
       "       'B-Occupation', 'I-Occupation', 'B-Feminine', 'I-Feminine'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perso_df.tag.unique()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the five groups of training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['split0' 'split1' 'split2' 'split3' 'split4']\n",
      "(['split0', 'split1', 'split2', 'split3'], 'split4')\n",
      "(['split1', 'split2', 'split3', 'split4'], 'split0')\n",
      "(['split2', 'split3', 'split4', 'split0'], 'split1')\n",
      "(['split3', 'split4', 'split0', 'split1'], 'split2')\n",
      "(['split4', 'split0', 'split1', 'split2'], 'split3')\n"
     ]
    }
   ],
   "source": [
    "split_col = \"fold\"\n",
    "splits = perso_df[split_col].unique()\n",
    "splits.sort()\n",
    "print(splits)\n",
    "train0, test0 = list(splits[:4]), splits[4]\n",
    "train1, test1 = list(splits[1:]), splits[0]\n",
    "train2, test2 = list(splits[2:])+[splits[0]], splits[1]\n",
    "train3, test3 = list(splits[3:])+list(splits[:2]), splits[2]\n",
    "train4, test4 = [splits[4]]+list(splits[:3]), splits[3]\n",
    "runs = [(train0, test0), (train1, test1), (train2, test2), (train3, test3), (train4, test4)]\n",
    "for run in runs:\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prep\"></a>\n",
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = perso_train.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# dev_df = perso_dev.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"subset\", \"pos\"])\n",
    "# ------------------------\n",
    "df = perso_df.drop(columns=[\"description_id\", \"ann_id\", \"token_offsets\", \"field\", \"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_train_token_groups = utils.implodeDataFrame(train_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_train_token_groups = df_train_token_groups.reset_index()\n",
    "# # df_train_token_groups.head()\n",
    "# df_dev_token_groups = utils.implodeDataFrame(dev_df, ['token_id', 'sentence_id', 'token'])\n",
    "# df_dev_token_groups = df_dev_token_groups.reset_index()\n",
    "# # df_dev_token_groups.head()\n",
    "# df_train_grouped = utils.implodeDataFrame(df_train_token_groups, ['sentence_id'])\n",
    "# df_dev_grouped = utils.implodeDataFrame(df_dev_token_groups, ['sentence_id'])\n",
    "# df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_train_grouped.head()\n",
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_groups = utils.implodeDataFrame(df, ['token_id', 'sentence_id', 'token', 'fold'])\n",
    "df_token_groups = df_token_groups.reset_index()\n",
    "# # df_token_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>split4</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Identifier, :, AA5]</td>\n",
       "      <td>[[O], [O], [O]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>split2</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n",
       "      <td>[Title, :, Papers, of, The, Very, Rev, Prof, J...</td>\n",
       "      <td>[[O], [O], [O], [O], [O, B-Unknown, B-Masculin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>split1</td>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>split2</td>\n",
       "      <td>[109, 110, 111, 112, 113, 114, 115, 116, 117, ...</td>\n",
       "      <td>[Biographical, /, Historical, :, Professor, Ja...</td>\n",
       "      <td>[[O], [O], [O], [O], [B-Masculine], [I-Masculi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>split4</td>\n",
       "      <td>[134, 135, 136, 137, 138, 139, 140, 141, 142, ...</td>\n",
       "      <td>[He, was, educated, at, Daniel, Stewart, 's, C...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold                                           token_id  \\\n",
       "0            0  split4                                          [0, 1, 2]   \n",
       "1            1  split2      [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]   \n",
       "2            2  split1  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...   \n",
       "3            3  split2  [109, 110, 111, 112, 113, 114, 115, 116, 117, ...   \n",
       "4            4  split4  [134, 135, 136, 137, 138, 139, 140, 141, 142, ...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0                               [Identifier, :, AA5]   \n",
       "1  [Title, :, Papers, of, The, Very, Rev, Prof, J...   \n",
       "2  [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "3  [Biographical, /, Historical, :, Professor, Ja...   \n",
       "4  [He, was, educated, at, Daniel, Stewart, 's, C...   \n",
       "\n",
       "                                                 tag  \n",
       "0                                    [[O], [O], [O]]  \n",
       "1  [[O], [O], [O], [O], [O, B-Unknown, B-Masculin...  \n",
       "2  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  \n",
       "3  [[O], [O], [O], [O], [B-Masculine], [I-Masculi...  \n",
       "4  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped = utils.implodeDataFrame(df_token_groups, ['sentence_id', 'fold'])\n",
    "df_grouped = df_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_grouped = df_grouped.reset_index()\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Zip the linguistic label and BIO tags together with the tokens so each sentence item is a tuple: `(TOKEN, TAG_LIST)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_train_grouped = df_train_grouped.reset_index()\n",
    "# df_dev_grouped = df_dev_grouped.reset_index()\n",
    "# train_sentences_pers = utils1.zip1Feature1AndTarget(df_train_grouped, \"tag\")  # Dev because not using additional feature col for linguistic labels\n",
    "# print(train_sentences_pers[2][:3])\n",
    "# dev_sentences_pers = utils1.zip1FeatureAndTarget(df_dev_grouped, \"tag\")\n",
    "# print(dev_sentences_pers[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_sentences = train_sentences_pers\n",
    "# dev_sentences = dev_sentences_pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Features\n",
    "# X_train = [utils1.extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "# X_dev = [utils1.extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# # Target\n",
    "# y_train = [utils1.extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "# y_dev = [utils1.extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tp\"></a>\n",
    "#### Training & Prediction\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Person Name** category of tags.  We'll increase the max iterations to 100 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"arow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# clf_pers = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "# # https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "# try:\n",
    "#     clf_pers.fit(X_train, y_train)\n",
    "# except AttributeError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: ['split0', 'split1', 'split2', 'split3']\n",
      "Predicting on: split4\n",
      "Predictions for split4 saved!\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "\n",
    "# Specify the run one at a time (with for loop, kernel dies; also, \n",
    "# crf_suite for sklearn's models will keep learning from previous runs if not restarted)\n",
    "run = runs[0]  # 1, 2, 3, 4\n",
    "\n",
    "# Get the train (80%) and test (20%) subsets of data\n",
    "train_splits, test_split = run[0], run[1]\n",
    "print(\"Training on:\", train_splits)\n",
    "train_df = df_grouped.loc[df_grouped[split_col].isin(train_splits)]\n",
    "dev_df = df_grouped.loc[df_grouped[split_col] == test_split]\n",
    "\n",
    "# Zip feature and target columns together so each \n",
    "# sentence item is a tuple: `(TOKEN, TAG_LIST)`\n",
    "train_sentences = utils1.zip1FeatureAndTarget(train_df, \"tag\")\n",
    "dev_sentences = utils1.zip1FeatureAndTarget(dev_df, \"tag\")\n",
    "# Extract features\n",
    "X_train = [utils1.extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [utils1.extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# Extract targets\n",
    "y_train = [utils1.extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [utils1.extractSentenceTargets(sentence) for sentence in dev_sentences]\n",
    "\n",
    "# Train a classification model\n",
    "clf_pers = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf_pers.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict with the trained model\n",
    "print(\"Predicting on:\", test_split)\n",
    "predictions = clf_pers.predict(X_dev)\n",
    "dev_df = dev_df.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "dev_df.insert(len(dev_df.columns), \"tag_pers_o_predicted\", predictions)\n",
    "dev_df = dev_df.set_index([\"sentence_id\", \"fold\"])\n",
    "dev_df_exploded = dev_df.explode(list(dev_df.columns))\n",
    "\n",
    "if pred_df.shape[0] > 0:\n",
    "    pred_df = pd.concat([pred_df, dev_df_exploded])\n",
    "else:\n",
    "    pred_df = dev_df_exploded\n",
    "\n",
    "assert pred_df.loc[pred_df.tag_pers_o_predicted.isna()].shape[0] == 0, \"Any NaN values should be replaced with 'O'\"\n",
    "\n",
    "filename = \"crf_{a}_pers_o_baseline_fastText{d}_nolingfeatures_predictions_{s}.csv\".format(a=a, t=target_labels, d=d, s=test_split)\n",
    "pred_df.to_csv(predictions_dir+filename)\n",
    "\n",
    "print(\"Predictions for {} saved!\".format(test_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(753521, 5)\n"
     ]
    }
   ],
   "source": [
    "pred_df0 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split0.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df1 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split1.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df2 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split2.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df3 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split3.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_df4 = pd.read_csv(predictions_dir+\"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_predictions_split4.csv\".format(a=a, t=target_labels, d=d), index_col=0)\n",
    "pred_perso = pd.concat([pred_df0, pred_df1, pred_df2, pred_df3, pred_df4])\n",
    "print(pred_perso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[B-Masculine]</td>\n",
       "      <td>B-Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[I-Masculine]</td>\n",
       "      <td>I-Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id sentence tag_pers_o_expected  \\\n",
       "0            8  split0       233    James       [B-Masculine]   \n",
       "1            8  split0       234    Whyte       [I-Masculine]   \n",
       "2            8  split0       235      was                 [O]   \n",
       "3            8  split0       236   called                 [O]   \n",
       "4            8  split0       237     upon                 [O]   \n",
       "\n",
       "  tag_pers_o_predicted  \n",
       "0          B-Masculine  \n",
       "1            I-Unknown  \n",
       "2                    O  \n",
       "3                    O  \n",
       "4                    O  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso = pred_perso.reset_index()\n",
    "pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_pers_o_expected\")\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-Unknown', 'I-Masculine', 'B-Masculine', 'I-Occupation', 'B-Occupation', 'B-Unknown', 'I-Feminine', 'B-Feminine']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf_pers.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# y_pred = clf_pers.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.42825014312210974\n",
      "  - Prec: 0.4496106745338277\n",
      "  - Rec 0.41771448419590135\n"
     ]
    }
   ],
   "source": [
    "# print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "# print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_dev_grouped = df_dev_grouped.rename(columns={\"tag\":\"tag_pers_o_expected\"})\n",
    "# df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_pers_o_predicted\", y_pred)\n",
    "# # df_dev_grouped.head()\n",
    "# df_dev_grouped = df_dev_grouped.set_index(\"sentence_id\")\n",
    "# df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns))\n",
    "# df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# filename = \"crf_{a}_pers_o_baseline_fastText{d}_nolingfeatures_predictions.csv\".format(a=a, d=d)\n",
    "# df_dev_exploded.to_csv(config.experiment1_output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eval\"></a>\n",
    "### Evaluation\n",
    "#### Evaluate: Strict, Each Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# a = \"arow\"\n",
    "# category = \"pers_o\"\n",
    "# filename = \"crf_{a}_{c}_baseline_fastText{d}_nolingfeatures_predictions.csv\".format(a=a, c=category, d=d)\n",
    "# pred_perso = pd.read_csv(config.experiment1_output_path+filename)\n",
    "# pred_perso = utils.getColumnValuesAsLists(pred_perso, \"tag_{}_expected\".format(category))\n",
    "# # pred_pers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics for each category of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_pers_o_expected</th>\n",
       "      <th>tag_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[B-Masculine]</td>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[I-Masculine]</td>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id sentence tag_pers_o_expected  \\\n",
       "0            8  split0       233    James       [B-Masculine]   \n",
       "1            8  split0       234    Whyte       [I-Masculine]   \n",
       "2            8  split0       235      was                 [O]   \n",
       "3            8  split0       236   called                 [O]   \n",
       "4            8  split0       237     upon                 [O]   \n",
       "\n",
       "  tag_pers_o_predicted          _merge  \n",
       "0          B-Masculine   true positive  \n",
       "1            I-Unknown  false positive  \n",
       "2                    O   true negative  \n",
       "3                    O   true negative  \n",
       "4                    O   true negative  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso = utils.isPredictedInExpected(pred_perso, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the combined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_{t}_baseline_fastText{d}_nolingfeatures_evaluation.csv\".format(a=a, t=target_labels, d=d)\n",
    "pred_perso.to_csv(predictions_dir+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>2337</td>\n",
       "      <td>2529</td>\n",
       "      <td>3496</td>\n",
       "      <td>0.580249</td>\n",
       "      <td>0.599349</td>\n",
       "      <td>0.589644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>4858</td>\n",
       "      <td>4434</td>\n",
       "      <td>6261</td>\n",
       "      <td>0.585414</td>\n",
       "      <td>0.563090</td>\n",
       "      <td>0.574035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>109</td>\n",
       "      <td>436</td>\n",
       "      <td>747</td>\n",
       "      <td>0.631445</td>\n",
       "      <td>0.872664</td>\n",
       "      <td>0.732712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>477</td>\n",
       "      <td>1024</td>\n",
       "      <td>1492</td>\n",
       "      <td>0.593005</td>\n",
       "      <td>0.757745</td>\n",
       "      <td>0.665329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>816</td>\n",
       "      <td>1101</td>\n",
       "      <td>1531</td>\n",
       "      <td>0.581687</td>\n",
       "      <td>0.652322</td>\n",
       "      <td>0.614983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>1631</td>\n",
       "      <td>2807</td>\n",
       "      <td>1886</td>\n",
       "      <td>0.401875</td>\n",
       "      <td>0.536252</td>\n",
       "      <td>0.459440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>1021</td>\n",
       "      <td>1016</td>\n",
       "      <td>1521</td>\n",
       "      <td>0.599527</td>\n",
       "      <td>0.598348</td>\n",
       "      <td>0.598937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>1626</td>\n",
       "      <td>1488</td>\n",
       "      <td>1538</td>\n",
       "      <td>0.508262</td>\n",
       "      <td>0.486094</td>\n",
       "      <td>0.496931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     B-Unknown            2337            2529           3496   0.580249   \n",
       "0     I-Unknown            4858            4434           6261   0.585414   \n",
       "0    B-Feminine             109             436            747   0.631445   \n",
       "0    I-Feminine             477            1024           1492   0.593005   \n",
       "0   B-Masculine             816            1101           1531   0.581687   \n",
       "0   I-Masculine            1631            2807           1886   0.401875   \n",
       "0  B-Occupation            1021            1016           1521   0.599527   \n",
       "0  I-Occupation            1626            1488           1538   0.508262   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.599349  0.589644  \n",
       "0  0.563090  0.574035  \n",
       "0  0.872664  0.732712  \n",
       "0  0.757745  0.665329  \n",
       "0  0.652322  0.614983  \n",
       "0  0.536252  0.459440  \n",
       "0  0.598348  0.598937  \n",
       "0  0.486094  0.496931  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso, \"_merge\", pers_o_label_subset[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(pers_o_label_subset)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso, \"_merge\", pers_o_label_subset[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_strict_agmt.csv\".format(a=a, c=category, d=d)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_strict_agmt.csv\".format(a=a, c=category, d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation Agreement\n",
    "\n",
    "Calculate agreement at the annotation level, so if the model labels any word correctly from a manually annotated text span, that annotation is recorded as being correctly labeled (`true positive`).  Note whether the models' labels are an `exact_match`, `label_match`, `category_match` or `mismatch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the annotation data:\n",
    "\n",
    "*Note: `ann_id` of `9999` indicates no annotation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_df =  pd.read_csv(config.tokc_path+\"experiment_input/token_train.csv\", usecols=[\"sentence_id\", \"ann_id\", \"token_id\", \"tag\"])\n",
    "# dev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the annotation data by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ann = perso_df[[\"sentence_id\", \"ann_id\", \"token_id\", \"tag\"]] \n",
    "# ---------------------------\n",
    "df_ann = utils.implodeDataFrame(df_ann, [\"sentence_id\", \"ann_id\", \"token_id\"])\n",
    "df_ann = df_ann.reset_index()\n",
    "# print(df_ann.shape)\n",
    "# df_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the columns of the dev and prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename `sentence` column `token`\n",
    "pred_perso = pred_perso.rename(columns={\"sentence\":\"token\"})\n",
    "# pred_perso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data, adding the annotation IDs (`ann_id` column) to the prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [\"sentence_id\", \"token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso.join(df_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag\"])  # duplicate of tag_expected\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_perso_ann.loc[pred_perso_ann[\"tag_pers_o_expected\"].isna()].shape[0] == 0\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_ann = pred_perso_ann.explode([\"tag_pers_o_expected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the BIO tags to label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "pred_labels = list(pred_perso_ann[\"tag_{}_predicted\".format(category)])\n",
    "pred_labels = [label if label == \"O\" else label[2:] for label in pred_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_predicted\".format(category), pred_labels)\n",
    "# Get the lists of expected labels\n",
    "exp_labels = list(pred_perso_ann[\"tag_{}_expected\".format(category)])\n",
    "exp_labels = [label if label == \"O\" else label[2:] for label in exp_labels]\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"label_{}_expected\".format(category), exp_labels)\n",
    "# pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>[split4, split4, split4]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Identifier, :, AA5]</td>\n",
       "      <td>[true negative, true negative, true negative]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>[split2, split2, split2, split2, split2, split...</td>\n",
       "      <td>[7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...</td>\n",
       "      <td>[The, The, The, Very, Very, Very, Rev, Rev, Re...</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Masculine, Masculine, Masculine, Unknown, Unk...</td>\n",
       "      <td>[O, Unknown, Masculine, Unknown, Masculine, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24275</td>\n",
       "      <td>[split2, split2, split2, split2, split2, split...</td>\n",
       "      <td>[7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...</td>\n",
       "      <td>[The, The, The, Very, Very, Very, Rev, Rev, Re...</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Masculine, Masculine, Masculine, Unknown, Unk...</td>\n",
       "      <td>[O, Unknown, Masculine, Unknown, Masculine, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26233</td>\n",
       "      <td>[split2, split2, split2, split2, split2, split...</td>\n",
       "      <td>[9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 1...</td>\n",
       "      <td>[Rev, Rev, Rev, Rev, Prof, Prof, Prof, Prof, J...</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Unknown, Unknown, Unknown, Unknown, Unknown, ...</td>\n",
       "      <td>[Unknown, Masculine, Unknown, O, Unknown, O, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>52952</td>\n",
       "      <td>[split2, split2, split2, split2, split2, split...</td>\n",
       "      <td>[7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...</td>\n",
       "      <td>[The, The, The, Very, Very, Very, Rev, Rev, Re...</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Masculine, Masculine, Masculine, Unknown, Unk...</td>\n",
       "      <td>[O, Unknown, Masculine, Unknown, Masculine, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id                                               fold  \\\n",
       "0            0   99999                           [split4, split4, split4]   \n",
       "1            1   14384  [split2, split2, split2, split2, split2, split...   \n",
       "2            1   24275  [split2, split2, split2, split2, split2, split...   \n",
       "3            1   26233  [split2, split2, split2, split2, split2, split...   \n",
       "4            1   52952  [split2, split2, split2, split2, split2, split...   \n",
       "\n",
       "                                            token_id  \\\n",
       "0                                          [0, 1, 2]   \n",
       "1  [7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...   \n",
       "2  [7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...   \n",
       "3  [9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 1...   \n",
       "4  [7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...   \n",
       "\n",
       "                                               token  \\\n",
       "0                               [Identifier, :, AA5]   \n",
       "1  [The, The, The, Very, Very, Very, Rev, Rev, Re...   \n",
       "2  [The, The, The, Very, Very, Very, Rev, Rev, Re...   \n",
       "3  [Rev, Rev, Rev, Rev, Prof, Prof, Prof, Prof, J...   \n",
       "4  [The, The, The, Very, Very, Very, Rev, Rev, Re...   \n",
       "\n",
       "                                              _merge  \\\n",
       "0      [true negative, true negative, true negative]   \n",
       "1  [true positive, true positive, true positive, ...   \n",
       "2  [true positive, true positive, true positive, ...   \n",
       "3  [true positive, true positive, true positive, ...   \n",
       "4  [true positive, true positive, true positive, ...   \n",
       "\n",
       "                              label_pers_o_predicted  \\\n",
       "0                                          [O, O, O]   \n",
       "1  [Masculine, Masculine, Masculine, Unknown, Unk...   \n",
       "2  [Masculine, Masculine, Masculine, Unknown, Unk...   \n",
       "3  [Unknown, Unknown, Unknown, Unknown, Unknown, ...   \n",
       "4  [Masculine, Masculine, Masculine, Unknown, Unk...   \n",
       "\n",
       "                               label_pers_o_expected  \n",
       "0                                          [O, O, O]  \n",
       "1  [O, Unknown, Masculine, Unknown, Masculine, O,...  \n",
       "2  [O, Unknown, Masculine, Unknown, Masculine, O,...  \n",
       "3  [Unknown, Masculine, Unknown, O, Unknown, O, M...  \n",
       "4  [O, Unknown, Masculine, Unknown, Masculine, O,...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann = pred_perso_ann.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_ann = utils.implodeDataFrame(pred_perso_ann, [\"sentence_id\", \"ann_id\"])\n",
    "pred_perso_ann = pred_perso_ann.reset_index()\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreements and disagreements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_types_perso, agmt_labels_perso = utils1.getAnnotationAgreement(pred_perso_ann, \"label_pers_o_predicted\", \"label_pers_o_expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>_merge</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>annotation_agreement</th>\n",
       "      <th>agreement_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>99999</td>\n",
       "      <td>[split4, split4, split4]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Identifier, :, AA5]</td>\n",
       "      <td>[true negative, true negative, true negative]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>true negative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>[split2, split2, split2, split2, split2, split...</td>\n",
       "      <td>[7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...</td>\n",
       "      <td>[The, The, The, Very, Very, Very, Rev, Rev, Re...</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Masculine, Masculine, Masculine, Unknown, Unk...</td>\n",
       "      <td>[O, Unknown, Masculine, Unknown, Masculine, O,...</td>\n",
       "      <td>true positive</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24275</td>\n",
       "      <td>[split2, split2, split2, split2, split2, split...</td>\n",
       "      <td>[7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...</td>\n",
       "      <td>[The, The, The, Very, Very, Very, Rev, Rev, Re...</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Masculine, Masculine, Masculine, Unknown, Unk...</td>\n",
       "      <td>[O, Unknown, Masculine, Unknown, Masculine, O,...</td>\n",
       "      <td>true positive</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26233</td>\n",
       "      <td>[split2, split2, split2, split2, split2, split...</td>\n",
       "      <td>[9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 1...</td>\n",
       "      <td>[Rev, Rev, Rev, Rev, Prof, Prof, Prof, Prof, J...</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Unknown, Unknown, Unknown, Unknown, Unknown, ...</td>\n",
       "      <td>[Unknown, Masculine, Unknown, O, Unknown, O, M...</td>\n",
       "      <td>true positive</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>52952</td>\n",
       "      <td>[split2, split2, split2, split2, split2, split...</td>\n",
       "      <td>[7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...</td>\n",
       "      <td>[The, The, The, Very, Very, Very, Rev, Rev, Re...</td>\n",
       "      <td>[true positive, true positive, true positive, ...</td>\n",
       "      <td>[Masculine, Masculine, Masculine, Unknown, Unk...</td>\n",
       "      <td>[O, Unknown, Masculine, Unknown, Masculine, O,...</td>\n",
       "      <td>true positive</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id                                               fold  \\\n",
       "0            0   99999                           [split4, split4, split4]   \n",
       "1            1   14384  [split2, split2, split2, split2, split2, split...   \n",
       "2            1   24275  [split2, split2, split2, split2, split2, split...   \n",
       "3            1   26233  [split2, split2, split2, split2, split2, split...   \n",
       "4            1   52952  [split2, split2, split2, split2, split2, split...   \n",
       "\n",
       "                                            token_id  \\\n",
       "0                                          [0, 1, 2]   \n",
       "1  [7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...   \n",
       "2  [7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...   \n",
       "3  [9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 1...   \n",
       "4  [7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 10, 10, 10, 10,...   \n",
       "\n",
       "                                               token  \\\n",
       "0                               [Identifier, :, AA5]   \n",
       "1  [The, The, The, Very, Very, Very, Rev, Rev, Re...   \n",
       "2  [The, The, The, Very, Very, Very, Rev, Rev, Re...   \n",
       "3  [Rev, Rev, Rev, Rev, Prof, Prof, Prof, Prof, J...   \n",
       "4  [The, The, The, Very, Very, Very, Rev, Rev, Re...   \n",
       "\n",
       "                                              _merge  \\\n",
       "0      [true negative, true negative, true negative]   \n",
       "1  [true positive, true positive, true positive, ...   \n",
       "2  [true positive, true positive, true positive, ...   \n",
       "3  [true positive, true positive, true positive, ...   \n",
       "4  [true positive, true positive, true positive, ...   \n",
       "\n",
       "                              label_pers_o_predicted  \\\n",
       "0                                          [O, O, O]   \n",
       "1  [Masculine, Masculine, Masculine, Unknown, Unk...   \n",
       "2  [Masculine, Masculine, Masculine, Unknown, Unk...   \n",
       "3  [Unknown, Unknown, Unknown, Unknown, Unknown, ...   \n",
       "4  [Masculine, Masculine, Masculine, Unknown, Unk...   \n",
       "\n",
       "                               label_pers_o_expected annotation_agreement  \\\n",
       "0                                          [O, O, O]        true negative   \n",
       "1  [O, Unknown, Masculine, Unknown, Masculine, O,...        true positive   \n",
       "2  [O, Unknown, Masculine, Unknown, Masculine, O,...        true positive   \n",
       "3  [Unknown, Masculine, Unknown, O, Unknown, O, M...        true positive   \n",
       "4  [O, Unknown, Masculine, Unknown, Masculine, O,...        true positive   \n",
       "\n",
       "  agreement_label  \n",
       "0               O  \n",
       "1         Unknown  \n",
       "2         Unknown  \n",
       "3         Unknown  \n",
       "4         Unknown  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"annotation_agreement\", agmt_types_perso)\n",
    "pred_perso_ann.insert(len(pred_perso_ann.columns), \"agreement_label\", agmt_labels_perso)\n",
    "pred_perso_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>false negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>false positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>10537</td>\n",
       "      <td>14834</td>\n",
       "      <td>6152</td>\n",
       "      <td>0.706852</td>\n",
       "      <td>0.584683</td>\n",
       "      <td>0.639990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person Name</td>\n",
       "      <td>9045</td>\n",
       "      <td>12875</td>\n",
       "      <td>5199</td>\n",
       "      <td>0.712349</td>\n",
       "      <td>0.587363</td>\n",
       "      <td>0.643847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>5820</td>\n",
       "      <td>6783</td>\n",
       "      <td>2702</td>\n",
       "      <td>0.715129</td>\n",
       "      <td>0.538205</td>\n",
       "      <td>0.614180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>714</td>\n",
       "      <td>1912</td>\n",
       "      <td>746</td>\n",
       "      <td>0.719338</td>\n",
       "      <td>0.728104</td>\n",
       "      <td>0.723694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2511</td>\n",
       "      <td>4180</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.704772</td>\n",
       "      <td>0.624720</td>\n",
       "      <td>0.662336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>1492</td>\n",
       "      <td>1959</td>\n",
       "      <td>953</td>\n",
       "      <td>0.672734</td>\n",
       "      <td>0.567662</td>\n",
       "      <td>0.615747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels  false negative  true positive  false positive  precision  \\\n",
       "0          all           10537          14834            6152   0.706852   \n",
       "0  Person Name            9045          12875            5199   0.712349   \n",
       "0      Unknown            5820           6783            2702   0.715129   \n",
       "0     Feminine             714           1912             746   0.719338   \n",
       "0    Masculine            2511           4180            1751   0.704772   \n",
       "0   Occupation            1492           1959             953   0.672734   \n",
       "\n",
       "     recall       f_1  \n",
       "0  0.584683  0.639990  \n",
       "0  0.587363  0.643847  \n",
       "0  0.538205  0.614180  \n",
       "0  0.728104  0.723694  \n",
       "0  0.624720  0.662336  \n",
       "0  0.567662  0.615747  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_perso_all = utils1.getAnnotationAgreementMetrics(pred_perso_ann, \"all\")\n",
    "metrics_perso_pn = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[~(pred_perso_ann.agreement_label.isin([\"Occupation\",\"O\"]))], \"Person Name\")\n",
    "metrics_perso_unk = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Unknown\"], \"Unknown\")\n",
    "metrics_perso_fem = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Feminine\"], \"Feminine\")\n",
    "metrics_perso_mas = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Masculine\"], \"Masculine\")\n",
    "metrics_perso_occ = utils1.getAnnotationAgreementMetrics(pred_perso_ann.loc[pred_perso_ann.agreement_label == \"Occupation\"], \"Occupation\")\n",
    "metrics_perso = pd.concat([metrics_perso_all, metrics_perso_pn, metrics_perso_unk, metrics_perso_fem, metrics_perso_mas, metrics_perso_occ])\n",
    "metrics_perso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics and annotation-level data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_perso.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_annot_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "metrics_perso.to_csv(agreement_dir+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_annot_agmt.csv\".format(a=a, d=d, c=category))\n",
    "pred_perso_ann[[\"sentence_id\", \"ann_id\", \"fold\", \"token_id\", \"annotation_agreement\", \"agreement_label\"]].to_csv(\n",
    "    predictions_dir+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_annot_evaluation.csv\".format(a=a, d=d, c=category)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loose Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the manual annotation evaluation, we want to evaluate the predictions more loosely, considering overlapping text spans in addition to exactly matching text spans.\n",
    "\n",
    "#### Token Agreement\n",
    "\n",
    "First, generalize the tokens' IOB tags to the label, and calculate agreement scores for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels = pred_perso.copy()\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"_merge\"])\n",
    "tag_exp = list(pred_perso_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_perso_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_perso_labels = pred_perso_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_perso_labels.insert(len(pred_perso_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_pers_labels.loc[pred_pers_labels.label_personname_predicted == \"Feminine\"].head()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the agreement metrics at the label level for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>7179</td>\n",
       "      <td>6197</td>\n",
       "      <td>10523</td>\n",
       "      <td>0.629366</td>\n",
       "      <td>0.594453</td>\n",
       "      <td>0.611411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>584</td>\n",
       "      <td>1264</td>\n",
       "      <td>2435</td>\n",
       "      <td>0.658286</td>\n",
       "      <td>0.806558</td>\n",
       "      <td>0.724918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>2438</td>\n",
       "      <td>3488</td>\n",
       "      <td>3837</td>\n",
       "      <td>0.523823</td>\n",
       "      <td>0.611474</td>\n",
       "      <td>0.564265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>2647</td>\n",
       "      <td>2287</td>\n",
       "      <td>3276</td>\n",
       "      <td>0.588891</td>\n",
       "      <td>0.553098</td>\n",
       "      <td>0.570434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0     Unknown            7179            6197          10523   0.629366   \n",
       "0    Feminine             584            1264           2435   0.658286   \n",
       "0   Masculine            2438            3488           3837   0.523823   \n",
       "0  Occupation            2647            2287           3276   0.588891   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.594453  0.611411  \n",
       "0  0.806558  0.724918  \n",
       "0  0.611474  0.564265  \n",
       "0  0.553098  0.570434  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['Unknown', 'Feminine', 'Masculine', 'Occupation']\n",
    "pred_perso_labels = utils.isPredictedInExpected(pred_perso_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "pred_perso_stats = utils.getScoresByCatTags(\n",
    "    pred_perso_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_perso_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_perso_stats = pd.concat([pred_perso_stats, tag_stats])\n",
    "pred_perso_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine and save the performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_perso_stats.to_csv(\n",
    "#     config.experiment1_agmt_path+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_loose_agmt.csv\".format(a=a, d=d, c=category)\n",
    "# )\n",
    "pred_perso_stats.to_csv(agreement_dir+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_loose_agmt.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>label_pers_o_expected</th>\n",
       "      <th>label_pers_o_predicted</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>233</td>\n",
       "      <td>James</td>\n",
       "      <td>[Masculine]</td>\n",
       "      <td>Masculine</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>234</td>\n",
       "      <td>Whyte</td>\n",
       "      <td>[Masculine]</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>false positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>235</td>\n",
       "      <td>was</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>236</td>\n",
       "      <td>called</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>split0</td>\n",
       "      <td>237</td>\n",
       "      <td>upon</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    fold  token_id   token label_pers_o_expected  \\\n",
       "0            8  split0       233   James           [Masculine]   \n",
       "1            8  split0       234   Whyte           [Masculine]   \n",
       "2            8  split0       235     was                   [O]   \n",
       "3            8  split0       236  called                   [O]   \n",
       "4            8  split0       237    upon                   [O]   \n",
       "\n",
       "  label_pers_o_predicted          _merge  \n",
       "0              Masculine   true positive  \n",
       "1                Unknown  false positive  \n",
       "2                      O   true negative  \n",
       "3                      O   true negative  \n",
       "4                      O   true negative  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_perso_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the loose predictions (with labels instead of BIO tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perso_labels.to_csv(predictions_dir+\"crf_{a}_baseline_fastText{d}_{c}_nolingfeatures_loose_evaluation.csv\".format(a=a, d=d, c=category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

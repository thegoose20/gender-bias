{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data for Baseline Gender Bias Token Classifiers\n",
    "\n",
    "* **Supervised learning**\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/doc_clf_data/`\n",
    "* **Multilabel classification**\n",
    "    * 3 categories of labels:\n",
    "        1. *Person Name:* Unknown, Non-binary, Feminine, Masculine\n",
    "        2. *Linguistic:* Generalization, Gendered Pronoun, Gendered Role\n",
    "        3. *Contextual:* Empowering, Occupation, Omission, Stereotype\n",
    "\n",
    "***\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I. Setup](#i)\n",
    "\n",
    "[II. Tokenization and Categorization](#ii)\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"i\"></a>\n",
    "\n",
    "## I. Setup\n",
    "\n",
    "**Import libraries and load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions\n",
    "# import utils\n",
    "\n",
    "# For working with data files and directories\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing the text\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# # For visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # For classification with scikit-learn\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "# # from sklearn.svm import SVC\n",
    "# # from sklearn.tree import DecisionTreeClassifier\n",
    "# # from sklearn.tree import export_text\n",
    "# # from sklearn import tree\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n",
    "# from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same data splits used for document classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = PlaintextCorpusReader(data_dir, \".*.txt\")\n",
    "# print(corpus.fileids())\n",
    "\n",
    "# RERUN WITH THESE FILES!!!\n",
    "# dir_name = \"data/aggregated_data/splits/\"\n",
    "# Path(dir_name).mkdir(parents=True, exist_ok=True)\n",
    "# train.to_csv(dir_name+\"aggregated_train.csv\")\n",
    "# validate.to_csv(dir_name+\"aggregated_validate.csv\")\n",
    "# test.to_csv(dir_name+\"aggregated_test.csv\")\n",
    "\n",
    "df = pd.read_csv(\"../data/aggregated_data/aggregated_with_eadid_descid_desc_cols.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>eadid</th>\n",
       "      <th>field</th>\n",
       "      <th>file_ann</th>\n",
       "      <th>offsets_ann</th>\n",
       "      <th>text_ann</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>file_desc</th>\n",
       "      <th>desc_id</th>\n",
       "      <th>file</th>\n",
       "      <th>desc_start_offset</th>\n",
       "      <th>desc_end_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BAI</td>\n",
       "      <td>Title</td>\n",
       "      <td>BAI_01000.ann</td>\n",
       "      <td>(1290, 1302)</td>\n",
       "      <td>John Baillie</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Person-Name</td>\n",
       "      <td>211</td>\n",
       "      <td>John Baillie: posthumous</td>\n",
       "      <td>BAI_01000.txt</td>\n",
       "      <td>70381</td>\n",
       "      <td>BAI_01000.txt</td>\n",
       "      <td>1290</td>\n",
       "      <td>1315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>BAI</td>\n",
       "      <td>Scope and Contents</td>\n",
       "      <td>BAI_01300.ann</td>\n",
       "      <td>(5875, 5894)</td>\n",
       "      <td>Henry Sloane Coffin</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Person-Name</td>\n",
       "      <td>524</td>\n",
       "      <td>Letters received from Henry Sloane Coffin, wit...</td>\n",
       "      <td>BAI_01300.txt</td>\n",
       "      <td>47675</td>\n",
       "      <td>BAI_01300.txt</td>\n",
       "      <td>5853</td>\n",
       "      <td>5983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BAI</td>\n",
       "      <td>Scope and Contents</td>\n",
       "      <td>BAI_01300.ann</td>\n",
       "      <td>(5925, 5936)</td>\n",
       "      <td>Hugh Martin</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Person-Name</td>\n",
       "      <td>525</td>\n",
       "      <td>Letters received from Henry Sloane Coffin, wit...</td>\n",
       "      <td>BAI_01300.txt</td>\n",
       "      <td>47675</td>\n",
       "      <td>BAI_01300.txt</td>\n",
       "      <td>5853</td>\n",
       "      <td>5983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BAI</td>\n",
       "      <td>Scope and Contents</td>\n",
       "      <td>BAI_01300.ann</td>\n",
       "      <td>(5951, 5963)</td>\n",
       "      <td>John Baillie</td>\n",
       "      <td>Masculine</td>\n",
       "      <td>Person-Name</td>\n",
       "      <td>526</td>\n",
       "      <td>Letters received from Henry Sloane Coffin, wit...</td>\n",
       "      <td>BAI_01300.txt</td>\n",
       "      <td>47675</td>\n",
       "      <td>BAI_01300.txt</td>\n",
       "      <td>5853</td>\n",
       "      <td>5983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BAI</td>\n",
       "      <td>Scope and Contents</td>\n",
       "      <td>BAI_01300.ann</td>\n",
       "      <td>(5951, 5963)</td>\n",
       "      <td>John Baillie</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Person-Name</td>\n",
       "      <td>527</td>\n",
       "      <td>Letters received from Henry Sloane Coffin, wit...</td>\n",
       "      <td>BAI_01300.txt</td>\n",
       "      <td>47675</td>\n",
       "      <td>BAI_01300.txt</td>\n",
       "      <td>5853</td>\n",
       "      <td>5983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id eadid               field       file_ann   offsets_ann  \\\n",
       "0       0   BAI               Title  BAI_01000.ann  (1290, 1302)   \n",
       "1       1   BAI  Scope and Contents  BAI_01300.ann  (5875, 5894)   \n",
       "2       2   BAI  Scope and Contents  BAI_01300.ann  (5925, 5936)   \n",
       "3       3   BAI  Scope and Contents  BAI_01300.ann  (5951, 5963)   \n",
       "4       4   BAI  Scope and Contents  BAI_01300.ann  (5951, 5963)   \n",
       "\n",
       "              text_ann      label     category   id  \\\n",
       "0         John Baillie    Unknown  Person-Name  211   \n",
       "1  Henry Sloane Coffin    Unknown  Person-Name  524   \n",
       "2          Hugh Martin    Unknown  Person-Name  525   \n",
       "3         John Baillie  Masculine  Person-Name  526   \n",
       "4         John Baillie    Unknown  Person-Name  527   \n",
       "\n",
       "                                         description      file_desc  desc_id  \\\n",
       "0                           John Baillie: posthumous  BAI_01000.txt    70381   \n",
       "1  Letters received from Henry Sloane Coffin, wit...  BAI_01300.txt    47675   \n",
       "2  Letters received from Henry Sloane Coffin, wit...  BAI_01300.txt    47675   \n",
       "3  Letters received from Henry Sloane Coffin, wit...  BAI_01300.txt    47675   \n",
       "4  Letters received from Henry Sloane Coffin, wit...  BAI_01300.txt    47675   \n",
       "\n",
       "            file  desc_start_offset  desc_end_offset  \n",
       "0  BAI_01000.txt               1290             1315  \n",
       "1  BAI_01300.txt               5853             5983  \n",
       "2  BAI_01300.txt               5853             5983  \n",
       "3  BAI_01300.txt               5853             5983  \n",
       "4  BAI_01300.txt               5853             5983  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename({\"Unnamed: 0\":\"ann_id\"}, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new directory to save the token-level classification data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dir = \"../data/token_clf_data/\"\n",
    "Path(new_data_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ii\"></a>\n",
    "## II. Tokenization and Categorization\n",
    "\n",
    "**Categorize every token as either:**\n",
    "* **`B-[LABEL_NAME]` for the *beginning* token of an annotated text span with a particular label**\n",
    "* **`I` for *inside* an annotated text span**\n",
    "* **`O` or *outside* an annotated text span, respectively**\n",
    "\n",
    "**Associate each token with a description ID and, for `B-` and `I` tokens, an annotation ID.**\n",
    "\n",
    "#### Create dictionaries with relevant data from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique description data\n",
    "desc_df = pd.DataFrame({\"description\":df.description, \"desc_id\": df.desc_id, \"desc_start_offset\":df.desc_start_offset, \"desc_end_offset\":df.desc_end_offset})\n",
    "desc_df = desc_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate each description with its ID: \n",
    "# create a dictionary with description texts as keys and description identifiers as values\n",
    "\n",
    "text_desc_list = list(desc_df.description)\n",
    "desc_id_list = list(desc_df.desc_id)\n",
    "desc_text_to_id = dict(zip(text_desc_list, desc_id_list))\n",
    "\n",
    "assert desc_text_to_id[\"John Baillie: posthumous\"] == 70381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1290, 1315), (5853, 5983), (5967, 6202), (5297, 5506), (15180, 15419)]\n",
      "(1290, 1315)\n"
     ]
    }
   ],
   "source": [
    "# Associate each description ID to its offsets:\n",
    "# create a dictionary with description IDs as keys and tuples of offsets as values\n",
    "\n",
    "desc_starts = list(desc_df.desc_start_offset)\n",
    "desc_ends = list(desc_df.desc_end_offset)\n",
    "desc_offset_tuples = [tuple((desc_starts[i], desc_ends[i])) for i in range(len(desc_starts))]\n",
    "print(desc_offset_tuples[:5])\n",
    "\n",
    "desc_id_to_offsets = dict(zip(desc_id_list, desc_offset_tuples))\n",
    "print(desc_id_to_offsets[70381])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate each description ID to its annotations: \n",
    "# create a dictionary with description IDs as keys and arrays of annotation IDs as values \n",
    "\n",
    "id_df = pd.DataFrame({\"ann_id\":df.ann_id, \"desc_id\":df.desc_id})\n",
    "desc_id_to_ann_id = dict.fromkeys(desc_id_list)\n",
    "for desc_id in desc_id_list:\n",
    "    desc_ann_ids = list((id_df.loc[id_df.desc_id == desc_id]).ann_id)\n",
    "    desc_id_to_ann_id[desc_id] = desc_ann_ids\n",
    "\n",
    "assert 2 in desc_id_to_ann_id[47675]\n",
    "assert not 0 in desc_id_to_ann_id[47675]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate each annotation ID to its label, text span, and offsets:\n",
    "# create a dictionaries with annotation IDs as keys and labels, text spans, and offsets as values\n",
    "\n",
    "ann_id_list = list(df.ann_id)\n",
    "# Make sure every annotation ID is unique\n",
    "assert len(list(df.ann_id)) == len(set(df.ann_id))\n",
    "    \n",
    "# Store the annotation's label name\n",
    "ann_id_to_label = dict(zip(ann_id_list, list(df.label)))\n",
    "\n",
    "# Store the text span of the annotation\n",
    "ann_id_to_text = dict(zip(ann_id_list, list(df.text_ann)))\n",
    "\n",
    "# Store the annotation's start and end offsets (as a tuple, like the description offsets)\n",
    "ann_id_list = list(df.ann_id)\n",
    "ann_offsets_list = list(df.offsets_ann)\n",
    "ann_offsets_tuples = []\n",
    "for offset in ann_offsets_list:\n",
    "    offset_str_pair = offset[1:-1].split(\", \")\n",
    "    offset_int_pair = [int(o) for o in offset_str_pair]\n",
    "    ann_offsets_tuples += [offset_int_pair]\n",
    "ann_id_to_offsets = dict(zip(ann_id_list, ann_offsets_tuples))\n",
    "assert ann_id_to_offsets[ann_id_list[2]] == ann_offsets_tuples[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Descriptions and Calculate Token Offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

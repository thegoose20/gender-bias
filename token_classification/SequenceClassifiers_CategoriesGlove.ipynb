{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Gender Biased Token Classifiers\n",
    "\n",
    "### Target: Label Categories\n",
    "\n",
    "### Word Embeddings: Custom fastText\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/model_input/`\n",
    "    * Prediction Data: Data: under directory `../data/token_clf_data/model_output/`\n",
    "* Sequence classification\n",
    "    * 3 categories of labels: Linguistic, Person Name, Contextual\n",
    "    * 1 model per category\n",
    "* Word embeddings\n",
    "    * Custom fastText (word2vec with subwords, trained on Archives' descriptive metadata extracted in October 2020)  \n",
    "\n",
    "***\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "**[0.](#0) Preprocessing**\n",
    "\n",
    "**[1.](#1) Baseline Model**\n",
    "\n",
    "**[2.](#2) Hyperparameter Optimization**\n",
    "\n",
    "**[3.](#3) Error Analysis**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import scipy.stats\n",
    "# For classification\n",
    "# from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer, FunctionTransformer, OneHotEncoder\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# LR with OvR provides multilabel model\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# -------------------------------\n",
    "# Multilabel models in sklearn\n",
    "# -------------------------------\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "#     tree.DecisionTreeClassifier\n",
    "#     tree.ExtraTreeClassifier\n",
    "#     ensemble.ExtraTreesClassifier\n",
    "#     neighbors.KNeighborsClassifier\n",
    "#     neural_network.MLPClassifier\n",
    "#     neighbors.RadiusNeighborsClassifier\n",
    "#     linear_model.RidgeClassifier\n",
    "#     linear_model.RidgeClassifierCV\n",
    "\n",
    "# For evaluation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from intervaltree import Interval, IntervalTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## 0. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and validation (dev) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicate rows with all but the same annotation ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463441, 9) (156146, 9)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop(columns=[\"ann_id\"])\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev.drop(columns=[\"ann_id\"])\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Non-binary labels as these were mistaken labels identified early on that were meant to be excluded, and because only one token has this label, it prevents the data from being input into the models with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.tag != \"B-Nonbinary\"]\n",
    "df_train = df_train.loc[df_train.tag != \"I-Nonbinary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463439, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Label Categories\n",
    "\n",
    "Add the annotation label categories as a column of higher-level Inside-Outside-Beginning (IOB) tags so they can be used as targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = utils.addCategoryTagColumn(df_train)\n",
    "# df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "      <th>tag_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>(907, 912)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>(913, 916)</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>B-Linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>(917, 927)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>(928, 930)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>B-Linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>(931, 936)</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     description_id  sentence_id  token_id       token token_offsets   pos  \\\n",
       "172               3            5       154       After    (907, 912)    IN   \n",
       "173               3            5       155         his    (913, 916)  PRP$   \n",
       "174               3            5       156  ordination    (917, 927)    NN   \n",
       "175               3            5       157          he    (928, 930)   PRP   \n",
       "176               3            5       158       spent    (931, 936)   VBD   \n",
       "\n",
       "                    tag                      field subset       tag_cat  \n",
       "172                   O  Biographical / Historical    dev             O  \n",
       "173  B-Gendered-Pronoun  Biographical / Historical    dev  B-Linguistic  \n",
       "174                   O  Biographical / Historical    dev             O  \n",
       "175  B-Gendered-Pronoun  Biographical / Historical    dev  B-Linguistic  \n",
       "176                   O  Biographical / Historical    dev             O  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = utils.addCategoryTagColumn(df_dev)\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that won't be used as features for the classifiers and remove any duplicate rows that remain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"sentence_id\", \"token_id\", \"pos\", \"token\", \"tag_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[cols_to_keep]\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev[cols_to_keep]\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "# df_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create columns for each category so they can be passed into the models as individual features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ling_cat_tags = [\"B-Linguistic\", \"I-Linguistic\"]\n",
    "# df_train_ling = df_train.loc[df_train.tag_cat.isin(ling_cat_tags)]\n",
    "# df_dev_ling = df_dev.loc[df_dev.tag_cat.isin(ling_cat_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pers_cat_tags = [\"B-Person-Name\", \"I-Person-Name\"]\n",
    "# df_train_pers = df_train.loc[df_train.tag_cat.isin(pers_cat_tags)]\n",
    "# df_dev_pers = df_dev.loc[df_dev.tag_cat.isin(pers_cat_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont_cat_tags = [\"B-Contextual\", \"I-Contextual\"]\n",
    "# df_train_cont = df_train.loc[df_train.tag_cat.isin(cont_cat_tags)]\n",
    "# df_dev_cont = df_dev.loc[df_dev.tag_cat.isin(cont_cat_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = (df_train.drop(columns=[\"tag_cat\"])).drop_duplicates()\n",
    "# df_dev = (df_dev.drop(columns=[\"tag_cat\"])).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_cols = [\"sentence_id\", \"token_id\", \"pos\", \"token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.join(df_train_ling.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "# df_train = df_train.join(df_train_pers.set_index(join_cols), on=join_cols, how=\"outer\", lsuffix=\"\", rsuffix=\"_personname\")\n",
    "# df_train = df_train.join(df_train_cont.set_index(join_cols), on=join_cols, how=\"outer\", lsuffix=\"\", rsuffix=\"_contextual\")\n",
    "# df_train = df_train.rename(columns={\"tag_cat\":\"tag_cat_linguistic\"})\n",
    "# # df_train.head(30)  # Should have one row per token!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev = df_dev.join(df_dev_ling.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "# df_dev = df_dev.join(df_dev_pers.set_index(join_cols), on=join_cols, how=\"outer\", lsuffix=\"\", rsuffix=\"_personname\")\n",
    "# df_dev = df_dev.join(df_dev_cont.set_index(join_cols), on=join_cols, how=\"outer\", lsuffix=\"\", rsuffix=\"_contextual\")\n",
    "# df_dev = df_dev.rename(columns={\"tag_cat\":\"tag_cat_linguistic\"})\n",
    "# # df_dev.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_cat_linguistic</th>\n",
       "      <th>tag_cat_personname</th>\n",
       "      <th>tag_cat_contextual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>779229</th>\n",
       "      <td>42027</td>\n",
       "      <td>753891</td>\n",
       "      <td>NN</td>\n",
       "      <td>treatment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779230</th>\n",
       "      <td>42027</td>\n",
       "      <td>753892</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779231</th>\n",
       "      <td>42027</td>\n",
       "      <td>753893</td>\n",
       "      <td>NN</td>\n",
       "      <td>homosexuality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779232</th>\n",
       "      <td>42027</td>\n",
       "      <td>753894</td>\n",
       "      <td>IN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779233</th>\n",
       "      <td>42027</td>\n",
       "      <td>753895</td>\n",
       "      <td>JJ</td>\n",
       "      <td>contemporary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779234</th>\n",
       "      <td>42027</td>\n",
       "      <td>753896</td>\n",
       "      <td>JJ</td>\n",
       "      <td>medical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779235</th>\n",
       "      <td>42027</td>\n",
       "      <td>753897</td>\n",
       "      <td>NNS</td>\n",
       "      <td>journals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779236</th>\n",
       "      <td>42027</td>\n",
       "      <td>753898</td>\n",
       "      <td>CC</td>\n",
       "      <td>and</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779237</th>\n",
       "      <td>42027</td>\n",
       "      <td>753899</td>\n",
       "      <td>NNS</td>\n",
       "      <td>books</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779238</th>\n",
       "      <td>42027</td>\n",
       "      <td>753900</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779239</th>\n",
       "      <td>42027</td>\n",
       "      <td>753901</td>\n",
       "      <td>CC</td>\n",
       "      <td>and</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779240</th>\n",
       "      <td>42027</td>\n",
       "      <td>753902</td>\n",
       "      <td>NNS</td>\n",
       "      <td>references</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779241</th>\n",
       "      <td>42027</td>\n",
       "      <td>753903</td>\n",
       "      <td>IN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779242</th>\n",
       "      <td>42027</td>\n",
       "      <td>753904</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Church</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779243</th>\n",
       "      <td>42027</td>\n",
       "      <td>753905</td>\n",
       "      <td>NNS</td>\n",
       "      <td>reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779244</th>\n",
       "      <td>42027</td>\n",
       "      <td>753906</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779245</th>\n",
       "      <td>42028</td>\n",
       "      <td>753907</td>\n",
       "      <td>NN</td>\n",
       "      <td>F6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779246</th>\n",
       "      <td>42028</td>\n",
       "      <td>753908</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779247</th>\n",
       "      <td>42028</td>\n",
       "      <td>753909</td>\n",
       "      <td>VBN</td>\n",
       "      <td>Printed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779248</th>\n",
       "      <td>42028</td>\n",
       "      <td>753910</td>\n",
       "      <td>NN</td>\n",
       "      <td>material</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779249</th>\n",
       "      <td>42028</td>\n",
       "      <td>753911</td>\n",
       "      <td>IN</td>\n",
       "      <td>on</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779250</th>\n",
       "      <td>42028</td>\n",
       "      <td>753912</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779251</th>\n",
       "      <td>42028</td>\n",
       "      <td>753913</td>\n",
       "      <td>NN</td>\n",
       "      <td>history</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779252</th>\n",
       "      <td>42028</td>\n",
       "      <td>753914</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779253</th>\n",
       "      <td>42028</td>\n",
       "      <td>753915</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779254</th>\n",
       "      <td>42028</td>\n",
       "      <td>753916</td>\n",
       "      <td>JJ</td>\n",
       "      <td>medical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779255</th>\n",
       "      <td>42028</td>\n",
       "      <td>753917</td>\n",
       "      <td>NN</td>\n",
       "      <td>treatment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779256</th>\n",
       "      <td>42028</td>\n",
       "      <td>753918</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779257</th>\n",
       "      <td>42028</td>\n",
       "      <td>753919</td>\n",
       "      <td>NN</td>\n",
       "      <td>homosexuality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779258</th>\n",
       "      <td>42028</td>\n",
       "      <td>753920</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence_id  token_id  pos          token tag_cat_linguistic  \\\n",
       "779229        42027    753891   NN      treatment                NaN   \n",
       "779230        42027    753892   IN             of                NaN   \n",
       "779231        42027    753893   NN  homosexuality                NaN   \n",
       "779232        42027    753894   IN             in                NaN   \n",
       "779233        42027    753895   JJ   contemporary                NaN   \n",
       "779234        42027    753896   JJ        medical                NaN   \n",
       "779235        42027    753897  NNS       journals                NaN   \n",
       "779236        42027    753898   CC            and                NaN   \n",
       "779237        42027    753899  NNS          books                NaN   \n",
       "779238        42027    753900    ,              ,                NaN   \n",
       "779239        42027    753901   CC            and                NaN   \n",
       "779240        42027    753902  NNS     references                NaN   \n",
       "779241        42027    753903   IN             in                NaN   \n",
       "779242        42027    753904  NNP         Church                NaN   \n",
       "779243        42027    753905  NNS        reports                NaN   \n",
       "779244        42027    753906    .              .                NaN   \n",
       "779245        42028    753907   NN             F6                NaN   \n",
       "779246        42028    753908    :              :                NaN   \n",
       "779247        42028    753909  VBN        Printed                NaN   \n",
       "779248        42028    753910   NN       material                NaN   \n",
       "779249        42028    753911   IN             on                NaN   \n",
       "779250        42028    753912   DT            the                NaN   \n",
       "779251        42028    753913   NN        history                NaN   \n",
       "779252        42028    753914   IN             of                NaN   \n",
       "779253        42028    753915   DT            the                NaN   \n",
       "779254        42028    753916   JJ        medical                NaN   \n",
       "779255        42028    753917   NN      treatment                NaN   \n",
       "779256        42028    753918   IN             of                NaN   \n",
       "779257        42028    753919   NN  homosexuality                NaN   \n",
       "779258        42028    753920    .              .                NaN   \n",
       "\n",
       "       tag_cat_personname tag_cat_contextual  \n",
       "779229                NaN                NaN  \n",
       "779230                NaN                NaN  \n",
       "779231                NaN                NaN  \n",
       "779232                NaN                NaN  \n",
       "779233                NaN                NaN  \n",
       "779234                NaN                NaN  \n",
       "779235                NaN                NaN  \n",
       "779236                NaN                NaN  \n",
       "779237                NaN                NaN  \n",
       "779238                NaN                NaN  \n",
       "779239                NaN                NaN  \n",
       "779240                NaN                NaN  \n",
       "779241                NaN                NaN  \n",
       "779242                NaN                NaN  \n",
       "779243                NaN                NaN  \n",
       "779244                NaN                NaN  \n",
       "779245                NaN                NaN  \n",
       "779246                NaN                NaN  \n",
       "779247                NaN                NaN  \n",
       "779248                NaN                NaN  \n",
       "779249                NaN                NaN  \n",
       "779250                NaN                NaN  \n",
       "779251                NaN                NaN  \n",
       "779252                NaN                NaN  \n",
       "779253                NaN                NaN  \n",
       "779254                NaN                NaN  \n",
       "779255                NaN                NaN  \n",
       "779256                NaN                NaN  \n",
       "779257                NaN                NaN  \n",
       "779258                NaN                NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train.tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMEMBER:** check that model input data created on correct subset of files - no stereotypes about homosexuality \"offences\" or \"medical treatment\" of homosexuality??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the `tag_cat_` columns' `nan` values with `'O'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_cat_cols = [\"tag_cat_linguistic\", \"tag_cat_personname\", \"tag_cat_contextual\"]\n",
    "df_train[tag_cat_cols] = df_train[tag_cat_cols].fillna('O')\n",
    "df_dev[tag_cat_cols] = df_dev[tag_cat_cols].fillna('O')\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by sentence, so the token column becomes a list of tokens for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[154, 155, 156, 157, 158, 159, 160, 161, 162, ...</td>\n",
       "      <td>[IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...</td>\n",
       "      <td>[After, his, ordination, he, spent, three, yea...</td>\n",
       "      <td>[O, B-Linguistic, O, B-Linguistic, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[NN, :, NN]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[321, 322, 323, 324, 325, 326, 327, 328, 329, ...</td>\n",
       "      <td>[NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[498, 499, 500, 500, 501, 501, 502, 503, 503, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, NNP, VBD, NNP, NNP, NN...</td>\n",
       "      <td>[In, 1941, Tom, Tom, Allan, Allan, married, Ja...</td>\n",
       "      <td>[O, O, B-Person-Name, B-Contextual, I-Person-N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[649, 650, 651, 652, 653, 654, 655, 656, 657, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...</td>\n",
       "      <td>[In, 1955, Rev, Tom, Allan, accepted, a, call,...</td>\n",
       "      <td>[O, O, B-Person-Name, I-Person-Name, I-Person-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      token_id  \\\n",
       "sentence_id                                                      \n",
       "5            [154, 155, 156, 157, 158, 159, 160, 161, 162, ...   \n",
       "11                                             [308, 309, 310]   \n",
       "13           [321, 322, 323, 324, 325, 326, 327, 328, 329, ...   \n",
       "18           [498, 499, 500, 500, 501, 501, 502, 503, 503, ...   \n",
       "24           [649, 650, 651, 652, 653, 654, 655, 656, 657, ...   \n",
       "\n",
       "                                                           pos  \\\n",
       "sentence_id                                                      \n",
       "5            [IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...   \n",
       "11                                                 [NN, :, NN]   \n",
       "13           [NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...   \n",
       "18           [IN, CD, NNP, NNP, NNP, NNP, VBD, NNP, NNP, NN...   \n",
       "24           [IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...   \n",
       "\n",
       "                                                      sentence  \\\n",
       "sentence_id                                                      \n",
       "5            [After, his, ordination, he, spent, three, yea...   \n",
       "11                                        [Identifier, :, AA6]   \n",
       "13           [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "18           [In, 1941, Tom, Tom, Allan, Allan, married, Ja...   \n",
       "24           [In, 1955, Rev, Tom, Allan, accepted, a, call,...   \n",
       "\n",
       "                                                       tag_cat  \n",
       "sentence_id                                                     \n",
       "5            [O, B-Linguistic, O, B-Linguistic, O, O, O, O,...  \n",
       "11                                                   [O, O, O]  \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "18           [O, O, B-Person-Name, B-Contextual, I-Person-N...  \n",
       "24           [O, O, B-Person-Name, I-Person-Name, I-Person-...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_grouped = utils.implodeDataFrame(df_train, [\"sentence_id\"])\n",
    "df_dev_grouped = utils.implodeDataFrame(df_dev, [\"sentence_id\"])\n",
    "df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sentences so they all have the same lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', ':', 'Papers', 'of', 'The', 'The', 'Very', 'Very', 'Rev', 'Rev', 'Rev', 'Prof', 'Prof', 'James', 'James', 'Whyte', 'Whyte', '(', '1920-2005', ')']\n"
     ]
    }
   ],
   "source": [
    "# df_train_grouped = utils.addPaddedSentenceColumn(df_train_grouped)\n",
    "# print(df_train_grouped.sentence.values[0][:20])\n",
    "# df_dev_grouped = utils.addPaddedSentenceColumn(df_dev_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the POS and category tags together with the tokens so each sentence item is a tuple: `(TOKEN, POS-TAG, CATEGORY-TAG)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences = utils.zipFeaturesAndTarget(df_dev_grouped, \"tag_cat\")\n",
    "# print(dev_sentences[0])\n",
    "train_sentences = utils.zipFeaturesAndTarget(df_train_grouped, \"tag_cat\")\n",
    "# print(train_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "Get GloVe word embeddings (which were trained on English Wikipedia entries) for the vocabulary of the dataset (the unique tokens in the training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]\n",
    "d = dimensions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = utils.getGloveEmbeddings(d)\n",
    "# print(glove[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 35968\n",
      "Lowercased vocabulary size: 31335\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(df_train.token.unique())\n",
    "vocabulary_lowercased = [token.lower() for token in vocabulary]\n",
    "vocabulary_lowercased = list(set(vocabulary_lowercased))\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"Lowercased vocabulary size:\", len(vocabulary_lowercased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = utils.getEmbeddingsForTokens(glove, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(word_embeddings[0], glove[vocabulary[0].lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = dict(zip(vocabulary, word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict_keys = list(embedding_dict.keys())\n",
    "for token in vocabulary:\n",
    "    assert token in embedding_dict_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature dictionaries:\n",
    "\n",
    "*References:*\n",
    "* *https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html*\n",
    "* *https://stackoverflow.com/questions/58736548/how-to-use-word-embedding-as-features-for-crf-sklearn-crfsuite-model-training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vector representation of a token from a dictionray of word embeddings\n",
    "def extractEmbedding(token, embedding_dict=glove, dimensions=int(d)):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    try:\n",
    "        embedding = embedding_dict[token]\n",
    "    except KeyError:\n",
    "        embedding = np.zeros((dimensions,))\n",
    "    return embedding.reshape(-1,1)\n",
    "\n",
    "def extractTokenFeatures(sentence, i):\n",
    "    token = sentence[i][0]\n",
    "    pos = sentence[i][1]\n",
    "    features = {\n",
    "        'bias': 1.0,    # HOW IS THIS DECIDED? WHAT DOES THIS DO?\n",
    "        'pos': pos,\n",
    "        'pos[:2]': pos[:2],\n",
    "        'token': token\n",
    "    }\n",
    "    \n",
    "    # Add each value in a token's word embedding as a separate feature\n",
    "    embedding = extractEmbedding(token)\n",
    "    for i,n in enumerate(embedding):\n",
    "        features['e{}'.format(i)] = n\n",
    "    \n",
    "    # Record whether a token is the first or last token of a sentence\n",
    "    if i == 0:\n",
    "        features['START'] = True\n",
    "    elif i == (len(sentence) - 1):\n",
    "        features['END'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extractSentenceFeatures(sentence):\n",
    "    return [extractTokenFeatures(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "def extractSentenceTargets(sentence):\n",
    "    return [tag for token, pos, tag in sentence]\n",
    "\n",
    "def extractSentenceTokens(sentence):\n",
    "    return [token for token, pos, tag in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'pos': 'DT',\n",
       " 'pos[:2]': 'DT',\n",
       " 'token': 'The',\n",
       " 'e0': array([0.418], dtype=float32),\n",
       " 'e1': array([0.24968], dtype=float32),\n",
       " 'e2': array([-0.41242], dtype=float32),\n",
       " 'e3': array([0.1217], dtype=float32),\n",
       " 'e4': array([0.34527], dtype=float32),\n",
       " 'e5': array([-0.044457], dtype=float32),\n",
       " 'e6': array([-0.49688], dtype=float32),\n",
       " 'e7': array([-0.17862], dtype=float32),\n",
       " 'e8': array([-0.00066023], dtype=float32),\n",
       " 'e9': array([-0.6566], dtype=float32),\n",
       " 'e10': array([0.27843], dtype=float32),\n",
       " 'e11': array([-0.14767], dtype=float32),\n",
       " 'e12': array([-0.55677], dtype=float32),\n",
       " 'e13': array([0.14658], dtype=float32),\n",
       " 'e14': array([-0.0095095], dtype=float32),\n",
       " 'e15': array([0.011658], dtype=float32),\n",
       " 'e16': array([0.10204], dtype=float32),\n",
       " 'e17': array([-0.12792], dtype=float32),\n",
       " 'e18': array([-0.8443], dtype=float32),\n",
       " 'e19': array([-0.12181], dtype=float32),\n",
       " 'e20': array([-0.016801], dtype=float32),\n",
       " 'e21': array([-0.33279], dtype=float32),\n",
       " 'e22': array([-0.1552], dtype=float32),\n",
       " 'e23': array([-0.23131], dtype=float32),\n",
       " 'e24': array([-0.19181], dtype=float32),\n",
       " 'e25': array([-1.8823], dtype=float32),\n",
       " 'e26': array([-0.76746], dtype=float32),\n",
       " 'e27': array([0.099051], dtype=float32),\n",
       " 'e28': array([-0.42125], dtype=float32),\n",
       " 'e29': array([-0.19526], dtype=float32),\n",
       " 'e30': array([4.0071], dtype=float32),\n",
       " 'e31': array([-0.18594], dtype=float32),\n",
       " 'e32': array([-0.52287], dtype=float32),\n",
       " 'e33': array([-0.31681], dtype=float32),\n",
       " 'e34': array([0.00059213], dtype=float32),\n",
       " 'e35': array([0.0074449], dtype=float32),\n",
       " 'e36': array([0.17778], dtype=float32),\n",
       " 'e37': array([-0.15897], dtype=float32),\n",
       " 'e38': array([0.012041], dtype=float32),\n",
       " 'e39': array([-0.054223], dtype=float32),\n",
       " 'e40': array([-0.29871], dtype=float32),\n",
       " 'e41': array([-0.15749], dtype=float32),\n",
       " 'e42': array([-0.34758], dtype=float32),\n",
       " 'e43': array([-0.045637], dtype=float32),\n",
       " 'e44': array([-0.44251], dtype=float32),\n",
       " 'e45': array([0.18785], dtype=float32),\n",
       " 'e46': array([0.0027849], dtype=float32),\n",
       " 'e47': array([-0.18411], dtype=float32),\n",
       " 'e48': array([-0.11514], dtype=float32),\n",
       " 'e49': array([-0.78581], dtype=float32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractSentenceFeatures(train_sentences[0])[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [extractSentenceFeatures(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Baseline Model\n",
    "\n",
    "* **Features:** part-of-speech tag, first 2 letters of part-of-speech tag abbreviation, GloVe embeddings\n",
    "* **Target:** label category IOB tags\n",
    "* **Algorithm:** L2SGD\n",
    "\n",
    "### Train\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow']\n",
    "# Available algorithms with sklearn_crfsuite are:\n",
    "#     'lbfgs' - Gradient descent using the L-BFGS method\n",
    "#     'l2sgd' - Stochastic Gradient Descent with L2 regularization term\n",
    "#     'ap' - Averaged Perceptron\n",
    "#     'pa' - Passive Aggressive (PA)\n",
    "#     'arow' - Adaptive Regularization Of Weight Vector (AROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100) #iterations unlimited\n",
    "clf = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=0.1, max_iterations=100)     # up to 1000 iterations allowed\n",
    "# clf = sklearn_crfsuite.CRF(algorithm=algorithms[2], max_iterations=100)           # max iterations allowed\n",
    "# clf = sklearn_crfsuite.CRF(algorithm=algorithms[3], max_iterations=100)           # max iterations allowed\n",
    "# clf = sklearn_crfsuite.CRF(algorithm=algorithms[4], max_iterations=100)           # max iterations allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Person-Name', 'B-Contextual', 'I-Person-Name', 'I-Contextual', 'B-Linguistic', 'I-Linguistic']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "#### Strict Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.369708061511727"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # LBFGS - third-best\n",
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " B-Contextual    0.45215   0.20301   0.28021      1862\n",
      " I-Contextual    0.72059   0.03346   0.06395      2929\n",
      " B-Linguistic    0.70822   0.51477   0.59620      1523\n",
      " I-Linguistic    0.63636   0.02536   0.04878       276\n",
      "B-Person-Name    0.58921   0.40075   0.47704      2670\n",
      "I-Person-Name    0.67856   0.38080   0.48783      4396\n",
      "\n",
      "    micro avg    0.62937   0.29372   0.40052     13656\n",
      "    macro avg    0.63085   0.25969   0.32567     13656\n",
      " weighted avg    0.64169   0.29372   0.36971     13656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# L2SGD - tied for first (about the same as the fourth)\n",
    "targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " B-Contextual    0.13750   0.04726   0.07034      1862\n",
      " I-Contextual    0.19298   0.00376   0.00737      2929\n",
      " B-Linguistic    0.61205   0.54695   0.57767      1523\n",
      " I-Linguistic    0.36842   0.02536   0.04746       276\n",
      "B-Person-Name    0.44488   0.29775   0.35674      2670\n",
      "I-Person-Name    0.57406   0.25569   0.35379      4396\n",
      "\n",
      "    micro avg    0.49090   0.20929   0.29346     13656\n",
      "    macro avg    0.38831   0.19613   0.23556     13656\n",
      " weighted avg    0.40762   0.20929   0.26020     13656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # AP - second-best\n",
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " B-Contextual    0.24719   0.05908   0.09536      1862\n",
      " I-Contextual    0.36977   0.05428   0.09467      2929\n",
      " B-Linguistic    0.64818   0.53710   0.58743      1523\n",
      " I-Linguistic    0.38462   0.01812   0.03460       276\n",
      "B-Person-Name    0.47510   0.34307   0.39843      2670\n",
      "I-Person-Name    0.54005   0.29754   0.38369      4396\n",
      "\n",
      "    micro avg    0.51015   0.24282   0.32903     13656\n",
      "    macro avg    0.44415   0.21820   0.26570     13656\n",
      " weighted avg    0.45981   0.24282   0.30094     13656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # PA - tied for first (about the same as the second)\n",
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " B-Contextual    0.14218   0.11224   0.12545      1862\n",
      " I-Contextual    0.07963   0.09116   0.08500      2929\n",
      " B-Linguistic    0.46378   0.38674   0.42177      1523\n",
      " I-Linguistic    0.00444   0.00362   0.00399       276\n",
      "B-Person-Name    0.30427   0.40000   0.34563      2670\n",
      "I-Person-Name    0.27737   0.42357   0.33522      4396\n",
      "\n",
      "    micro avg    0.24158   0.29262   0.26466     13656\n",
      "    macro avg    0.21195   0.23622   0.21951     13656\n",
      " weighted avg    0.23706   0.29262   0.25795     13656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # AROW - worst-performing\n",
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect.\n",
    "\n",
    "As with the manual annotation evaluation, we want to evaluate the predictions more loosely, considering overlapping text spans in addition to exactly matching text spans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loose Evaluation\n",
    "\n",
    "Using the model with one of the best performing algorithms, Stochastic Gradient Descent with L2 regularization (`l2sgd`),  conduct a loose evaluation of the model's performance (as the manual annotation were evaluated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-Linguistic', 'B-Contextual', 'B-Linguistic', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'B-Linguistic', 'O', 'B-Linguistic', 'O', 'O', 'O', 'O', 'O', 'B-Contextual', 'I-Contextual', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[0])\n",
    "print(y_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'his', 'ordination', 'he', 'spent', 'three', 'years', 'as', 'an', 'army', 'Chaplain', 'and', 'then', 'in', '1948', 'was', 'inducted', 'to', 'Dunollie', 'Road', 'Church', 'in', 'Oban', '.']\n"
     ]
    }
   ],
   "source": [
    "dev_sent0 = X_dev[0]\n",
    "tokens = [features[\"token\"] for features in dev_sent0]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to improve the model's performance, use cross-validation and randomized search for choosing regularization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3  # 3-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2 = 0.1, max_iterations=100) #, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

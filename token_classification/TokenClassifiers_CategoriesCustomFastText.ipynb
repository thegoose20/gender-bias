{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Gender Biased Token Classifiers\n",
    "\n",
    "### Target: Label Categories\n",
    "\n",
    "### Word Embeddings: GloVe\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory ../data/token_clf_data/model_input/\n",
    "* Multilabel classification\n",
    "    * 3 categories of labels: Linguistic, Person Name, Contextual\n",
    "* Word Embeddings\n",
    "    * GloVe (trained on English Wikipedia dump)\n",
    "\n",
    "***\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "**[0.](#0) Preprocessing**\n",
    "\n",
    "**[1.](#1) Baseline Model**\n",
    "\n",
    "**[2.](#2) Hyperparameter Optimization**\n",
    "\n",
    "**[3.](#3) Error Analysis**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import scipy.stats\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "# For classification\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# For evaluation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, ConfusionMatrixDisplay#, plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from intervaltree import Interval, IntervalTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## 0. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and validation (dev) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicate rows with all but the same annotation ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463441, 9) (156146, 9)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop(columns=[\"ann_id\"])\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev.drop(columns=[\"ann_id\"])\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Non-binary labels as these were mistaken labels identified early on that were meant to be excluded, and because only one token has this label, it prevents the data from being input into the models with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.tag != \"B-Nonbinary\"]\n",
    "df_train = df_train.loc[df_train.tag != \"I-Nonbinary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463439, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Label Categories\n",
    "\n",
    "Add the annotation label categories as a column of higher-level Inside-Outside-Beginning (IOB) tags so they can be used as targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = utils.addCategoryTagColumn(df_train)\n",
    "# df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = utils.addCategoryTagColumn(df_dev)\n",
    "# df_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that won't be used as features for the classifiers and remove any duplicate rows that remain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"sentence_id\", \"token_id\", \"pos\", \"token\", \"tag_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[cols_to_keep]\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev[cols_to_keep]\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "# df_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by sentence, so the token column becomes a list of tokens for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[154, 155, 156, 157, 158, 159, 160, 161, 162, ...</td>\n",
       "      <td>[IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...</td>\n",
       "      <td>[After, his, ordination, he, spent, three, yea...</td>\n",
       "      <td>[O, B-Linguistic, O, B-Linguistic, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[NN, :, NN]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[321, 322, 323, 324, 325, 326, 327, 328, 329, ...</td>\n",
       "      <td>[NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[498, 499, 500, 500, 501, 501, 502, 503, 503, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, NNP, VBD, NNP, NNP, NN...</td>\n",
       "      <td>[In, 1941, Tom, Tom, Allan, Allan, married, Ja...</td>\n",
       "      <td>[O, O, B-Person-Name, B-Contextual, I-Person-N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[649, 650, 651, 652, 653, 654, 655, 656, 657, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...</td>\n",
       "      <td>[In, 1955, Rev, Tom, Allan, accepted, a, call,...</td>\n",
       "      <td>[O, O, B-Person-Name, I-Person-Name, I-Person-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      token_id  \\\n",
       "sentence_id                                                      \n",
       "5            [154, 155, 156, 157, 158, 159, 160, 161, 162, ...   \n",
       "11                                             [308, 309, 310]   \n",
       "13           [321, 322, 323, 324, 325, 326, 327, 328, 329, ...   \n",
       "18           [498, 499, 500, 500, 501, 501, 502, 503, 503, ...   \n",
       "24           [649, 650, 651, 652, 653, 654, 655, 656, 657, ...   \n",
       "\n",
       "                                                           pos  \\\n",
       "sentence_id                                                      \n",
       "5            [IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...   \n",
       "11                                                 [NN, :, NN]   \n",
       "13           [NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...   \n",
       "18           [IN, CD, NNP, NNP, NNP, NNP, VBD, NNP, NNP, NN...   \n",
       "24           [IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...   \n",
       "\n",
       "                                                      sentence  \\\n",
       "sentence_id                                                      \n",
       "5            [After, his, ordination, he, spent, three, yea...   \n",
       "11                                        [Identifier, :, AA6]   \n",
       "13           [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "18           [In, 1941, Tom, Tom, Allan, Allan, married, Ja...   \n",
       "24           [In, 1955, Rev, Tom, Allan, accepted, a, call,...   \n",
       "\n",
       "                                                       tag_cat  \n",
       "sentence_id                                                     \n",
       "5            [O, B-Linguistic, O, B-Linguistic, O, O, O, O,...  \n",
       "11                                                   [O, O, O]  \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "18           [O, O, B-Person-Name, B-Contextual, I-Person-N...  \n",
       "24           [O, O, B-Person-Name, I-Person-Name, I-Person-...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_grouped = utils.implodeDataFrame(df_train, [\"sentence_id\"])\n",
    "df_dev_grouped = utils.implodeDataFrame(df_dev, [\"sentence_id\"])\n",
    "df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sentences so they all have the same lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', ':', 'Papers', 'of', 'The', 'The', 'Very', 'Very', 'Rev', 'Rev', 'Rev', 'Prof', 'Prof', 'James', 'James', 'Whyte', 'Whyte', '(', '1920-2005', ')']\n"
     ]
    }
   ],
   "source": [
    "# df_train_grouped = utils.addPaddedSentenceColumn(df_train_grouped)\n",
    "# print(df_train_grouped.sentence.values[0][:20])\n",
    "# df_dev_grouped = utils.addPaddedSentenceColumn(df_dev_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the POS and category tags together with the tokens so each sentence item is a tuple: `(TOKEN, POS-TAG, CATEGORY-TAG)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences = utils.zipFeaturesAndTarget(df_dev_grouped, \"tag_cat\")\n",
    "# print(dev_sentences[0])\n",
    "train_sentences = utils.zipFeaturesAndTarget(df_train_grouped, \"tag_cat\")\n",
    "# print(train_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "Use the custom fastText word embeddings, trained on the entire dataset of descriptive metadata from the Archives (harvested in October 2020) using the Continuous Bag-of-Words (CBOW) algorithm.  Subword embeddings (for subwords from 2 to 6 characters long, inclusive) are used to infer the embeddings for out-of-vocabulary (OOV) words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the word embedding model trained on lowercased text to 100 dimensions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_tmpfile(config.tokc_path+\"fasttext100_lowercased.model\")\n",
    "embedding_model = FastText.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 35968\n",
      "Lowercased vocabulary size: 31335\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(df_train.token.unique())\n",
    "vocabulary_lowercased = [token.lower() for token in vocabulary]\n",
    "vocabulary_lowercased = list(set(vocabulary_lowercased))\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"Lowercased vocabulary size:\", len(vocabulary_lowercased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature dictionaries:\n",
    "\n",
    "*References:*\n",
    "* *https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html*\n",
    "* *https://stackoverflow.com/questions/58736548/how-to-use-word-embedding-as-features-for-crf-sklearn-crfsuite-model-training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vector representation of a token from a fastText word embedding model\n",
    "def extractEmbedding(token, fasttext_model=embedding_model):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    embedding = fasttext_model.wv[token]\n",
    "    return embedding\n",
    "\n",
    "def extractTokenFeatures(sentence, i):\n",
    "    token = sentence[i][0]\n",
    "    pos = sentence[i][1]\n",
    "    features = {\n",
    "        'bias': 1.0,    # HOW IS THIS DECIDED? WHAT DOES THIS DO?\n",
    "        'pos': pos,\n",
    "        'pos[:2]': pos[:2],\n",
    "        'token': token\n",
    "    }\n",
    "    \n",
    "    # Add each value in a token's word embedding as a separate feature\n",
    "    embedding = extractEmbedding(token)\n",
    "    for i,n in enumerate(embedding):\n",
    "        features['e{}'.format(i)] = n\n",
    "    \n",
    "    # Record whether a token is the first or last token of a sentence\n",
    "    if i == 0:\n",
    "        features['START'] = True\n",
    "    elif i == (len(sentence) - 1):\n",
    "        features['END'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extractSentenceFeatures(sentence):\n",
    "    return [extractTokenFeatures(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "def extractSentenceTargets(sentence):\n",
    "    return [tag for token, pos, tag in sentence]\n",
    "\n",
    "def extractSentenceTokens(sentence):\n",
    "    return [token for token, pos, tag in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractSentenceFeatures(train_sentences[0])[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [extractSentenceFeatures(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Baseline Model\n",
    "\n",
    "* **Features:** part-of-speech tag, first 2 letters of part-of-speech tag abbreviation\n",
    "* **Target:** label category IOB tags\n",
    "* **Algorithm:** L2SGD\n",
    "\n",
    "### Train\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow']\n",
    "# Available algorithms with sklearn_crfsuite are:\n",
    "#     'lbfgs' - Gradient descent using the L-BFGS method\n",
    "#     'l2sgd' - Stochastic Gradient Descent with L2 regularization term\n",
    "#     'ap' - Averaged Perceptron\n",
    "#     'pa' - Passive Aggressive (PA)\n",
    "#     'arow' - Adaptive Regularization Of Weight Vector (AROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100) #iterations unlimited\n",
    "clf = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=0.1, max_iterations=100)     # up to 1000 iterations allowed\n",
    "# clf = sklearn_crfsuite.CRF(algorithm=algorithms[2], max_iterations=100)           # max iterations allowed\n",
    "# clf = sklearn_crfsuite.CRF(algorithm=algorithms[3], max_iterations=100)           # max iterations allowed\n",
    "# clf = sklearn_crfsuite.CRF(algorithm=algorithms[4], max_iterations=100)           # max iterations allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Person-Name', 'B-Contextual', 'I-Person-Name', 'I-Contextual', 'B-Linguistic', 'I-Linguistic']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "#### Strict Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: B-Contextual\n",
      "  - F1: 0.9126541151660342\n",
      "  - Prec: 0.9097454884859033\n",
      "  - Rec 0.9309258721399296\n",
      "\n",
      "Label: I-Contextual\n",
      "  - F1: 0.9126541151660342\n",
      "  - Prec: 0.9097454884859033\n",
      "  - Rec 0.9309258721399296\n",
      "\n",
      "Label: B-Linguistic\n",
      "  - F1: 0.9126541151660342\n",
      "  - Prec: 0.9097454884859033\n",
      "  - Rec 0.9309258721399296\n",
      "\n",
      "Label: I-Linguistic\n",
      "  - F1: 0.9126541151660342\n",
      "  - Prec: 0.9097454884859033\n",
      "  - Rec 0.9309258721399296\n",
      "\n",
      "Label: B-Person-Name\n",
      "  - F1: 0.9126541151660342\n",
      "  - Prec: 0.9097454884859033\n",
      "  - Rec 0.9309258721399296\n",
      "\n",
      "Label: I-Person-Name\n",
      "  - F1: 0.9126541151660342\n",
      "  - Prec: 0.9097454884859033\n",
      "  - Rec 0.9309258721399296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "for target in targets_sorted:\n",
    "    print(\"Label:\", target)\n",
    "    print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", labels=target))\n",
    "    print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", labels=target))\n",
    "    print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", labels=target))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect.\n",
    "\n",
    "As with the manual annotation evaluation, we want to evaluate the predictions more loosely, considering overlapping text spans in addition to exactly matching text spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_cat_expected</th>\n",
       "      <th>tag_cat_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[154, 155, 156, 157, 158, 159, 160, 161, 162, ...</td>\n",
       "      <td>[IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...</td>\n",
       "      <td>[After, his, ordination, he, spent, three, yea...</td>\n",
       "      <td>[O, B-Linguistic, O, B-Linguistic, O, O, O, O,...</td>\n",
       "      <td>[O, B-Linguistic, O, B-Linguistic, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[NN, :, NN]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[321, 322, 323, 324, 325, 326, 327, 328, 329, ...</td>\n",
       "      <td>[NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[498, 499, 500, 500, 501, 501, 502, 503, 503, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, NNP, VBD, NNP, NNP, NN...</td>\n",
       "      <td>[In, 1941, Tom, Tom, Allan, Allan, married, Ja...</td>\n",
       "      <td>[O, O, B-Person-Name, B-Contextual, I-Person-N...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[649, 650, 651, 652, 653, 654, 655, 656, 657, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...</td>\n",
       "      <td>[In, 1955, Rev, Tom, Allan, accepted, a, call,...</td>\n",
       "      <td>[O, O, B-Person-Name, I-Person-Name, I-Person-...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      token_id  \\\n",
       "sentence_id                                                      \n",
       "5            [154, 155, 156, 157, 158, 159, 160, 161, 162, ...   \n",
       "11                                             [308, 309, 310]   \n",
       "13           [321, 322, 323, 324, 325, 326, 327, 328, 329, ...   \n",
       "18           [498, 499, 500, 500, 501, 501, 502, 503, 503, ...   \n",
       "24           [649, 650, 651, 652, 653, 654, 655, 656, 657, ...   \n",
       "\n",
       "                                                           pos  \\\n",
       "sentence_id                                                      \n",
       "5            [IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...   \n",
       "11                                                 [NN, :, NN]   \n",
       "13           [NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...   \n",
       "18           [IN, CD, NNP, NNP, NNP, NNP, VBD, NNP, NNP, NN...   \n",
       "24           [IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...   \n",
       "\n",
       "                                                      sentence  \\\n",
       "sentence_id                                                      \n",
       "5            [After, his, ordination, he, spent, three, yea...   \n",
       "11                                        [Identifier, :, AA6]   \n",
       "13           [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "18           [In, 1941, Tom, Tom, Allan, Allan, married, Ja...   \n",
       "24           [In, 1955, Rev, Tom, Allan, accepted, a, call,...   \n",
       "\n",
       "                                              tag_cat_expected  \\\n",
       "sentence_id                                                      \n",
       "5            [O, B-Linguistic, O, B-Linguistic, O, O, O, O,...   \n",
       "11                                                   [O, O, O]   \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "18           [O, O, B-Person-Name, B-Contextual, I-Person-N...   \n",
       "24           [O, O, B-Person-Name, I-Person-Name, I-Person-...   \n",
       "\n",
       "                                             tag_cat_predicted  \n",
       "sentence_id                                                     \n",
       "5            [O, B-Linguistic, O, B-Linguistic, O, O, O, O,...  \n",
       "11                                                   [O, O, O]  \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "18           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "24           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev_sentences[0]\n",
    "df_dev_grouped = df_dev_grouped.rename(columns={\"tag_cat\":\"tag_cat_expected\"})\n",
    "df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_cat_predicted\", y_pred)\n",
    "df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_cat_expected</th>\n",
       "      <th>tag_cat_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>B-Linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>B-Linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token_id   pos       token tag_cat_expected tag_cat_predicted\n",
       "sentence_id                                                              \n",
       "5                154    IN       After                O                 O\n",
       "5                155  PRP$         his     B-Linguistic      B-Linguistic\n",
       "5                156    NN  ordination                O                 O\n",
       "5                157   PRP          he     B-Linguistic      B-Linguistic\n",
       "5                158   VBD       spent                O                 O"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns))\n",
    "df_dev_exploded = df_dev_exploded.rename(columns={\"sentence\":\"token\"})\n",
    "df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154935, 5)\n",
      "(154935, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df_dev_exploded.shape)\n",
    "print(df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_grouped.to_csv(config.tokc_path+\"model_output/categoryTags_crfL2sgd_POS-CustomFastText_grouped.csv\")\n",
    "df_dev_exploded.to_csv(config.tokc_path+\"model_output/categoryTags_crfL2sgd_POS-CustomFastText.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loose Evaluation\n",
    "\n",
    "Using the model with one of the best performing algorithms, Stochastic Gradient Descent with L2 regularization (`l2sgd`),  conduct a loose evaluation of the model's performance (as the manual annotation were evaluated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to improve the model's performance, use cross-validation and randomized search for choosing regularization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3  # 3-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2 = 0.1, max_iterations=100) #, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (gender-bias)",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

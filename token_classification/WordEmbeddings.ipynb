{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Word Embeddings for Token Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = list(df_train.token)\n",
    "lemmas_train = [lmtzr.lemmatize(token) for token in tokens_train]\n",
    "tokens_dev = list(df_dev.token)\n",
    "lemmas_dev = [lmtzr.lemmatize(token) for token in tokens_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.insert((list(df_train.columns).index(\"token\")+1), \"lemma\", lemmas_train)\n",
    "df_dev.insert((list(df_dev.columns).index(\"token\")+1), \"lemma\", lemmas_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the vocabulary of the annotated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_dev])  # df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32957 31157 26757\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = list(set(list(df.token)))\n",
    "unique_lemmas = list(set(list(df.lemma))) \n",
    "unique_lemmas = [lemma for lemma in unique_lemmas if lemma.isalpha()]\n",
    "lemmas_lower = [lemma.lower() for lemma in unique_lemmas]\n",
    "unique_lemmas_lower = list(set(lemmas_lower))\n",
    "unique_words = [token for token in unique_tokens if token.isalpha()]  # keep tokens with only alphabetic characters\n",
    "print(len(unique_words), len(unique_lemmas), len(unique_lemmas_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Contextual Word Embeddings: sense2vec\n",
    "\n",
    "Load [spaCy's contextual word embeddings](https://github.com/explosion/sense2vec), which were trained on 2015 Reddit posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sense2vec.component.Sense2VecComponent at 0x7f87677c5d30>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "s2v = nlp.add_pipe(\"sense2vec\")\n",
    "s2v.from_disk(config.s2v_reddit_path)\n",
    "\n",
    "#-------------\n",
    "# doc = nlp(\"A sentence about natural language processing.\")\n",
    "# assert doc[3:6].text == \"natural language processing\"\n",
    "# freq = doc[3:6]._.s2v_freq\n",
    "# vector = doc[3:6]._.s2v_vec\n",
    "# most_similar = doc[3:6]._.s2v_most_similar(3)\n",
    "# # [(('machine learning', 'NOUN'), 0.8986967),\n",
    "# #  (('computer vision', 'NOUN'), 0.8636297),\n",
    "# #  (('deep learning', 'NOUN'), 0.8573361)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cliffs', 'rpm', 'Heal', 'hereditary', 'Medjid', 'presumedly', 'Mondays', 'Routine', 'recipientESPMedawar', 'Kirkuk', 'Seton', 'Venado', 'Edith', 'Mackay', 'Visiting', 'interwar', 'atherogenic', 'Cawdor', 'jockey', 'Burgesses']\n"
     ]
    }
   ],
   "source": [
    "print(unique_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary not in Sense2Vec: 11529\n",
      "Proportion of vocabulary not in Sense2Vec: 0.40388859695218077\n"
     ]
    }
   ],
   "source": [
    "not_in_s2v = []\n",
    "for word in unique_words:\n",
    "    lowercased = word.lower()\n",
    "    w = (nlp(lowercased))[0]\n",
    "    if w._.s2v_vec is None:\n",
    "        not_in_s2v += [word]\n",
    "\n",
    "print(\"Total words in vocabulary not in Sense2Vec:\", len(not_in_s2v))\n",
    "print(\"Proportion of vocabulary not in Sense2Vec:\",(len(not_in_s2v))/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recipient', 'squabs', 'Darby', 'poulterer', 'Scotsman', 'sepolero', 'Morphogenetic', 'Hynes', 'Repleta', 'Simal', 'Mai', 'Arithmetic', 'Duce', 'Mme', 'lectureKatchalsky', 'inA', 'Tulsk', 'Berg', 'Mode', 'Bohme', 'Envelope', 'furnitureKoestler', 'Burmester', 'Evang', 'Ilona', 'compagne', 'BarnArthur', 'Cant', 'GollyArthur', 'Hatano', 'ElectionsAccompanied', 'accomodement', 'Finney', 'poetarum', 'Realites', 'Margaropus', 'LI', 'Lennox', 'Basberg', 'Ignacio', 'Glennie', 'Duffus', 'Takagi', 'Aliza', 'emir', 'Hersham', 'Rossini', 'Bald', 'Magnus', 'Pattison', 'Skefhill', 'Sorrel', 'Landolphin', 'Staub', 'ME', 'Alumbadi', 'Model', 'Majesties', 'Harian', 'trichocysts', 'Wandervogel', 'Rumped', 'Waetjen', 'Verbena', 'ofThe', 'sturzte', 'Lee', 'Copernicus', 'Verasis', 'Altenberg', 'unnumbered', 'environs', 'Neutral', 'Wynne', 'Mossman', 'ReneeESPCutten', 'ReadingSent', 'sequelae', 'Calary', 'Soviets', 'AustriaKoestler', 'Karlsburg', 'Peckham', 'Macdougall', 'Goldschmidt', 'electroencephalogram', 'neckHewitt', 'Simulans', 'Rothwell', 'prayeel', 'prickly', 'Dusen', 'Montagu', 'Prince', 'JournalSent', 'Microsatellite', 'Caius', 'Bubalus', 'Tyttenhanger', 'Conurbations']\n"
     ]
    }
   ],
   "source": [
    "# print(not_in_s2v[:100])\n",
    "# print(not_in_s2v[1000:1100])\n",
    "print(not_in_s2v[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2659\n"
     ]
    }
   ],
   "source": [
    "# x = \"lettersBaillie\"\n",
    "# y = \"writer'\"\n",
    "# z = \"MunsterSent\"\n",
    "newly_found = 0\n",
    "for word in not_in_s2v:\n",
    "    found = re.findall(\"[A-Z]{0,1}[a-z]+\", word)\n",
    "    for f in found:\n",
    "        lowercased = f.lower()\n",
    "        w = nlp(lowercased)[0]\n",
    "        if not w._.s2v_vec is None:\n",
    "            newly_found += 1\n",
    "print(newly_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion not found but possible to find: 0.23063578801283718\n",
      "Proportion of vocabulary not possible to find in Sense2Vec: 0.3107374321247154\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion not found but possible to find:\", newly_found/(len(not_in_s2v)))\n",
    "print(\"Proportion of vocabulary not possible to find in Sense2Vec:\",(len(not_in_s2v)-newly_found)/len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure it's worth reworking the tokenization and part-of-speech tagging to increase Sense2Vec's coverage of the vocabulary by only about 9%, so we'll keep going with the model input data as it is for now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [GloVe word embeddings](https://github.com/stanfordnlp/GloVe), which were trained on 2014 English Wikipedia entries and Gigaword 5:\n",
    "\n",
    "*Note: could also try [GN-GloVe](https://github.com/uclanlp/gn_glove), which supposedly has gender-neutral word embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\n",
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]  # pretrained GloVe embeddings come as vectors with one of these four dimensions\n",
    "d = dimensions[0]  # start small to begin with\n",
    "glove_path = config.inf_data_path+\"glove.6B/glove.6B.{}d.txt\".format(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13412    1.5041     0.39706   -0.41357    0.71336    0.63438\n",
      "  0.1214     0.16746    1.4104     0.013127   0.68386    0.85236\n",
      "  0.92599    0.098158   0.425     -0.83799    0.08482    0.22135\n",
      " -0.15247    0.72654    0.052728  -1.0064    -0.032626  -0.63342\n",
      "  0.0039789 -1.5288    -0.74005   -1.2051    -1.0227    -0.10588\n",
      "  1.2225     0.14845   -0.1641    -0.52887   -0.29012    0.59774\n",
      "  0.62847    0.49003   -0.14227   -1.2193     0.56094   -0.17673\n",
      " -0.11216   -0.41801   -0.40841   -0.41748   -0.40276    0.25091\n",
      " -0.43016    0.26412  ]\n"
     ]
    }
   ],
   "source": [
    "glove = dict()\n",
    "with open(glove_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        glove[word] = vector\n",
    "print(glove[\"recipient\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate GloVe's coverage of our vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary not in GloVe: 6453\n",
      "Proportion of vocabulary not in GloVe: 0.1958005886458112\n"
     ]
    }
   ],
   "source": [
    "not_in_glove = []\n",
    "in_glove = dict()\n",
    "for word in unique_words:\n",
    "    lowercased = word.lower()\n",
    "    try:\n",
    "        vector = glove[lowercased]\n",
    "        in_glove[word] = vector\n",
    "    except KeyError:\n",
    "        not_in_glove += [word]\n",
    "\n",
    "print(\"Total words in vocabulary not in GloVe:\", len(not_in_glove))\n",
    "print(\"Proportion of vocabulary not in GloVe:\",(len(not_in_glove))/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not possible to find in GloVe: 4187\n",
      "Proportion not found but possible to find in GloVe: 0.7311328064466139\n",
      "Proportion of vocabulary not possible to find in GloVe: 0.12704433049124617\n"
     ]
    }
   ],
   "source": [
    "still_not_found = []\n",
    "newly_found = 0\n",
    "# partial_glove_match = dict()\n",
    "for word in not_in_glove:\n",
    "    found = re.findall(\"[A-Z]{0,1}[a-z]+\", word)\n",
    "    for f in found:\n",
    "        lowercased = f.lower()\n",
    "        try:\n",
    "            vector = glove[lowercased]\n",
    "            in_glove[word] = vector  # partial_glove_match[word] = vector\n",
    "            newly_found += 1\n",
    "        except KeyError:\n",
    "            still_not_found += [word]\n",
    "print(\"Not possible to find in GloVe:\", len(still_not_found))\n",
    "print(\"Proportion not found but possible to find in GloVe:\", newly_found/(len(not_in_glove)))\n",
    "print(\"Proportion of vocabulary not possible to find in GloVe:\",(len(still_not_found))/len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe has much better coverage than sense2vec, as expected due to the better domain match (Wikipedia entries are more similar to archival metadata descriptions than Reddit comments!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_vectors = in_glove.copy()\n",
    "key_array = np.array(words_to_vectors.keys())\n",
    "for word in unique_words:\n",
    "    if word not in key_array:\n",
    "        if word.lower() in key_array:\n",
    "            vector = words_to_vectors[word.lower()]\n",
    "        else:\n",
    "            vector = np.array([])\n",
    "        words_to_vectors[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(words_to_vectors) == len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset associating each token to a GloVe word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_col_name = \"glove_embedding\"\n",
    "embedding_dict = glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>glove_embedding</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>Title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>Papers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_id                                    glove_embedding   token\n",
       "3         3  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...   Title\n",
       "4         4                                                 []       :\n",
       "5         5  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...  Papers\n",
       "6         6  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...      of\n",
       "7         7  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...     The"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings = utils.createEmbeddingDataFrame(df_train, embedding_dict, embedding_col_name)\n",
    "train_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>glove_embedding</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>154</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>After</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>155</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>156</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>ordination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>157</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>158</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>spent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token_id                                    glove_embedding       token\n",
       "172       154  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...       After\n",
       "173       155  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...         his\n",
       "174       156  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...  ordination\n",
       "175       157  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...          he\n",
       "176       158  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...       spent"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_embeddings = utils.createEmbeddingDataFrame(df_dev, embedding_dict, embedding_col_name)\n",
    "dev_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings.to_csv(config.tokc_path+\"glove_embeddings_train.csv\")\n",
    "# dev_embeddings.to_csv(config.tokc_path+\"glove_embeddings_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

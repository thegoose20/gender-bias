{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Word Embeddings for Token Classifiers\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[0.](#0) Preprocessing\n",
    "\n",
    "[1.](#1) SpaCy's sense2cec\n",
    "\n",
    "[2.](#2) GloVe\n",
    "\n",
    "[3.](#3) Custom with fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In both virtual envs\n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "\n",
    "## In gender-bias virtual env only\n",
    "# import utils\n",
    "# import spacy\n",
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## In fasttext virtual env only\n",
    "from gensim.models import FastText\n",
    "from gensim.utils import tokenize\n",
    "from gensim import utils\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## 0. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_train = list(df_train.token)\n",
    "# lemmas_train = [lmtzr.lemmatize(token) for token in tokens_train]\n",
    "# tokens_dev = list(df_dev.token)\n",
    "# lemmas_dev = [lmtzr.lemmatize(token) for token in tokens_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.insert((list(df_train.columns).index(\"token\")+1), \"lemma\", lemmas_train)\n",
    "# df_dev.insert((list(df_dev.columns).index(\"token\")+1), \"lemma\", lemmas_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the vocabulary of the annotated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_dev])  # df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32957 42272\n",
      "27687 37002\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = list(set(list(df.token)))\n",
    "# unique_lemmas = list(set(list(df.lemma))) \n",
    "# unique_lemmas = [lemma for lemma in unique_lemmas if lemma.isalpha()]\n",
    "# lemmas_lower = [lemma.lower() for lemma in unique_lemmas]\n",
    "# unique_lemmas_lower = list(set(lemmas_lower))\n",
    "unique_words = [token for token in unique_tokens if token.isalpha()]  # keep tokens with only alphabetic characters\n",
    "print(len(unique_words), len(unique_tokens)) #, len(unique_lemmas), len(unique_lemmas_lower))\n",
    "\n",
    "unique_tokens_lower = [token.lower() if token.isalpha() else token for token in unique_tokens]\n",
    "unique_tokens_lower = list(set(unique_tokens_lower))\n",
    "unique_words_lower = [token.lower() for token in unique_words]\n",
    "unique_words_lower = list(set(unique_words_lower))\n",
    "print(len(unique_words_lower), len(unique_tokens_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. SpaCy's sense2vec\n",
    "\n",
    "Load [spaCy's contextual word embeddings](https://github.com/explosion/sense2vec), which were trained on 2015 Reddit posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sense2vec.component.Sense2VecComponent at 0x7f87677c5d30>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "s2v = nlp.add_pipe(\"sense2vec\")\n",
    "s2v.from_disk(config.s2v_reddit_path)\n",
    "\n",
    "#-------------\n",
    "# doc = nlp(\"A sentence about natural language processing.\")\n",
    "# assert doc[3:6].text == \"natural language processing\"\n",
    "# freq = doc[3:6]._.s2v_freq\n",
    "# vector = doc[3:6]._.s2v_vec\n",
    "# most_similar = doc[3:6]._.s2v_most_similar(3)\n",
    "# # [(('machine learning', 'NOUN'), 0.8986967),\n",
    "# #  (('computer vision', 'NOUN'), 0.8636297),\n",
    "# #  (('deep learning', 'NOUN'), 0.8573361)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cliffs', 'rpm', 'Heal', 'hereditary', 'Medjid', 'presumedly', 'Mondays', 'Routine', 'recipientESPMedawar', 'Kirkuk', 'Seton', 'Venado', 'Edith', 'Mackay', 'Visiting', 'interwar', 'atherogenic', 'Cawdor', 'jockey', 'Burgesses']\n"
     ]
    }
   ],
   "source": [
    "print(unique_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary not in Sense2Vec: 11529\n",
      "Proportion of vocabulary not in Sense2Vec: 0.40388859695218077\n"
     ]
    }
   ],
   "source": [
    "not_in_s2v = []\n",
    "for word in unique_words:\n",
    "    lowercased = word.lower()\n",
    "    w = (nlp(lowercased))[0]\n",
    "    if w._.s2v_vec is None:\n",
    "        not_in_s2v += [word]\n",
    "\n",
    "print(\"Total words in vocabulary not in Sense2Vec:\", len(not_in_s2v))\n",
    "print(\"Proportion of vocabulary not in Sense2Vec:\",(len(not_in_s2v))/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recipient', 'squabs', 'Darby', 'poulterer', 'Scotsman', 'sepolero', 'Morphogenetic', 'Hynes', 'Repleta', 'Simal', 'Mai', 'Arithmetic', 'Duce', 'Mme', 'lectureKatchalsky', 'inA', 'Tulsk', 'Berg', 'Mode', 'Bohme', 'Envelope', 'furnitureKoestler', 'Burmester', 'Evang', 'Ilona', 'compagne', 'BarnArthur', 'Cant', 'GollyArthur', 'Hatano', 'ElectionsAccompanied', 'accomodement', 'Finney', 'poetarum', 'Realites', 'Margaropus', 'LI', 'Lennox', 'Basberg', 'Ignacio', 'Glennie', 'Duffus', 'Takagi', 'Aliza', 'emir', 'Hersham', 'Rossini', 'Bald', 'Magnus', 'Pattison', 'Skefhill', 'Sorrel', 'Landolphin', 'Staub', 'ME', 'Alumbadi', 'Model', 'Majesties', 'Harian', 'trichocysts', 'Wandervogel', 'Rumped', 'Waetjen', 'Verbena', 'ofThe', 'sturzte', 'Lee', 'Copernicus', 'Verasis', 'Altenberg', 'unnumbered', 'environs', 'Neutral', 'Wynne', 'Mossman', 'ReneeESPCutten', 'ReadingSent', 'sequelae', 'Calary', 'Soviets', 'AustriaKoestler', 'Karlsburg', 'Peckham', 'Macdougall', 'Goldschmidt', 'electroencephalogram', 'neckHewitt', 'Simulans', 'Rothwell', 'prayeel', 'prickly', 'Dusen', 'Montagu', 'Prince', 'JournalSent', 'Microsatellite', 'Caius', 'Bubalus', 'Tyttenhanger', 'Conurbations']\n"
     ]
    }
   ],
   "source": [
    "# print(not_in_s2v[:100])\n",
    "# print(not_in_s2v[1000:1100])\n",
    "print(not_in_s2v[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2659\n"
     ]
    }
   ],
   "source": [
    "# x = \"lettersBaillie\"\n",
    "# y = \"writer'\"\n",
    "# z = \"MunsterSent\"\n",
    "newly_found = 0\n",
    "for word in not_in_s2v:\n",
    "    found = re.findall(\"[A-Z]{0,1}[a-z]+\", word)\n",
    "    for f in found:\n",
    "        lowercased = f.lower()\n",
    "        w = nlp(lowercased)[0]\n",
    "        if not w._.s2v_vec is None:\n",
    "            newly_found += 1\n",
    "print(newly_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion not found but possible to find: 0.23063578801283718\n",
      "Proportion of vocabulary not possible to find in Sense2Vec: 0.3107374321247154\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion not found but possible to find:\", newly_found/(len(not_in_s2v)))\n",
    "print(\"Proportion of vocabulary not possible to find in Sense2Vec:\",(len(not_in_s2v)-newly_found)/len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure it's worth reworking the tokenization and part-of-speech tagging to increase Sense2Vec's coverage of the vocabulary by only about 9%, so we'll keep going with the model input data as it is for now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [GloVe word embeddings](https://github.com/stanfordnlp/GloVe), which were trained on 2014 English Wikipedia entries and Gigaword 5:\n",
    "\n",
    "*Note: could also try [GN-GloVe](https://github.com/uclanlp/gn_glove), which supposedly has gender-neutral word embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\n",
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]  # pretrained GloVe embeddings come as vectors with one of these four dimensions\n",
    "d = dimensions[0]  # start small to begin with\n",
    "glove_path = config.inf_data_path+\"glove.6B/glove.6B.{}d.txt\".format(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13412    1.5041     0.39706   -0.41357    0.71336    0.63438\n",
      "  0.1214     0.16746    1.4104     0.013127   0.68386    0.85236\n",
      "  0.92599    0.098158   0.425     -0.83799    0.08482    0.22135\n",
      " -0.15247    0.72654    0.052728  -1.0064    -0.032626  -0.63342\n",
      "  0.0039789 -1.5288    -0.74005   -1.2051    -1.0227    -0.10588\n",
      "  1.2225     0.14845   -0.1641    -0.52887   -0.29012    0.59774\n",
      "  0.62847    0.49003   -0.14227   -1.2193     0.56094   -0.17673\n",
      " -0.11216   -0.41801   -0.40841   -0.41748   -0.40276    0.25091\n",
      " -0.43016    0.26412  ]\n"
     ]
    }
   ],
   "source": [
    "glove = dict()\n",
    "with open(glove_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        glove[word] = vector\n",
    "print(glove[\"recipient\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate GloVe's coverage of our vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary not in GloVe: 6453\n",
      "Proportion of vocabulary not in GloVe: 0.1958005886458112\n"
     ]
    }
   ],
   "source": [
    "not_in_glove = []\n",
    "in_glove = dict()\n",
    "for word in unique_words:\n",
    "    lowercased = word.lower()\n",
    "    try:\n",
    "        vector = glove[lowercased]\n",
    "        in_glove[word] = vector\n",
    "    except KeyError:\n",
    "        not_in_glove += [word]\n",
    "\n",
    "print(\"Total words in vocabulary not in GloVe:\", len(not_in_glove))\n",
    "print(\"Proportion of vocabulary not in GloVe:\",(len(not_in_glove))/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not possible to find in GloVe: 4187\n",
      "Proportion not found but possible to find in GloVe: 0.7311328064466139\n",
      "Proportion of vocabulary not possible to find in GloVe: 0.12704433049124617\n"
     ]
    }
   ],
   "source": [
    "still_not_found = []\n",
    "newly_found = 0\n",
    "# partial_glove_match = dict()\n",
    "for word in not_in_glove:\n",
    "    found = re.findall(\"[A-Z]{0,1}[a-z]+\", word)\n",
    "    for f in found:\n",
    "        lowercased = f.lower()\n",
    "        try:\n",
    "            vector = glove[lowercased]\n",
    "            in_glove[word] = vector  # partial_glove_match[word] = vector\n",
    "            newly_found += 1\n",
    "        except KeyError:\n",
    "            still_not_found += [word]\n",
    "print(\"Not possible to find in GloVe:\", len(still_not_found))\n",
    "print(\"Proportion not found but possible to find in GloVe:\", newly_found/(len(not_in_glove)))\n",
    "print(\"Proportion of vocabulary not possible to find in GloVe:\",(len(still_not_found))/len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe has much better coverage than sense2vec, as expected due to the better domain match (Wikipedia entries are more similar to archival metadata descriptions than Reddit comments!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_vectors = in_glove.copy()\n",
    "key_array = np.array(words_to_vectors.keys())\n",
    "for word in unique_words:\n",
    "    if word not in key_array:\n",
    "        if word.lower() in key_array:\n",
    "            vector = words_to_vectors[word.lower()]\n",
    "        else:\n",
    "            vector = np.array([])\n",
    "        words_to_vectors[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(words_to_vectors) == len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset associating each token to a GloVe word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_col_name = \"glove_embedding\"\n",
    "embedding_dict = glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>glove_embedding</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>Title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>Papers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_id                                    glove_embedding   token\n",
       "3         3  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...   Title\n",
       "4         4                                                 []       :\n",
       "5         5  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...  Papers\n",
       "6         6  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...      of\n",
       "7         7  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...     The"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings = utils.createEmbeddingDataFrame(df_train, embedding_dict, embedding_col_name)\n",
    "train_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>glove_embedding</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>154</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>After</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>155</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>156</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>ordination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>157</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>158</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>spent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token_id                                    glove_embedding       token\n",
       "172       154  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...       After\n",
       "173       155  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...         his\n",
       "174       156  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...  ordination\n",
       "175       157  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...          he\n",
       "176       158  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...       spent"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_embeddings = utils.createEmbeddingDataFrame(df_dev, embedding_dict, embedding_col_name)\n",
    "dev_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings.to_csv(config.tokc_path+\"glove_embeddings_train.csv\")\n",
    "# dev_embeddings.to_csv(config.tokc_path+\"glove_embeddings_dev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Custom with fastText\n",
    "\n",
    "Train custom word embeddings on my own data (metadata descriptions from the CRC's Archives catalog) using fastText.\n",
    "\n",
    "* Data file: `data/descriptions_by_fonds`\n",
    "* Date of harvesting: October 2020\n",
    "* Harvesting and transformation code: [annot-prep/PreparationForAnnotation.ipynb](https://github.com/thegoose20/annot-prep/blob/main/PreparationForAnnotation.ipynb)\n",
    "\n",
    "*References:* \n",
    "* *https://radimrehurek.com/gensim/models/fasttext.html*\n",
    "* *https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1079\n"
     ]
    }
   ],
   "source": [
    "dir_path = config.inf_data_path+\"descriptions_by_fonds/\"\n",
    "file_list = os.listdir(dir_path)\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusIterator:\n",
    "    def __iter__(self):\n",
    "        file_list = os.listdir(dir_path)\n",
    "        for fonds_f in file_list:\n",
    "            assert \".txt\" in fonds_f, \"All files should be Plaintext.\" \n",
    "            file_path = dir_path+fonds_f\n",
    "            with utils.open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    yield list(tokenize(line.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters for the unsupervised training of the fastText model (essentially a word2vec model that uses using character n-grams so subwords can help to assign embeddings to unseen words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training architecture (default = \"cbow\" for Continuous Bag of Words)\n",
    "models = [\"cbow\", \"skipgram\"]\n",
    "model = models[0]\n",
    "# Specify the learning rate (default = 0.025)\n",
    "alpha = 0.025\n",
    "# Specify the training objective (default = \"ns\")\n",
    "# losses = [\"ns\", \"hs\", \"softmax\"]\n",
    "# loss = losses[0]\n",
    "# Specify the number of negative words to sample for 'ns' training objective (default = 5)\n",
    "negative = 5\n",
    "# Specify the threshold for downsampling higher-frequency words (default = 0.001)\n",
    "sample = 0.001\n",
    "# Specify the word embeddings' dimensions\n",
    "vector_dimensions = 100\n",
    "# Specify the context window (default is 5) \n",
    "context_window = 5\n",
    "# Specify the number of epochs (default is 5)\n",
    "epochs = 5\n",
    "# Specify the threshold of word occurrences (ignore words that occur less than specified number of times; default = 5)\n",
    "min_count = 5\n",
    "# Specify the minimum and maximum length of character ngrams (defaults are 3 and 6)\n",
    "min_n = 2\n",
    "max_n = 6  # if 0, no character n-grams (subword vectors) will be used\n",
    "# Specify the number of buckets for hashing ngrams (default = 2000000) \n",
    "bucket = 2000000\n",
    "# Sort vocabulary by descending frequency (default = 1)\n",
    "sorted_vocab = 1\n",
    "# Specify the number of threads to use (default = 12)\n",
    "# threads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\n",
    "    alpha=alpha, negative=negative, sample=sample,\n",
    "    vector_size=vector_dimensions, window=context_window, \n",
    "    epochs=epochs, min_count=min_count, min_n=min_n, \n",
    "    max_n=max_n, bucket=bucket, sorted_vocab=sorted_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(corpus_iterable=CorpusIterator())\n",
    "total_examples = model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7321074, 10119275)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On not lowercased text\n",
    "model.train(corpus_iterable=CorpusIterator(), total_examples=total_examples, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7322411, 10119275)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On lowercased text\n",
    "model.train(corpus_iterable=CorpusIterator(), total_examples=total_examples, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.56952626,  0.7291794 ,  0.6758853 ,  0.8381137 , -2.0523994 ,\n",
       "       -2.647903  , -0.49456945,  0.490795  , -0.62281513, -0.62517834,\n",
       "       -0.36668217,  4.0207033 , -0.13548039, -2.505383  , -2.4192085 ,\n",
       "       -0.16992553,  0.44060597,  3.1694894 ,  3.1508877 ,  3.1576695 ,\n",
       "        1.4699876 , -0.90761054,  1.6876612 ,  2.0810297 ,  0.23714465,\n",
       "        0.98775154, -0.90306866, -0.17202905, -0.21620439, -3.5056663 ,\n",
       "        0.42750782,  1.7894748 ,  0.94500417, -3.2958972 ,  0.46764252,\n",
       "        0.92541003, -4.444386  , -1.0722455 ,  2.5362773 , -2.226714  ,\n",
       "       -1.4917594 , -1.036551  , -2.5088217 , -0.45545828, -0.67297757,\n",
       "       -3.637434  ,  0.97905415, -0.3767865 ,  0.36619383, -0.53772867,\n",
       "        0.8300506 ,  1.1588293 , -0.7510228 , -1.4198551 ,  1.9997263 ,\n",
       "        0.52530605,  3.3363237 ,  0.57558274, -0.33637553, -0.65354246,\n",
       "        0.7279299 ,  1.0040433 ,  3.7352421 ,  0.67701054,  1.7470617 ,\n",
       "       -0.9334592 , -0.21391785,  2.0906909 ,  2.2587235 ,  0.90430415,\n",
       "        0.29615343, -0.26158354,  2.4882104 , -1.0618318 , -0.03030134,\n",
       "       -4.967284  ,  0.6284575 ,  1.0722994 , -2.1507077 ,  0.43718958,\n",
       "        0.601414  , -1.8759408 ,  1.4711026 ,  0.8531924 ,  1.2522733 ,\n",
       "       -1.3565432 ,  1.0570819 ,  2.23234   , -1.3894241 ,  2.379316  ,\n",
       "        2.3634028 , -1.4148608 , -1.4855757 ,  0.27357584,  1.0230831 ,\n",
       "        0.5536548 , -1.0140018 , -2.6377695 ,  0.46495438,  1.676566  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"recipient\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = get_tmpfile(config.tokc_path+\"fasttext100.model\")\n",
    "file_name = get_tmpfile(config.tokc_path+\"fasttext100_lowercased.model\")\n",
    "model.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20683"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv)  # Not lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17418"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv)  # Lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"the\" in model.wv.key_to_index  # Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The\" in model.wv.key_to_index  # Looks good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (gender-bias)",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

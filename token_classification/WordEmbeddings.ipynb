{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Word Embeddings for Token Classifiers\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[0.](#0) Preprocessing\n",
    "\n",
    "[1.](#1) SpaCy's sense2cec\n",
    "\n",
    "[2.](#2) GloVe\n",
    "\n",
    "[3.](#3) Custom with fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In both virtual envs\n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "## In gender-bias virtual env only\n",
    "# import utils\n",
    "# import spacy\n",
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## In fasttext virtual env only\n",
    "from gensim.models import FastText\n",
    "from gensim import utils\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## 0. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_train = list(df_train.token)\n",
    "# lemmas_train = [lmtzr.lemmatize(token) for token in tokens_train]\n",
    "# tokens_dev = list(df_dev.token)\n",
    "# lemmas_dev = [lmtzr.lemmatize(token) for token in tokens_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.insert((list(df_train.columns).index(\"token\")+1), \"lemma\", lemmas_train)\n",
    "# df_dev.insert((list(df_dev.columns).index(\"token\")+1), \"lemma\", lemmas_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the vocabulary of the annotated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_dev])  # df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32957 42272\n",
      "27687 37002\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = list(set(list(df.token)))\n",
    "# unique_lemmas = list(set(list(df.lemma))) \n",
    "# unique_lemmas = [lemma for lemma in unique_lemmas if lemma.isalpha()]\n",
    "# lemmas_lower = [lemma.lower() for lemma in unique_lemmas]\n",
    "# unique_lemmas_lower = list(set(lemmas_lower))\n",
    "unique_words = [token for token in unique_tokens if token.isalpha()]  # keep tokens with only alphabetic characters\n",
    "print(len(unique_words), len(unique_tokens)) #, len(unique_lemmas), len(unique_lemmas_lower))\n",
    "\n",
    "unique_tokens_lower = [token.lower() if token.isalpha() else token for token in unique_tokens]\n",
    "unique_tokens_lower = list(set(unique_tokens_lower))\n",
    "unique_words_lower = [token.lower() for token in unique_words]\n",
    "unique_words_lower = list(set(unique_words_lower))\n",
    "print(len(unique_words_lower), len(unique_tokens_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. SpaCy's sense2vec\n",
    "\n",
    "Load [spaCy's contextual word embeddings](https://github.com/explosion/sense2vec), which were trained on 2015 Reddit posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sense2vec.component.Sense2VecComponent at 0x7f87677c5d30>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "s2v = nlp.add_pipe(\"sense2vec\")\n",
    "s2v.from_disk(config.s2v_reddit_path)\n",
    "\n",
    "#-------------\n",
    "# doc = nlp(\"A sentence about natural language processing.\")\n",
    "# assert doc[3:6].text == \"natural language processing\"\n",
    "# freq = doc[3:6]._.s2v_freq\n",
    "# vector = doc[3:6]._.s2v_vec\n",
    "# most_similar = doc[3:6]._.s2v_most_similar(3)\n",
    "# # [(('machine learning', 'NOUN'), 0.8986967),\n",
    "# #  (('computer vision', 'NOUN'), 0.8636297),\n",
    "# #  (('deep learning', 'NOUN'), 0.8573361)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cliffs', 'rpm', 'Heal', 'hereditary', 'Medjid', 'presumedly', 'Mondays', 'Routine', 'recipientESPMedawar', 'Kirkuk', 'Seton', 'Venado', 'Edith', 'Mackay', 'Visiting', 'interwar', 'atherogenic', 'Cawdor', 'jockey', 'Burgesses']\n"
     ]
    }
   ],
   "source": [
    "print(unique_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary not in Sense2Vec: 11529\n",
      "Proportion of vocabulary not in Sense2Vec: 0.40388859695218077\n"
     ]
    }
   ],
   "source": [
    "not_in_s2v = []\n",
    "for word in unique_words:\n",
    "    lowercased = word.lower()\n",
    "    w = (nlp(lowercased))[0]\n",
    "    if w._.s2v_vec is None:\n",
    "        not_in_s2v += [word]\n",
    "\n",
    "print(\"Total words in vocabulary not in Sense2Vec:\", len(not_in_s2v))\n",
    "print(\"Proportion of vocabulary not in Sense2Vec:\",(len(not_in_s2v))/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recipient', 'squabs', 'Darby', 'poulterer', 'Scotsman', 'sepolero', 'Morphogenetic', 'Hynes', 'Repleta', 'Simal', 'Mai', 'Arithmetic', 'Duce', 'Mme', 'lectureKatchalsky', 'inA', 'Tulsk', 'Berg', 'Mode', 'Bohme', 'Envelope', 'furnitureKoestler', 'Burmester', 'Evang', 'Ilona', 'compagne', 'BarnArthur', 'Cant', 'GollyArthur', 'Hatano', 'ElectionsAccompanied', 'accomodement', 'Finney', 'poetarum', 'Realites', 'Margaropus', 'LI', 'Lennox', 'Basberg', 'Ignacio', 'Glennie', 'Duffus', 'Takagi', 'Aliza', 'emir', 'Hersham', 'Rossini', 'Bald', 'Magnus', 'Pattison', 'Skefhill', 'Sorrel', 'Landolphin', 'Staub', 'ME', 'Alumbadi', 'Model', 'Majesties', 'Harian', 'trichocysts', 'Wandervogel', 'Rumped', 'Waetjen', 'Verbena', 'ofThe', 'sturzte', 'Lee', 'Copernicus', 'Verasis', 'Altenberg', 'unnumbered', 'environs', 'Neutral', 'Wynne', 'Mossman', 'ReneeESPCutten', 'ReadingSent', 'sequelae', 'Calary', 'Soviets', 'AustriaKoestler', 'Karlsburg', 'Peckham', 'Macdougall', 'Goldschmidt', 'electroencephalogram', 'neckHewitt', 'Simulans', 'Rothwell', 'prayeel', 'prickly', 'Dusen', 'Montagu', 'Prince', 'JournalSent', 'Microsatellite', 'Caius', 'Bubalus', 'Tyttenhanger', 'Conurbations']\n"
     ]
    }
   ],
   "source": [
    "# print(not_in_s2v[:100])\n",
    "# print(not_in_s2v[1000:1100])\n",
    "print(not_in_s2v[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2659\n"
     ]
    }
   ],
   "source": [
    "# x = \"lettersBaillie\"\n",
    "# y = \"writer'\"\n",
    "# z = \"MunsterSent\"\n",
    "newly_found = 0\n",
    "for word in not_in_s2v:\n",
    "    found = re.findall(\"[A-Z]{0,1}[a-z]+\", word)\n",
    "    for f in found:\n",
    "        lowercased = f.lower()\n",
    "        w = nlp(lowercased)[0]\n",
    "        if not w._.s2v_vec is None:\n",
    "            newly_found += 1\n",
    "print(newly_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion not found but possible to find: 0.23063578801283718\n",
      "Proportion of vocabulary not possible to find in Sense2Vec: 0.3107374321247154\n"
     ]
    }
   ],
   "source": [
    "print(\"Proportion not found but possible to find:\", newly_found/(len(not_in_s2v)))\n",
    "print(\"Proportion of vocabulary not possible to find in Sense2Vec:\",(len(not_in_s2v)-newly_found)/len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure it's worth reworking the tokenization and part-of-speech tagging to increase Sense2Vec's coverage of the vocabulary by only about 9%, so we'll keep going with the model input data as it is for now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [GloVe word embeddings](https://github.com/stanfordnlp/GloVe), which were trained on 2014 English Wikipedia entries and Gigaword 5:\n",
    "\n",
    "*Note: could also try [GN-GloVe](https://github.com/uclanlp/gn_glove), which supposedly has gender-neutral word embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\n",
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]  # pretrained GloVe embeddings come as vectors with one of these four dimensions\n",
    "d = dimensions[0]  # start small to begin with\n",
    "glove_path = config.inf_data_path+\"glove.6B/glove.6B.{}d.txt\".format(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13412    1.5041     0.39706   -0.41357    0.71336    0.63438\n",
      "  0.1214     0.16746    1.4104     0.013127   0.68386    0.85236\n",
      "  0.92599    0.098158   0.425     -0.83799    0.08482    0.22135\n",
      " -0.15247    0.72654    0.052728  -1.0064    -0.032626  -0.63342\n",
      "  0.0039789 -1.5288    -0.74005   -1.2051    -1.0227    -0.10588\n",
      "  1.2225     0.14845   -0.1641    -0.52887   -0.29012    0.59774\n",
      "  0.62847    0.49003   -0.14227   -1.2193     0.56094   -0.17673\n",
      " -0.11216   -0.41801   -0.40841   -0.41748   -0.40276    0.25091\n",
      " -0.43016    0.26412  ]\n"
     ]
    }
   ],
   "source": [
    "glove = dict()\n",
    "with open(glove_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        glove[word] = vector\n",
    "print(glove[\"recipient\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate GloVe's coverage of our vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary not in GloVe: 6453\n",
      "Proportion of vocabulary not in GloVe: 0.1958005886458112\n"
     ]
    }
   ],
   "source": [
    "not_in_glove = []\n",
    "in_glove = dict()\n",
    "for word in unique_words:\n",
    "    lowercased = word.lower()\n",
    "    try:\n",
    "        vector = glove[lowercased]\n",
    "        in_glove[word] = vector\n",
    "    except KeyError:\n",
    "        not_in_glove += [word]\n",
    "\n",
    "print(\"Total words in vocabulary not in GloVe:\", len(not_in_glove))\n",
    "print(\"Proportion of vocabulary not in GloVe:\",(len(not_in_glove))/len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not possible to find in GloVe: 4187\n",
      "Proportion not found but possible to find in GloVe: 0.7311328064466139\n",
      "Proportion of vocabulary not possible to find in GloVe: 0.12704433049124617\n"
     ]
    }
   ],
   "source": [
    "still_not_found = []\n",
    "newly_found = 0\n",
    "# partial_glove_match = dict()\n",
    "for word in not_in_glove:\n",
    "    found = re.findall(\"[A-Z]{0,1}[a-z]+\", word)\n",
    "    for f in found:\n",
    "        lowercased = f.lower()\n",
    "        try:\n",
    "            vector = glove[lowercased]\n",
    "            in_glove[word] = vector  # partial_glove_match[word] = vector\n",
    "            newly_found += 1\n",
    "        except KeyError:\n",
    "            still_not_found += [word]\n",
    "print(\"Not possible to find in GloVe:\", len(still_not_found))\n",
    "print(\"Proportion not found but possible to find in GloVe:\", newly_found/(len(not_in_glove)))\n",
    "print(\"Proportion of vocabulary not possible to find in GloVe:\",(len(still_not_found))/len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe has much better coverage than sense2vec, as expected due to the better domain match (Wikipedia entries are more similar to archival metadata descriptions than Reddit comments!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_vectors = in_glove.copy()\n",
    "key_array = np.array(words_to_vectors.keys())\n",
    "for word in unique_words:\n",
    "    if word not in key_array:\n",
    "        if word.lower() in key_array:\n",
    "            vector = words_to_vectors[word.lower()]\n",
    "        else:\n",
    "            vector = np.array([])\n",
    "        words_to_vectors[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(words_to_vectors) == len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset associating each token to a GloVe word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_col_name = \"glove_embedding\"\n",
    "embedding_dict = glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>glove_embedding</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>Title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>Papers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_id                                    glove_embedding   token\n",
       "3         3  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...   Title\n",
       "4         4                                                 []       :\n",
       "5         5  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...  Papers\n",
       "6         6  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...      of\n",
       "7         7  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...     The"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings = utils.createEmbeddingDataFrame(df_train, embedding_dict, embedding_col_name)\n",
    "train_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>glove_embedding</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>154</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>After</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>155</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>156</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>ordination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>157</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>158</td>\n",
       "      <td>[-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...</td>\n",
       "      <td>spent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token_id                                    glove_embedding       token\n",
       "172       154  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...       After\n",
       "173       155  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...         his\n",
       "174       156  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...  ordination\n",
       "175       157  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...          he\n",
       "176       158  [-0.15234, 0.98085, 1.0065, 0.97812, 0.6628, 0...       spent"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_embeddings = utils.createEmbeddingDataFrame(df_dev, embedding_dict, embedding_col_name)\n",
    "dev_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings.to_csv(config.tokc_path+\"glove_embeddings_train.csv\")\n",
    "# dev_embeddings.to_csv(config.tokc_path+\"glove_embeddings_dev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Custom with fastText\n",
    "\n",
    "Train word embeddings on my own data (metadata descriptions from the CRC's Archives catalog) using fastText.\n",
    "\n",
    "*References:* \n",
    "* *https://radimrehurek.com/gensim/models/fasttext.html*\n",
    "* *https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusIterator:\n",
    "    def implodeDataFrame(self, df, cols_to_groupby):\n",
    "        cols_to_agg = list(df.columns)\n",
    "        for col in cols_to_groupby:\n",
    "            cols_to_agg.remove(col)\n",
    "        agg_dict = dict.fromkeys(cols_to_agg, lambda x: x.tolist())\n",
    "        return df.groupby(cols_to_groupby).agg(agg_dict).reset_index().set_index(cols_to_groupby)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", usecols=[\"sentence_id\", \"token\"])\n",
    "        df_train_imploded = self.implodeDataFrame(df_train, [\"sentence_id\"])\n",
    "        sentences = list(df_train_imploded.token)\n",
    "        for token_list in sentences:\n",
    "            yield(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters for the unsupervised training of the fastText model (essentially a word2vec model that uses using character n-grams so subwords can help to assign embeddings to unseen words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training architecture (default = \"cbow\" for Continuous Bag of Words)\n",
    "models = [\"cbow\", \"skipgram\"]\n",
    "model = models[0]\n",
    "# Specify the learning rate (default = 0.025)\n",
    "alpha = 0.025\n",
    "# Specify the training objective (default = \"ns\")\n",
    "# losses = [\"ns\", \"hs\", \"softmax\"]\n",
    "# loss = losses[0]\n",
    "# Specify the number of negative words to sample for 'ns' training objective (default = 5)\n",
    "negative = 5\n",
    "# Specify the threshold for downsampling higher-frequency words (default = 0.001)\n",
    "sample = 0.001\n",
    "# Specify the word embeddings' dimensions\n",
    "vector_dimensions = 100\n",
    "# Specify the context window (default is 5) \n",
    "context_window = 5\n",
    "# Specify the number of epochs (default is 5)\n",
    "epochs = 5\n",
    "# Specify the threshold of word occurrences (ignore words that occur less than specified number of times; default = 5)\n",
    "min_count = 5\n",
    "# Specify the minimum and maximum length of character ngrams (defaults are 3 and 6)\n",
    "min_n = 2\n",
    "max_n = 6  # if 0, no character n-grams (subword vectors) will be used\n",
    "# Specify the number of buckets for hashing ngrams (default = 2000000) \n",
    "bucket = 2000000\n",
    "# Sort vocabulary by descending frequency (default = 1)\n",
    "sorted_vocab = 1\n",
    "# Specify the number of threads to use (default = 12)\n",
    "# threads = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\n",
    "    alpha=alpha, negative=negative, sample=sample,\n",
    "    vector_size=vector_dimensions, window=context_window, \n",
    "    epochs=epochs, min_count=min_count, min_n=min_n, \n",
    "    max_n=max_n, bucket=bucket, sorted_vocab=sorted_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(corpus_iterable=CorpusIterator())\n",
    "total_examples = model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1307490, 2337820)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(corpus_iterable=CorpusIterator(), total_examples=total_examples, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6756068 ,  1.5919273 , -0.56705076,  1.6880698 ,  0.9636156 ,\n",
       "        0.31721365,  0.25568432,  0.2058282 , -0.1416149 , -0.18989426,\n",
       "       -0.54189414, -1.4998999 ,  0.03471487, -0.9126452 , -0.0782093 ,\n",
       "       -0.14311433, -1.1163957 ,  1.0090419 ,  1.6154461 ,  0.65505093,\n",
       "       -0.8446712 , -1.4965539 , -1.4920071 , -1.0338659 , -0.9557528 ,\n",
       "        0.34576824, -0.69574195, -1.7805381 ,  1.5230308 , -0.47035334,\n",
       "        0.03263632, -1.5507953 , -0.7169255 ,  0.15754858,  1.8103783 ,\n",
       "        0.4070575 ,  0.2572637 ,  1.3328292 , -0.14403729, -1.196998  ,\n",
       "        2.234289  , -1.3798985 ,  0.6903577 , -2.6350315 , -0.94290125,\n",
       "        1.2201816 , -1.5511761 ,  0.26433146,  1.7597557 ,  3.1027613 ,\n",
       "        2.854742  , -1.8140292 ,  2.5589483 ,  1.5211413 ,  1.2342783 ,\n",
       "       -0.47556603,  2.3050432 ,  0.23175682,  1.2051044 ,  0.06902517,\n",
       "        0.6075335 , -1.9958506 , -0.40674683,  0.3392531 ,  0.19527328,\n",
       "       -3.3649073 ,  1.6847279 ,  0.7022183 , -0.39973742,  0.7405594 ,\n",
       "       -0.28187686, -2.2621083 ,  1.5738987 ,  0.804079  , -1.7810291 ,\n",
       "       -0.6023401 ,  1.532785  , -0.50126266, -1.7317843 , -2.0655541 ,\n",
       "        2.8558273 ,  1.9368174 , -1.8241817 ,  0.8405646 , -1.060433  ,\n",
       "       -0.741891  , -1.8099079 , -0.39423892, -1.2813419 , -1.9725196 ,\n",
       "        0.23303045,  1.414299  , -0.64058423,  0.46137232, -2.6979005 ,\n",
       "        0.03059335, -1.669889  ,  2.8565264 ,  0.78288233, -0.47791556],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"recipient\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = get_tmpfile(config.tokc_path+\"fasttext100.model\")\n",
    "model.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7277"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.key_to_index[\"recipient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03737615, -0.13094172,  0.4192771 , -0.10888413,  0.4322802 ,\n",
       "       -0.6235069 , -0.04176774,  0.5860596 , -0.97697514,  0.9668318 ,\n",
       "        1.0936502 ,  0.7192852 ,  0.43130356, -0.13834317,  0.14606117,\n",
       "        1.014545  ,  0.46894675, -0.02479243,  0.38670725, -1.0797498 ,\n",
       "        1.552295  , -0.4531086 , -0.12248089,  0.24791214,  0.28702378,\n",
       "        0.5095818 ,  0.14123417,  0.07884683, -0.2801781 ,  0.2914682 ,\n",
       "        0.9706851 , -0.4849089 ,  0.98705786,  0.17181778, -1.0430001 ,\n",
       "       -0.34546047,  0.13864943,  0.13679619,  0.48705992, -0.31251132,\n",
       "       -0.48371068,  0.33515218, -0.07702316, -0.3278501 , -0.21714082,\n",
       "       -0.28916112,  0.36632246, -0.72787535, -0.8380801 ,  0.3025424 ,\n",
       "        0.42312002,  0.05307492,  0.17401198, -1.0034671 , -1.0878724 ,\n",
       "       -0.83836   ,  0.8597549 , -1.3812542 , -0.8466691 , -0.7410131 ,\n",
       "       -1.7701464 ,  0.05303479,  0.4593584 ,  0.03552771, -0.01448729,\n",
       "        1.8695406 ,  1.0092819 ,  0.05009551,  2.3934693 ,  0.44692302,\n",
       "       -0.46287727,  0.62252253, -0.29737478, -0.6069261 ,  0.7568382 ,\n",
       "       -0.9369674 ,  1.3727007 ,  0.1289076 , -0.53901565,  0.3409355 ,\n",
       "       -0.77126825, -0.49398908,  1.4447385 , -0.77188575,  1.8950756 ,\n",
       "       -0.01626079, -0.33861881,  2.153005  , -1.0230808 , -1.0144427 ,\n",
       "        0.86287147, -0.33307582,  0.02943535, -0.04924192, -0.56257224,\n",
       "        2.0533156 , -0.970897  ,  0.03476514, -0.132358  ,  0.03242276],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6756068 ,  1.5919273 , -0.56705076,  1.6880698 ,  0.9636156 ,\n",
       "        0.31721365,  0.25568432,  0.2058282 , -0.1416149 , -0.18989426,\n",
       "       -0.54189414, -1.4998999 ,  0.03471487, -0.9126452 , -0.0782093 ,\n",
       "       -0.14311433, -1.1163957 ,  1.0090419 ,  1.6154461 ,  0.65505093,\n",
       "       -0.8446712 , -1.4965539 , -1.4920071 , -1.0338659 , -0.9557528 ,\n",
       "        0.34576824, -0.69574195, -1.7805381 ,  1.5230308 , -0.47035334,\n",
       "        0.03263632, -1.5507953 , -0.7169255 ,  0.15754858,  1.8103783 ,\n",
       "        0.4070575 ,  0.2572637 ,  1.3328292 , -0.14403729, -1.196998  ,\n",
       "        2.234289  , -1.3798985 ,  0.6903577 , -2.6350315 , -0.94290125,\n",
       "        1.2201816 , -1.5511761 ,  0.26433146,  1.7597557 ,  3.1027613 ,\n",
       "        2.854742  , -1.8140292 ,  2.5589483 ,  1.5211413 ,  1.2342783 ,\n",
       "       -0.47556603,  2.3050432 ,  0.23175682,  1.2051044 ,  0.06902517,\n",
       "        0.6075335 , -1.9958506 , -0.40674683,  0.3392531 ,  0.19527328,\n",
       "       -3.3649073 ,  1.6847279 ,  0.7022183 , -0.39973742,  0.7405594 ,\n",
       "       -0.28187686, -2.2621083 ,  1.5738987 ,  0.804079  , -1.7810291 ,\n",
       "       -0.6023401 ,  1.532785  , -0.50126266, -1.7317843 , -2.0655541 ,\n",
       "        2.8558273 ,  1.9368174 , -1.8241817 ,  0.8405646 , -1.060433  ,\n",
       "       -0.741891  , -1.8099079 , -0.39423892, -1.2813419 , -1.9725196 ,\n",
       "        0.23303045,  1.414299  , -0.64058423,  0.46137232, -2.6979005 ,\n",
       "        0.03059335, -1.669889  ,  2.8565264 ,  0.78288233, -0.47791556],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"recipient\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (fasttext)",
   "language": "python",
   "name": "fasttext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

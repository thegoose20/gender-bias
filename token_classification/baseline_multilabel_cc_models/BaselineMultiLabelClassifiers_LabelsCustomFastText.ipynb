{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Gender Bias Multilabel Token Classifiers with fastText\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/model_input/`\n",
    "    * Prediction Data: under directory `../data/token_clf_data/multilabel_model_output/`\n",
    "* Multilabel classification\n",
    "    * 3 categories of labels:\n",
    "        1. Person Name: Unknown, Feminine, Masculine (Non-binary not applied during annotation)\n",
    "        2. Linguistic: Generalization, Gendered Pronoun, Gendered Role\n",
    "        3. Contextual: Occupation, Omission, Stereotype (Empowering only applied by one annotator and too few times for training)\n",
    "* Word embeddings: custom fastText embeddings\n",
    "\n",
    "***\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[0.](#0) Preprocessing\n",
    "\n",
    "[1.](#CC) Classifier Chain Models\n",
    "\n",
    "[2.](#2) Person Name Model\n",
    "\n",
    "[3.](#3) Linguistic Model\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For embeddings\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For classification\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "# Base estimators\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## 0. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and validation (dev) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463441, 9) (156146, 9)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop(columns=[\"ann_id\"])\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev.drop(columns=[\"ann_id\"])\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Non-binary labels as these were mistaken labels identified early on that were meant to be excluded, and because only one token has this label, it prevents the data from being input into the models with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.tag != \"B-Nonbinary\"]\n",
    "df_train = df_train.loc[df_train.tag != \"I-Nonbinary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463439, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Optional Preprocessing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not classifying all labels at once, consider only the rows with tags for the select subset of labels, replacing all tags not in that subset with `\"O\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont_label_subset = [\"B-Stereotype\", \"I-Stereotype\", \"B-Omission\", \"I-Omission\"]\n",
    "pers_label_subset = [\"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \"B-Masculine\", \"I-Masculine\", \"B-Occupation\", \"I-Occupation\"]#, \"B-Nonbinary\", \"I-Nonbinary\"]\n",
    "# ling_label_subset = [\"B-Generalization\", \"I-Generalization\", \"B-Gendered-Role\", \"I-Gendered-Role\", \"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"]\n",
    "df_train, df_dev = utils.selectDataForLabels(df_train, df_dev, \"tag\", pers_label_subset)\n",
    "# df_train, df_dev = utils.selectDataForLabels(df_train, df_dev, \"tag\", ling_label_subset)\n",
    "# print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by token, so there is one row per token rather than one row per token-tag pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos  tag\n",
       "0            5       154       After    IN  [O]\n",
       "1            5       155         his  PRP$  [O]\n",
       "2            5       156  ordination    NN  [O]\n",
       "3            5       157          he   PRP  [O]\n",
       "4            5       158       spent   VBD  [O]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_train = df_train.drop(columns=[\"description_id\", \"field\", \"subset\", \"token_offsets\"])\n",
    "subdf_dev = df_dev.drop(columns=[\"description_id\", \"field\", \"subset\", \"token_offsets\"])\n",
    "df_train_imploded = utils.implodeDataFrame(subdf_train, [\"sentence_id\", \"token_id\", \"token\", \"pos\"])\n",
    "df_train_imploded = df_train_imploded.reset_index()\n",
    "df_dev_imploded = utils.implodeDataFrame(subdf_dev, [\"sentence_id\", \"token_id\", \"token\", \"pos\"])\n",
    "df_dev_imploded = df_dev_imploded.reset_index()\n",
    "df_dev_imploded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the tags with label names (remove ``B-`` and ``I-``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelColFromTagCol(df, col):\n",
    "    col_list = list(df[col])\n",
    "    new_col = []\n",
    "    for value_list in col_list:\n",
    "        new_value_list = []\n",
    "        for value in value_list:\n",
    "            if value != \"O\":\n",
    "                new_value = value[2:]\n",
    "                new_value_list += [new_value]\n",
    "            else:\n",
    "                new_value_list += [value]\n",
    "        # Remove any duplicates from the list of labels\n",
    "        unique_values = list(set(new_value_list))\n",
    "        # Sort the list of labels alphabetically\n",
    "        unique_values.sort()\n",
    "        new_col += [unique_values]\n",
    "    assert len(new_col) == len(col_list)\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = getLabelColFromTagCol(df_train_imploded, \"tag\")\n",
    "# # train_labels[:10]  # Looks good\n",
    "dev_labels = getLabelColFromTagCol(df_dev_imploded, \"tag\")\n",
    "# dev_labels[:10] # Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>[O, B-Unknown, B-Masculine]</td>\n",
       "      <td>[Masculine, O, Unknown]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id   token  pos                          tag  \\\n",
       "0            1         3   Title   NN                          [O]   \n",
       "1            1         4       :    :                          [O]   \n",
       "2            1         5  Papers  NNS                          [O]   \n",
       "3            1         6      of   IN                          [O]   \n",
       "4            1         7     The   DT  [O, B-Unknown, B-Masculine]   \n",
       "\n",
       "                     label  \n",
       "0                      [O]  \n",
       "1                      [O]  \n",
       "2                      [O]  \n",
       "3                      [O]  \n",
       "4  [Masculine, O, Unknown]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_imploded.insert(len(df_train_imploded.columns), \"label\", train_labels)\n",
    "df_train_imploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos  tag label\n",
       "0            5       154       After    IN  [O]   [O]\n",
       "1            5       155         his  PRP$  [O]   [O]\n",
       "2            5       156  ordination    NN  [O]   [O]\n",
       "3            5       157          he   PRP  [O]   [O]\n",
       "4            5       158       spent   VBD  [O]   [O]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_imploded.insert(len(df_dev_imploded.columns), \"label\", dev_labels)\n",
    "df_dev_imploded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate word embeddings to the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]\n",
    "d = dimensions[1]\n",
    "file_name = config.fasttext_path+\"fasttext{}_lowercased.model\".format(d)\n",
    "embedding_model = FastText.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 35968\n",
      "Lowercased vocabulary size: 31335\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(df_train.token.unique())\n",
    "vocabulary_lowercased = [token.lower() for token in vocabulary]\n",
    "vocabulary_lowercased = list(set(vocabulary_lowercased))\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"Lowercased vocabulary size:\", len(vocabulary_lowercased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize and binarize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"label\"\n",
    "feature_cols = [\"token_id\", \"token\"]\n",
    "train_data = df_train_imploded\n",
    "dev_data = df_dev_imploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vector representation of a token from a fastText word embedding model\n",
    "def extractEmbedding(token, fasttext_model=embedding_model):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    embedding = fasttext_model.wv[token]\n",
    "    return embedding\n",
    "\n",
    "def makeFeatureMatrix(token_data):\n",
    "    feature_list = [extractEmbedding(token) for token_id,token in token_data]\n",
    "    return np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = list(zip(train_data[feature_cols[0]], train_data[feature_cols[1]]))\n",
    "dev_tokens = list(zip(dev_data[feature_cols[0]], dev_data[feature_cols[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(452086, 100) (152455, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train = makeFeatureMatrix(train_tokens)\n",
    "X_dev = makeFeatureMatrix(dev_tokens)\n",
    "print(X_train.shape, X_dev.shape)  # number_of_samples, number_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarize targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(452086, 5) (152455, 5)\n"
     ]
    }
   ],
   "source": [
    "y_train_labels = train_data[target_col]\n",
    "y_train = mlb.fit_transform(y_train_labels)\n",
    "y_dev_labels = dev_data[target_col]\n",
    "y_dev = mlb.transform(y_dev_labels)\n",
    "print(y_train.shape, y_dev.shape)  # number_of_samples, number_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilabelled tokens exist, as expected.\n"
     ]
    }
   ],
   "source": [
    "for labels in y_train:\n",
    "    if sum(labels) > 1:\n",
    "        print(\"Multilabelled tokens exist, as expected.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For baseline models, use only the tokens' embeddings as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CC\"></a>\n",
    "## 1. Classifier Chain Model\n",
    "\n",
    "*Reference: http://scikit.ml/api/skmultilearn.problem_transform.cc.html#skmultilearn.problem_transform.ClassifierChain*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=RandomForestClassifier(random_state=22),\n",
       "                require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=RandomForestClassifier(random_state=22),\n",
       "                require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=22)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=22)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ClassifierChain(classifier=RandomForestClassifier(random_state=22),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"rf\"\n",
    "clf = ClassifierChain(\n",
    "    classifier = RandomForestClassifier(random_state=22),\n",
    ")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - micro: 0.9620258911430498\n",
      "Precision - macro: 0.7574251504839203\n",
      "\n",
      "Recall - micro: 0.9545840680627599\n",
      "Recall - macro: 0.4977462402132226\n",
      "\n",
      "F1 Score - micro: 0.9582905320282973\n",
      "F1 Score - macro: 0.5866765372981741\n",
      "\n",
      "Accuracy - normalized: 0.9539470663474467\n",
      "Accuracy - unnormalized: 145434\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - micro:\", metrics.precision_score(y_dev, predictions, average=\"micro\", zero_division=0))  # micro = calculated from TP, FP, FN sums across all labels\n",
    "print(\"Precision - macro:\", metrics.precision_score(y_dev, predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "print()\n",
    "print(\"Recall - micro:\", metrics.recall_score(y_dev, predictions, average=\"micro\", zero_division=0))\n",
    "print(\"Recall - macro:\", metrics.recall_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - micro:\", metrics.f1_score(y_dev, predictions, average=\"micro\", zero_division=0))\n",
    "print(\"F1 Score - macro:\", metrics.f1_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"Accuracy - normalized:\", metrics.accuracy_score(y_dev, predictions, normalize=True))  # fraction of correctly classified samples\n",
    "print(\"Accuracy - unnormalized:\", metrics.accuracy_score(y_dev, predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - per label: [0.80594059 0.66769231 0.97170646 0.70070922 0.64107717]\n",
      "\n",
      "Recall - per label: [0.43114407 0.39544419 0.99363917 0.34425087 0.3242529 ]\n",
      "\n",
      "F1 Score - per label: [0.56176674 0.49670959 0.98255044 0.46168224 0.43067369]\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - per label:\", metrics.precision_score(y_dev, predictions, average=None, zero_division=0))\n",
    "print()\n",
    "print(\"Recall - per label:\", metrics.recall_score(y_dev, predictions, average=None, zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - per label:\", metrics.f1_score(y_dev, predictions, average=None, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the scores above are calculated with the `\"O\"` label*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos  tag predicted_label\n",
       "0            5       154       After    IN  [O]               O\n",
       "1            5       155         his  PRP$  [O]               O\n",
       "2            5       156  ordination    NN  [O]               O\n",
       "3            5       157          he   PRP  [O]               O\n",
       "4            5       158       spent   VBD  [O]               O"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = utils.makePredictionDF(predictions, dev_data, \"label\", \"predicted_label\", \"O\", mlb)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = dev_data.explode([\"label\"])\n",
    "exp_df = exp_df.rename(columns={\"label\":\"expected_label\"})\n",
    "# exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos expected_label predicted_label  \\\n",
       "0            5       154       After    IN              O               O   \n",
       "1            5       155         his  PRP$              O               O   \n",
       "2            5       156  ordination    NN              O               O   \n",
       "3            5       157          he   PRP              O               O   \n",
       "4            5       158       spent   VBD              O               O   \n",
       "\n",
       "          _merge  \n",
       "0  true negative  \n",
       "1  true negative  \n",
       "2  true negative  \n",
       "3  true negative  \n",
       "4  true negative  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = utils.makeEvaluationDataFrame(\n",
    "    exp_df, \n",
    "    pred_df, \n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_label\"],   # left on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"predicted_label\"],  # right on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_label\", \"predicted_label\", \"_merge\"],  # final column list\n",
    "    \"expected_label\",\n",
    "    \"predicted_label\", \n",
    "    \"token_id\",  # ID column\n",
    "    \"O\"          # No tag value\n",
    ")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_output/\").mkdir(parents=True, exist_ok=True)\n",
    "# eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_baseline_fastText{d}_predictions.csv\".format(a=a,d=d))\n",
    "# eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_linguistic_baseline_fastText{d}_predictions.csv\".format(a=a,d=d))\n",
    "eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_pno_baseline_fastText{d}_predictions.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strict Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_stats = utils.getAgreementStatsForAllTags(eval_df, \"_merge\", \"token_id\", \"label(s)\", y_dev, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ \n",
    "    'Feminine', 'Masculine', 'Unknown',                      # Person Name category of labels\n",
    "#     'Gendered-Pronoun','Gendered-Role', 'Generalization',   # Linguistic category of labels\n",
    "    'Occupation' #, 'Omission', 'Stereotype'                  # Contextual category of labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>5906</td>\n",
       "      <td>5496</td>\n",
       "      <td>144965</td>\n",
       "      <td>2614</td>\n",
       "      <td>0.757425</td>\n",
       "      <td>0.497746</td>\n",
       "      <td>0.586677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>379</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>650</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.631681</td>\n",
       "      <td>0.745413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>956</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>944</td>\n",
       "      <td>0.775041</td>\n",
       "      <td>0.496842</td>\n",
       "      <td>0.605516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>2794</td>\n",
       "      <td>734</td>\n",
       "      <td>0</td>\n",
       "      <td>2698</td>\n",
       "      <td>0.786131</td>\n",
       "      <td>0.491260</td>\n",
       "      <td>0.604662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>849</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>936</td>\n",
       "      <td>0.822496</td>\n",
       "      <td>0.524370</td>\n",
       "      <td>0.640438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label(s)  false negative  false positive  true negative  true positive  \\\n",
       "0         all            5906            5496         144965           2614   \n",
       "0    Feminine             379              65              0            650   \n",
       "0   Masculine             956             274              0            944   \n",
       "0     Unknown            2794             734              0           2698   \n",
       "0  Occupation             849             202              0            936   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.757425  0.497746  0.586677  \n",
       "0   0.909091  0.631681  0.745413  \n",
       "0   0.775041  0.496842  0.605516  \n",
       "0   0.786131  0.491260  0.604662  \n",
       "0   0.822496  0.524370  0.640438  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in labels:\n",
    "    label_agmt_stats = utils.getScoresByTags(eval_df, \"_merge\", [label], exp_col=\"expected_label\", pred_col=\"predicted_label\")\n",
    "    label_agmt_stats = label_agmt_stats.rename(columns={\"tag(s)\":\"label(s)\"})\n",
    "    agmt_stats = pd.concat([agmt_stats, label_agmt_stats])\n",
    "agmt_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_performance/\").mkdir(parents=True, exist_ok=True)\n",
    "# agmt_stats.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_baseline_fastText{d}_strict_agmt.csv\".format(a=a,d=d))\n",
    "# agmt_stats.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_linguistic_baseline_fastText{d}_strict_agmt.csv\".format(a=a,d=d))\n",
    "agmt_stats.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_pno_baseline_fastText{d}_strict_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "***\n",
    "*Note: models below use tags as targets instead of labels*\n",
    "\n",
    "## 2. Person Name Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multilabel models with the `PassiveAggressiveClassifier` for the Person Name category of labels in order to compare their performance to the sequence classifier's performance (the Passive Aggressive algorithm was top-performing for the baseline Person Name sequence classification model).\n",
    "\n",
    "Then, try the `RandomForestClassifier` since this yeilded high performance in the optimization experiments.\n",
    "\n",
    "#### Hypothesis\n",
    "* The baseline sequence classifiers will outperform (F1 score >0.1 higher) the baseline multilabel token classifiers for labels in the Person Name category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ClassifierChain(classifier=PassiveAggressiveClassifier(loss='squared_hinge',\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"pa\"\n",
    "pn_clf = ClassifierChain(classifier=PassiveAggressiveClassifier(\n",
    "        max_iter=100, \n",
    "        loss=\"squared_hinge\",  # equivalent to pa_type=2 (PA-II)\n",
    "        random_state=22,\n",
    "    )\n",
    ")\n",
    "# a = \"rf\"\n",
    "# pn_clf = ClassifierChain(classifier=RandomForestClassifier(random_state=22))\n",
    "pn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pn_clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - weighted: 0.9246753427798468\n",
      "Precision - macro: 0.29125171686438994\n",
      "\n",
      "Recall - weighted: 0.9385097847295113\n",
      "Recall - macro: 0.29917853276102013\n",
      "\n",
      "F1 Score - weighted: 0.9308920661329323\n",
      "F1 Score - macro: 0.2790846321002277\n",
      "\n",
      "Accuracy - normalized: 0.8960086582926109\n",
      "Accuracy - unnormalized: 136601\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - weighted:\", metrics.precision_score(y_dev, predictions, average=\"weighted\", zero_division=0))\n",
    "print(\"Precision - macro:\", metrics.precision_score(y_dev, predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "print()\n",
    "print(\"Recall - weighted:\", metrics.recall_score(y_dev, predictions, average=\"weighted\", zero_division=0))\n",
    "print(\"Recall - macro:\", metrics.recall_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - weighted:\", metrics.f1_score(y_dev, predictions, average=\"weighted\", zero_division=0))\n",
    "print(\"F1 Score - macro:\", metrics.f1_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"Accuracy - normalized:\", metrics.accuracy_score(y_dev, predictions, normalize=True))  # fraction of correctly classified samples\n",
    "print(\"Accuracy - unnormalized:\", metrics.accuracy_score(y_dev, predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>predicted_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>I-Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos predicted_tag\n",
       "0            5       154       After    IN             O\n",
       "1            5       155         his  PRP$             O\n",
       "2            5       156  ordination    NN             O\n",
       "3            5       157          he   PRP     I-Unknown\n",
       "3            5       157          he   PRP             O"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = utils.makePredictionDF(predictions, dev_data, \"tag\", \"predicted_tag\", \"O\", mlb)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = dev_data.explode([\"tag\"])\n",
    "exp_df = exp_df.rename(columns={\"tag\":\"expected_tag\"})\n",
    "# exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>predicted_tag</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos expected_tag predicted_tag  \\\n",
       "0            5       154       After    IN            O             O   \n",
       "1            5       155         his  PRP$            O             O   \n",
       "2            5       156  ordination    NN            O             O   \n",
       "3            5       157          he   PRP            O             O   \n",
       "4            5       158       spent   VBD            O             O   \n",
       "\n",
       "          _merge  \n",
       "0  true negative  \n",
       "1  true negative  \n",
       "2  true negative  \n",
       "3  true negative  \n",
       "4  true negative  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = utils.makeEvaluationDataFrame(\n",
    "    exp_df, \n",
    "    pred_df, \n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_tag\"],   # left on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"predicted_tag\"],  # right on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_tag\", \"predicted_tag\", \"_merge\"],  # final column list\n",
    "    \"expected_tag\",\n",
    "    \"predicted_tag\", \n",
    "    \"token_id\",  # ID column\n",
    "    \"O\"          # No tag value\n",
    ")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_output/\").mkdir(parents=True, exist_ok=True)\n",
    "eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_baseline_fastText{d}_pn_predictions.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strict Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_stats = utils.getAgreementStatsForAllTags(eval_df, \"_merge\", \"token_id\", \"tag(s)\", y_dev, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tags = [ \n",
    "    'B-Unknown', 'I-Unknown', 'B-Feminine', 'I-Feminine', 'B-Masculine',  'I-Masculine',\n",
    "#     'B-Gendered-Pronoun', 'I-Gendered-Pronoun','B-Gendered-Role', 'I-Gendered-Role', \n",
    "#     'B-Generalization', 'I-Generalization', \n",
    "#     'B-Stereotype', 'I-Stereotype', 'B-Omission', 'I-Omission', 'B-Occupation', 'I-Occupation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>8061</td>\n",
       "      <td>9982</td>\n",
       "      <td>145151</td>\n",
       "      <td>1042</td>\n",
       "      <td>0.291252</td>\n",
       "      <td>0.299179</td>\n",
       "      <td>0.279085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>1051</td>\n",
       "      <td>1440</td>\n",
       "      <td>0</td>\n",
       "      <td>730</td>\n",
       "      <td>0.336406</td>\n",
       "      <td>0.409882</td>\n",
       "      <td>0.369527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>2319</td>\n",
       "      <td>2247</td>\n",
       "      <td>0</td>\n",
       "      <td>714</td>\n",
       "      <td>0.241135</td>\n",
       "      <td>0.235410</td>\n",
       "      <td>0.238238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>156</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.512315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>382</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>0.422383</td>\n",
       "      <td>0.379870</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>431</td>\n",
       "      <td>658</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0.280467</td>\n",
       "      <td>0.235789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>839</td>\n",
       "      <td>725</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>0.155995</td>\n",
       "      <td>0.137718</td>\n",
       "      <td>0.146288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tag(s)  false negative  false positive  true negative  true positive  \\\n",
       "0          all            8061            9982         145151           1042   \n",
       "0    B-Unknown            1051            1440              0            730   \n",
       "0    I-Unknown            2319            2247              0            714   \n",
       "0   B-Feminine             156              42              0            104   \n",
       "0   I-Feminine             382             320              0            234   \n",
       "0  B-Masculine             431             658              0            168   \n",
       "0  I-Masculine             839             725              0            134   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.291252  0.299179  0.279085  \n",
       "0   0.336406  0.409882  0.369527  \n",
       "0   0.241135  0.235410  0.238238  \n",
       "0   0.712329  0.400000  0.512315  \n",
       "0   0.422383  0.379870  0.400000  \n",
       "0   0.203390  0.280467  0.235789  \n",
       "0   0.155995  0.137718  0.146288  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label_tag in label_tags:\n",
    "    label_agmt_stats = utils.getScoresByTags(eval_df, \"_merge\", [label_tag])\n",
    "    agmt_stats = pd.concat([agmt_stats, label_agmt_stats])\n",
    "agmt_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_performance/\").mkdir(parents=True, exist_ok=True)\n",
    "agmt_stats.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_baseline_fastText{d}_pn_strict_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loose Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each label, where a correct prediction is a prediction with the correct annotation label (not necessarily the correct IOB tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the evaluation DataFrame where tags are replaced by label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tags = {\n",
    "    \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "#     \"Gendered Pronoun\": [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"], \"Gendered Role\": [\"B-Gendered-Role\", \"I-Gendered-Role\"],\n",
    "#     \"Generalization\": [\"B-Generalization\", \"I-Generalization\"], \n",
    "#     \"Stereotype\": [\"B-Stereotype\", \"I-Stereotype\"], \"Omission\": [\"B-Omission\", \"I-Omission\"], \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df = eval_df.copy()\n",
    "for label,tags in label_tags.items():\n",
    "    for tag in tags:\n",
    "        loose_eval_df[\"expected_tag\"] = loose_eval_df[\"expected_tag\"].replace(to_replace=tag, value=label)\n",
    "        loose_eval_df[\"predicted_tag\"] = loose_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df = loose_eval_df.fillna(\"O\")\n",
    "loose_eval_df = loose_eval_df.drop(columns=[\"_merge\"])\n",
    "loose_eval_df = utils.compareExpectedPredicted(loose_eval_df, \"_merge\", \"O\")\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt = pd.DataFrame.from_dict({\n",
    "        \"tag(s)\":[], \"false negative\":[], \"false positive\":[], \"true negative\":[], \n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>3370.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.299958</td>\n",
       "      <td>0.461489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>538.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.385845</td>\n",
       "      <td>0.556837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.192112</td>\n",
       "      <td>0.322305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tag(s)  false negative  false positive  true negative  true positive  \\\n",
       "0    Unknown          3370.0             0.0            0.0         1444.0   \n",
       "0   Feminine           538.0             0.0            0.0          338.0   \n",
       "0  Masculine          1270.0             0.0            0.0          302.0   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0        1.0  0.299958  0.461489  \n",
       "0        1.0  0.385845  0.556837  \n",
       "0        1.0  0.192112  0.322305  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label,tags in label_tags.items():\n",
    "    labels_agmt_stats = utils.getScoresByTags(loose_eval_df, \"_merge\", [label])\n",
    "    loose_agmt = pd.concat([loose_agmt, labels_agmt_stats])\n",
    "loose_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Classifier Chain, the Random Forest estimator performs better than the Passive Aggressive estimator.\n",
    "\n",
    "Compared to the Baseline Sequence Classifier for Person Names:\n",
    "* Unknown F1: 0.597172\n",
    "* Feminine F1: 0.767750\n",
    "* Masculine F1: 0.599679\n",
    "\n",
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_baseline_fastText{d}_pn_loose_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Linguistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multilabel models with the `PassiveAggressiveClassifier` for the Linguistic category of labels in order to compare their performance to the sequence classifier's performance (the Passive Aggressive algorithm was top-performing for the baseline Linguistic sequence classification model).\n",
    "\n",
    "#### Hypothesis\n",
    "* The baseline sequence classifiers will have worse performance (F1 score >=0.1 lower) than the baseline multilabel token classifiers for labels in the Linguistic category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ClassifierChain(classifier=PassiveAggressiveClassifier(loss='squared_hinge',\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"pa\"\n",
    "l_clf = ClassifierChain(classifier=PassiveAggressiveClassifier(\n",
    "        max_iter=100, \n",
    "        loss=\"squared_hinge\",  # equivalent to pa_type=2 (PA-II)\n",
    "        random_state=22,\n",
    "    )\n",
    ")\n",
    "# a = \"rf\"\n",
    "# l_clf = ClassifierChain(classifier = RandomForestClassifier(random_state=22))\n",
    "l_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = l_clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - micro: 0.9936454279837008\n",
      "Precision - macro: 0.5736935609975572\n",
      "\n",
      "Recall - micro: 0.9898131652277191\n",
      "Recall - macro: 0.45892771073303495\n",
      "\n",
      "F1 Score - micro: 0.9917255944266271\n",
      "F1 Score - macro: 0.5023357350074222\n",
      "\n",
      "Accuracy - normalized: 0.9886720671673609\n",
      "Accuracy - unnormalized: 150728\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - micro:\", metrics.precision_score(y_dev, predictions, average=\"micro\", zero_division=0))  # micro = calculated from TP, FP, FN sums across all labels\n",
    "print(\"Precision - macro:\", metrics.precision_score(y_dev, predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "print()\n",
    "print(\"Recall - micro:\", metrics.recall_score(y_dev, predictions, average=\"micro\", zero_division=0))\n",
    "print(\"Recall - macro:\", metrics.recall_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - micro:\", metrics.f1_score(y_dev, predictions, average=\"micro\", zero_division=0))\n",
    "print(\"F1 Score - macro:\", metrics.f1_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"Accuracy - normalized:\", metrics.accuracy_score(y_dev, predictions, normalize=True))  # fraction of correctly classified samples\n",
    "print(\"Accuracy - unnormalized:\", metrics.accuracy_score(y_dev, predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - per label: [0.80073126 0.48641304 0.01149425 0.99613569]\n",
      "\n",
      "Recall - per label: [0.5770751  0.25829726 0.0025974  0.99774108]\n",
      "\n",
      "F1 Score - per label: [0.67075038 0.33741753 0.00423729 0.99693774]\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - per label:\", metrics.precision_score(y_dev, predictions, average=None, zero_division=0))\n",
    "print()\n",
    "print(\"Recall - per label:\", metrics.recall_score(y_dev, predictions, average=None, zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - per label:\", metrics.f1_score(y_dev, predictions, average=None, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the scores above are calculated with the `\"O\"` label*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos                   tag  \\\n",
       "0            5       154       After    IN                   [O]   \n",
       "1            5       155         his  PRP$  [B-Gendered-Pronoun]   \n",
       "2            5       156  ordination    NN                   [O]   \n",
       "3            5       157          he   PRP  [B-Gendered-Pronoun]   \n",
       "4            5       158       spent   VBD                   [O]   \n",
       "\n",
       "    predicted_label  \n",
       "0                 O  \n",
       "1                 O  \n",
       "2                 O  \n",
       "3  Gendered-Pronoun  \n",
       "4                 O  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = utils.makePredictionDF(predictions, dev_data, \"label\", \"predicted_label\", \"O\", mlb)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = dev_data.explode([\"label\"])\n",
    "exp_df = exp_df.rename(columns={\"label\":\"expected_label\"})\n",
    "# exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>false negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos    expected_label  \\\n",
       "0            5       154       After    IN                 O   \n",
       "1            5       155         his  PRP$  Gendered-Pronoun   \n",
       "2            5       156  ordination    NN                 O   \n",
       "3            5       157          he   PRP  Gendered-Pronoun   \n",
       "4            5       158       spent   VBD                 O   \n",
       "\n",
       "    predicted_label          _merge  \n",
       "0                 O   true negative  \n",
       "1               NaN  false negative  \n",
       "2                 O   true negative  \n",
       "3  Gendered-Pronoun   true positive  \n",
       "4                 O   true negative  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = utils.makeEvaluationDataFrame(\n",
    "    exp_df, \n",
    "    pred_df, \n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_label\"],   # left on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"predicted_label\"],  # right on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_label\", \"predicted_label\", \"_merge\"],  # final column list\n",
    "    \"expected_label\",\n",
    "    \"predicted_label\", \n",
    "    \"token_id\",  # ID column\n",
    "    \"O\"          # No tag value\n",
    ")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_output/\").mkdir(parents=True, exist_ok=True)\n",
    "# eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_baseline_fastText{d}_predictions.csv\".format(a=a,d=d))\n",
    "eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_linguistic_baseline_fastText{d}_predictions.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strict Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_stats = utils.getAgreementStatsForAllTags(eval_df, \"_merge\", \"token_id\", \"label(s)\", y_dev, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ \n",
    "#     'Feminine', 'Masculine', 'Unknown',                      # Person Name category of labels\n",
    "    'Gendered-Pronoun','Gendered-Role', 'Generalization',   # Linguistic category of labels\n",
    "#     'Occupation', 'Omission', 'Stereotype'                  # Contextual category of labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>911</td>\n",
       "      <td>784</td>\n",
       "      <td>151117</td>\n",
       "      <td>603</td>\n",
       "      <td>0.573694</td>\n",
       "      <td>0.458928</td>\n",
       "      <td>0.502336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>286</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>876</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.753873</td>\n",
       "      <td>0.816403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>200</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.716157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>142</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.024390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label(s)  false negative  false positive  true negative  \\\n",
       "0               all             911             784         151117   \n",
       "0  Gendered-Pronoun             286             108              0   \n",
       "0     Gendered-Role             200              60              0   \n",
       "0    Generalization             142              18              0   \n",
       "\n",
       "   true positive  precision    recall        f1  \n",
       "0            603   0.573694  0.458928  0.502336  \n",
       "0            876   0.890244  0.753873  0.816403  \n",
       "0            328   0.845361  0.621212  0.716157  \n",
       "0              2   0.100000  0.013889  0.024390  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in labels:\n",
    "    label_agmt_stats = utils.getScoresByTags(eval_df, \"_merge\", [label], exp_col=\"expected_label\", pred_col=\"predicted_label\")\n",
    "    label_agmt_stats = label_agmt_stats.rename(columns={\"tag(s)\":\"label(s)\"})\n",
    "    agmt_stats = pd.concat([agmt_stats, label_agmt_stats])\n",
    "agmt_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_performance/\").mkdir(parents=True, exist_ok=True)\n",
    "# agmt_stats.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_baseline_fastText{d}_strict_agmt.csv\".format(a=a,d=d))\n",
    "agmt_stats.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_linguistic_baseline_fastText{d}_strict_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Note: code below is for tags, not labels*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loose Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each label, where a correct prediction is a prediction with the correct annotation label (not necessarily the correct IOB tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the evaluation DataFrame where tags are replaced by label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tags = {\n",
    "#     \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "    \"Gendered Pronoun\": [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"], \"Gendered Role\": [\"B-Gendered-Role\", \"I-Gendered-Role\"],\n",
    "    \"Generalization\": [\"B-Generalization\", \"I-Generalization\"], \n",
    "#     \"Stereotype\": [\"B-Stereotype\", \"I-Stereotype\"], \"Omission\": [\"B-Omission\", \"I-Omission\"], \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df = eval_df.copy()\n",
    "for label,tags in label_tags.items():\n",
    "    for tag in tags:\n",
    "        loose_eval_df[\"expected_tag\"] = loose_eval_df[\"expected_tag\"].replace(to_replace=tag, value=label)\n",
    "        loose_eval_df[\"predicted_tag\"] = loose_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df = loose_eval_df.fillna(\"O\")\n",
    "loose_eval_df = loose_eval_df.drop(columns=[\"_merge\"])\n",
    "loose_eval_df = utils.compareExpectedPredicted(loose_eval_df, \"_merge\", \"O\")\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt = pd.DataFrame.from_dict({\n",
    "        \"tag(s)\":[], \"false negative\":[], \"false positive\":[], \"true negative\":[], \n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered Pronoun</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1048.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833068</td>\n",
       "      <td>0.908933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered Role</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.564103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.423423</td>\n",
       "      <td>0.594937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag(s)  false negative  false positive  true negative  \\\n",
       "0  Gendered Pronoun           210.0             0.0            0.0   \n",
       "0     Gendered Role           204.0             0.0            0.0   \n",
       "0    Generalization           128.0             0.0            0.0   \n",
       "\n",
       "   true positive  precision    recall        f1  \n",
       "0         1048.0        1.0  0.833068  0.908933  \n",
       "0          132.0        1.0  0.392857  0.564103  \n",
       "0           94.0        1.0  0.423423  0.594937  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label,tags in label_tags.items():\n",
    "    labels_agmt_stats = utils.getScoresByTags(loose_eval_df, \"_merge\", [label])\n",
    "    loose_agmt = pd.concat([loose_agmt, labels_agmt_stats])\n",
    "loose_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Classifier Chain, the Random Forest estimator yields better results than the Passive Aggressive estimator.\n",
    "\n",
    "Compared to the Baseline Sequence Classifier:\n",
    "* Gendered-Pronoun F1: 0.872418\n",
    "* Gendered-Role F1: 0.659875\n",
    "* Generalization F1: 0.319392\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_baseline_fastText{d}_ling_loose_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

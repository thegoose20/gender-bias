{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Gender Bias Multilabel Token Classifiers with fastText\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/model_input/`\n",
    "    * Prediction Data: under directory `../data/token_clf_data/multilabel_model_output/`\n",
    "* Multilabel classification\n",
    "    * 3 categories of labels:\n",
    "        1. Person Name: Unknown, Feminine, Masculine (Non-binary not applied during annotation)\n",
    "        2. Linguistic: Generalization, Gendered Pronoun, Gendered Role\n",
    "        3. Contextual: Occupation, Omission, Stereotype (Empowering only applied by one annotator and too few times for training)\n",
    "* Word embeddings: custom fastText embeddings\n",
    "\n",
    "***\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[0.](#0) Preprocessing\n",
    "\n",
    "[1.](#CC) Classifier Chain Models\n",
    "\n",
    "[2.](#2) Person Name Model\n",
    "\n",
    "[3.](#3) Linguistic Model\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For embeddings\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For classification\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "# Base estimators\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## 0. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and validation (dev) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463441, 9) (156146, 9)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop(columns=[\"ann_id\"])\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev.drop(columns=[\"ann_id\"])\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Non-binary labels as these were mistaken labels identified early on that were meant to be excluded, and because only one token has this label, it prevents the data from being input into the models with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.tag != \"B-Nonbinary\"]\n",
    "df_train = df_train.loc[df_train.tag != \"I-Nonbinary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463439, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                     141279\n",
       "I-Unknown               3062\n",
       "B-Unknown               1886\n",
       "I-Omission              1502\n",
       "I-Masculine             1240\n",
       "B-Omission              1008\n",
       "B-Masculine              968\n",
       "I-Occupation             781\n",
       "B-Gendered-Pronoun       744\n",
       "I-Stereotype             721\n",
       "I-Feminine               675\n",
       "B-Occupation             654\n",
       "B-Gendered-Role          575\n",
       "B-Feminine               281\n",
       "B-Stereotype             252\n",
       "B-Generalization         240\n",
       "I-Generalization         145\n",
       "I-Gendered-Role          118\n",
       "I-Gendered-Pronoun        15\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152455\n"
     ]
    }
   ],
   "source": [
    "# 675+281 # 956 Feminine tags\n",
    "dev_token_ids = set(list(df_dev.token_id))\n",
    "print(len(dev_token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Optional Preprocessing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not classifying all labels at once, consider only the rows with tags for the select subset of labels, replacing all tags not in that subset with `\"O\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463439, 9) (156146, 9)\n"
     ]
    }
   ],
   "source": [
    "# cont_label_subset = [\"B-Stereotype\", \"I-Stereotype\", \"B-Omission\", \"I-Omission\"]\n",
    "perso_label_subset = [\"B-Unknown\", \"I-Unknown\", \"B-Feminine\", \"I-Feminine\", \"B-Masculine\", \"I-Masculine\", \"B-Occupation\", \"I-Occupation\"]#, \"B-Nonbinary\", \"I-Nonbinary\"]\n",
    "# category = \"pers_o\"\n",
    "ling_label_subset = [\"B-Generalization\", \"I-Generalization\", \"B-Gendered-Role\", \"I-Gendered-Role\", \"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"]\n",
    "category = \"linguistic\"\n",
    "# df_train, df_dev = utils.selectDataForLabels(df_train, df_dev, \"tag\", perso_label_subset)\n",
    "df_train, df_dev = utils.selectDataForLabels(df_train, df_dev, \"tag\", ling_label_subset)\n",
    "print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                     154309\n",
       "B-Gendered-Pronoun       744\n",
       "B-Gendered-Role          575\n",
       "B-Generalization         240\n",
       "I-Generalization         145\n",
       "I-Gendered-Role          118\n",
       "I-Gendered-Pronoun        15\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152455\n"
     ]
    }
   ],
   "source": [
    "dev_token_ids = set(list(df_dev.token_id))\n",
    "print(len(dev_token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by token, so there is one row per token rather than one row per token-tag pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos                   tag\n",
       "0            5       154       After    IN                   [O]\n",
       "1            5       155         his  PRP$  [B-Gendered-Pronoun]\n",
       "2            5       156  ordination    NN                   [O]\n",
       "3            5       157          he   PRP  [B-Gendered-Pronoun]\n",
       "4            5       158       spent   VBD                   [O]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_train = df_train.drop(columns=[\"description_id\", \"field\", \"subset\", \"token_offsets\"])\n",
    "subdf_dev = df_dev.drop(columns=[\"description_id\", \"field\", \"subset\", \"token_offsets\"])\n",
    "df_train_imploded = utils.implodeDataFrame(subdf_train, [\"sentence_id\", \"token_id\", \"token\", \"pos\"])\n",
    "df_train_imploded = df_train_imploded.reset_index()\n",
    "df_dev_imploded = utils.implodeDataFrame(subdf_dev, [\"sentence_id\", \"token_id\", \"token\", \"pos\"])\n",
    "df_dev_imploded = df_dev_imploded.reset_index()\n",
    "df_dev_imploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(list(df_dev_imploded.token_id))) == len(dev_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the tags with label names (remove ``B-`` and ``I-``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelColFromTagCol(df, col):\n",
    "    col_list = list(df[col])\n",
    "    new_col = []\n",
    "    for value_list in col_list:\n",
    "        new_value_list = []\n",
    "        for value in value_list:\n",
    "            if value != \"O\":\n",
    "                new_value = value[2:]\n",
    "                new_value_list += [new_value]\n",
    "            else:\n",
    "                new_value_list += [value]\n",
    "        # Remove any duplicates from the list of labels\n",
    "        unique_values = list(set(new_value_list))\n",
    "        # Remove \"O\" from the list if there are labels present\n",
    "        if (\"O\" in unique_values) and (len(unique_values) > 1):\n",
    "            unique_values.remove(\"O\")\n",
    "        # Sort the list of labels alphabetically\n",
    "        unique_values.sort()\n",
    "        new_col += [unique_values]\n",
    "    assert len(new_col) == len(col_list)\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = getLabelColFromTagCol(df_train_imploded, \"tag\")\n",
    "# # train_labels[:10]  # Looks good\n",
    "dev_labels = getLabelColFromTagCol(df_dev_imploded, \"tag\")\n",
    "# dev_labels[:10] # Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id   token  pos        tag label\n",
       "0            1         3   Title   NN        [O]   [O]\n",
       "1            1         4       :    :        [O]   [O]\n",
       "2            1         5  Papers  NNS        [O]   [O]\n",
       "3            1         6      of   IN        [O]   [O]\n",
       "4            1         7     The   DT  [O, O, O]   [O]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_imploded.insert(len(df_train_imploded.columns), \"label\", train_labels)\n",
    "df_train_imploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>[Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>[Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos                   tag  \\\n",
       "0            5       154       After    IN                   [O]   \n",
       "1            5       155         his  PRP$  [B-Gendered-Pronoun]   \n",
       "2            5       156  ordination    NN                   [O]   \n",
       "3            5       157          he   PRP  [B-Gendered-Pronoun]   \n",
       "4            5       158       spent   VBD                   [O]   \n",
       "\n",
       "                label  \n",
       "0                 [O]  \n",
       "1  [Gendered-Pronoun]  \n",
       "2                 [O]  \n",
       "3  [Gendered-Pronoun]  \n",
       "4                 [O]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_imploded.insert(len(df_dev_imploded.columns), \"label\", dev_labels)\n",
    "df_dev_imploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[O]                                   150657\n",
       "[Gendered-Pronoun]                       742\n",
       "[Gendered-Role]                          671\n",
       "[Generalization]                         346\n",
       "[Gendered-Role, Generalization]           22\n",
       "[Gendered-Pronoun, Generalization]        17\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_imploded.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2+5+32+171+266+468 # 944 tokens with Feminine label\n",
    "assert len(set(list(df_dev_imploded.token_id))) == len(dev_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate word embeddings to the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]\n",
    "d = dimensions[1]\n",
    "file_name = config.fasttext_path+\"fasttext{}_lowercased.model\".format(d)\n",
    "embedding_model = FastText.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 35968\n",
      "Lowercased vocabulary size: 31335\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(df_train.token.unique())\n",
    "vocabulary_lowercased = [token.lower() for token in vocabulary]\n",
    "vocabulary_lowercased = list(set(vocabulary_lowercased))\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"Lowercased vocabulary size:\", len(vocabulary_lowercased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize and binarize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"label\"\n",
    "feature_cols = [\"token_id\", \"token\"]\n",
    "train_data = df_train_imploded\n",
    "dev_data = df_dev_imploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vector representation of a token from a fastText word embedding model\n",
    "def extractEmbedding(token, fasttext_model=embedding_model):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    embedding = fasttext_model.wv[token]\n",
    "    return embedding\n",
    "\n",
    "def makeFeatureMatrix(token_data):\n",
    "    feature_list = [extractEmbedding(token) for token_id,token in token_data]\n",
    "    return np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = list(zip(train_data[feature_cols[0]], train_data[feature_cols[1]]))\n",
    "dev_tokens = list(zip(dev_data[feature_cols[0]], dev_data[feature_cols[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(452086, 100) (152455, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train = makeFeatureMatrix(train_tokens)\n",
    "X_dev = makeFeatureMatrix(dev_tokens)\n",
    "print(X_train.shape, X_dev.shape)  # number_of_samples, number_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarize targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(452086, 4) (152455, 4)\n"
     ]
    }
   ],
   "source": [
    "y_train_labels = train_data[target_col]\n",
    "y_train = mlb.fit_transform(y_train_labels)\n",
    "y_dev_labels = dev_data[target_col]\n",
    "y_dev = mlb.transform(y_dev_labels)\n",
    "print(y_train.shape, y_dev.shape)  # number_of_samples, number_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilabelled tokens exist, as expected.\n"
     ]
    }
   ],
   "source": [
    "for labels in y_train:\n",
    "    if sum(labels) > 1:\n",
    "        print(\"Multilabelled tokens exist, as expected.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152455, 6)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert dev_data.shape[0] == len(dev_token_ids)\n",
    "dev_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For baseline models, use only the tokens' embeddings as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"CC\"></a>\n",
    "## 1. Classifier Chain Model\n",
    "\n",
    "*Reference: http://scikit.ml/api/skmultilearn.problem_transform.cc.html#skmultilearn.problem_transform.ClassifierChain*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"pa\"  # \"rf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ClassifierChain(classifier=PassiveAggressiveClassifier(loss='squared_hinge',\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if a == \"rf\":\n",
    "    clf = ClassifierChain(\n",
    "        classifier = RandomForestClassifier(random_state=22),\n",
    "    )\n",
    "elif a == \"pa\":\n",
    "    clf = ClassifierChain(\n",
    "        classifier = PassiveAggressiveClassifier(\n",
    "            max_iter=100, \n",
    "            loss=\"squared_hinge\",  # equivalent to pa_type=2 (PA-II)\n",
    "            random_state=22,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"a not recognized:\", a)\n",
    "    \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - micro: 0.9898032118243398\n",
      "Precision - macro: 0.5727272579451423\n",
      "\n",
      "Recall - micro: 0.9898356656655344\n",
      "Recall - macro: 0.45894317902331494\n",
      "\n",
      "F1 Score - micro: 0.989819438478916\n",
      "F1 Score - macro: 0.5018585566374425\n",
      "\n",
      "Accuracy - normalized: 0.9897412351185596\n",
      "Accuracy - unnormalized: 150891\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - micro:\", metrics.precision_score(y_dev, predictions, average=\"micro\", zero_division=0))  # micro = calculated from TP, FP, FN sums across all labels\n",
    "print(\"Precision - macro:\", metrics.precision_score(y_dev, predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "print()\n",
    "print(\"Recall - micro:\", metrics.recall_score(y_dev, predictions, average=\"micro\", zero_division=0))\n",
    "print(\"Recall - macro:\", metrics.recall_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - micro:\", metrics.f1_score(y_dev, predictions, average=\"micro\", zero_division=0))\n",
    "print(\"F1 Score - macro:\", metrics.f1_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"Accuracy - normalized:\", metrics.accuracy_score(y_dev, predictions, normalize=True))  # fraction of correctly classified samples\n",
    "print(\"Accuracy - unnormalized:\", metrics.accuracy_score(y_dev, predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - per label: [0.80073126 0.48641304 0.01149425 0.99227047]\n",
      "\n",
      "Recall - per label: [0.5770751  0.25829726 0.0025974  0.99780296]\n",
      "\n",
      "F1 Score - per label: [0.67075038 0.33741753 0.00423729 0.99502902]\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - per label:\", metrics.precision_score(y_dev, predictions, average=None, zero_division=0))\n",
    "print()\n",
    "print(\"Recall - per label:\", metrics.recall_score(y_dev, predictions, average=None, zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - per label:\", metrics.f1_score(y_dev, predictions, average=None, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gendered-Pronoun' 'Gendered-Role' 'Generalization' 'O']\n"
     ]
    }
   ],
   "source": [
    "original_classes = mlb.classes_\n",
    "print(original_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_matrix = multilabel_confusion_matrix(y_dev, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = dev_matrix[:, 0, 0]\n",
    "tp = dev_matrix[:, 1, 1]\n",
    "fn = dev_matrix[:, 1, 0]\n",
    "fp = dev_matrix[:, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true positive</th>\n",
       "      <th>false positive</th>\n",
       "      <th>false negative</th>\n",
       "      <th>true negative</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gendered-Pronoun</th>\n",
       "      <td>438</td>\n",
       "      <td>109</td>\n",
       "      <td>321</td>\n",
       "      <td>151587</td>\n",
       "      <td>0.800731</td>\n",
       "      <td>0.577075</td>\n",
       "      <td>0.670750</td>\n",
       "      <td>759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gendered-Role</th>\n",
       "      <td>179</td>\n",
       "      <td>189</td>\n",
       "      <td>514</td>\n",
       "      <td>151573</td>\n",
       "      <td>0.486413</td>\n",
       "      <td>0.258297</td>\n",
       "      <td>0.337418</td>\n",
       "      <td>693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Generalization</th>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>384</td>\n",
       "      <td>151984</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>385.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>150326</td>\n",
       "      <td>1171</td>\n",
       "      <td>331</td>\n",
       "      <td>627</td>\n",
       "      <td>0.992270</td>\n",
       "      <td>0.997803</td>\n",
       "      <td>0.995029</td>\n",
       "      <td>150657.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.989803</td>\n",
       "      <td>0.989836</td>\n",
       "      <td>0.989819</td>\n",
       "      <td>152494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.572727</td>\n",
       "      <td>0.458943</td>\n",
       "      <td>0.501859</td>\n",
       "      <td>152494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.986542</td>\n",
       "      <td>0.989836</td>\n",
       "      <td>0.987925</td>\n",
       "      <td>152494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.989981</td>\n",
       "      <td>0.990017</td>\n",
       "      <td>0.989971</td>\n",
       "      <td>152494.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 true positive false positive false negative true negative  \\\n",
       "Gendered-Pronoun           438            109            321        151587   \n",
       "Gendered-Role              179            189            514        151573   \n",
       "Generalization               1             86            384        151984   \n",
       "O                       150326           1171            331           627   \n",
       "micro avg                    -              -              -             -   \n",
       "macro avg                    -              -              -             -   \n",
       "weighted avg                 -              -              -             -   \n",
       "samples avg                  -              -              -             -   \n",
       "\n",
       "                  precision    recall  f1-score   support  \n",
       "Gendered-Pronoun   0.800731  0.577075  0.670750     759.0  \n",
       "Gendered-Role      0.486413  0.258297  0.337418     693.0  \n",
       "Generalization     0.011494  0.002597  0.004237     385.0  \n",
       "O                  0.992270  0.997803  0.995029  150657.0  \n",
       "micro avg          0.989803  0.989836  0.989819  152494.0  \n",
       "macro avg          0.572727  0.458943  0.501859  152494.0  \n",
       "weighted avg       0.986542  0.989836  0.987925  152494.0  \n",
       "samples avg        0.989981  0.990017  0.989971  152494.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_report = pd.DataFrame(classification_report(y_dev,predictions, target_names=original_classes, digits=4, output_dict=True)).transpose()\n",
    "clf_report.insert(0, \"true negative\", list(tn)+([\"-\"]*4))\n",
    "clf_report.insert(0, \"false negative\", list(fn)+([\"-\"]*4))\n",
    "clf_report.insert(0, \"false positive\", list(fp)+([\"-\"]*4))\n",
    "clf_report.insert(0, \"true positive\", list(tp)+([\"-\"]*4))\n",
    "clf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the scores above are calculated with the `\"O\"` label*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label\n",
    "Calculate the performance metrics considering only the Taxonomy's labels to be positive classes (a.k.a. calculate without considering `\"O\"` a class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos                   tag  \\\n",
       "0            5       154       After    IN                   [O]   \n",
       "1            5       155         his  PRP$  [B-Gendered-Pronoun]   \n",
       "2            5       156  ordination    NN                   [O]   \n",
       "3            5       157          he   PRP  [B-Gendered-Pronoun]   \n",
       "4            5       158       spent   VBD                   [O]   \n",
       "\n",
       "    predicted_label  \n",
       "0                 O  \n",
       "1                 O  \n",
       "2                 O  \n",
       "3  Gendered-Pronoun  \n",
       "4                 O  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = utils.makePredictionDF(predictions, dev_data, \"label\", \"predicted_label\", \"O\", mlb)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = dev_data.explode([\"label\"])\n",
    "exp_df = exp_df.rename(columns={\"label\":\"expected_label\"})\n",
    "# exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(list(pred_df.token_id))) == len(set(list(exp_df.token_id)))\n",
    "assert len(set(list(pred_df.token_id))) == len(set(list(dev_data.token_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>O</td>\n",
       "      <td>false negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos    expected_label  \\\n",
       "0            5       154       After    IN                 O   \n",
       "1            5       155         his  PRP$  Gendered-Pronoun   \n",
       "2            5       156  ordination    NN                 O   \n",
       "3            5       157          he   PRP  Gendered-Pronoun   \n",
       "4            5       158       spent   VBD                 O   \n",
       "\n",
       "    predicted_label          _merge  \n",
       "0                 O   true negative  \n",
       "1                 O  false negative  \n",
       "2                 O   true negative  \n",
       "3  Gendered-Pronoun   true positive  \n",
       "4                 O   true negative  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = utils.makeEvaluationDataFrame(\n",
    "    exp_df, \n",
    "    pred_df, \n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_label\"],   # left on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"predicted_label\"],  # right on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_label\", \"predicted_label\", \"_merge\"],  # final column list\n",
    "    \"expected_label\",\n",
    "    \"predicted_label\", \n",
    "    \"token_id\",  # ID column\n",
    "    \"O\"          # No tag value\n",
    ")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154049, 7)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_output/\").mkdir(parents=True, exist_ok=True)\n",
    "eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_{c}_baseline_fastText{d}_predictions.csv\".format(a=a,d=d,c=category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strict Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if category == \"linguistic\":\n",
    "    labels = ['Gendered-Pronoun','Gendered-Role', 'Generalization']   # Linguistic category of labels\n",
    "elif category == \"pers_o\":\n",
    "    labels = ['Feminine', 'Masculine', 'Unknown', 'Occupation']      # Person Name category of labels\n",
    "else:\n",
    "    print(\"Category not recognized:\",category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>321</td>\n",
       "      <td>109</td>\n",
       "      <td>438</td>\n",
       "      <td>0.800731</td>\n",
       "      <td>0.577075</td>\n",
       "      <td>0.670750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>514</td>\n",
       "      <td>189</td>\n",
       "      <td>179</td>\n",
       "      <td>0.486413</td>\n",
       "      <td>0.258297</td>\n",
       "      <td>0.337418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>384</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.004237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label  false negative  false positive  true positive  precision  \\\n",
       "0  Gendered-Pronoun             321             109            438   0.800731   \n",
       "0     Gendered-Role             514             189            179   0.486413   \n",
       "0    Generalization             384              86              1   0.011494   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.577075  0.670750  \n",
       "0  0.258297  0.337418  \n",
       "0  0.002597  0.004237  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agmt_scores = pd.DataFrame()\n",
    "exp_col, pred_col = \"expected_label\", \"predicted_label\"\n",
    "for label in labels:\n",
    "    agmt_df = pd.concat([eval_df.loc[eval_df[exp_col] == label], eval_df.loc[eval_df[pred_col] == label]])\n",
    "    agmt_df = agmt_df.drop_duplicates() # True positives will have been duplicated in line above\n",
    "    tp = agmt_df.loc[agmt_df._merge == \"true positive\"].shape[0]\n",
    "    fp = agmt_df.loc[agmt_df._merge == \"false positive\"].shape[0]\n",
    "    fn = agmt_df.loc[agmt_df._merge == \"false negative\"].shape[0]\n",
    "    prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "    label_agmt = pd.DataFrame.from_dict({\n",
    "            \"label\":[label], \"false negative\":[fn], \"false positive\":[fp],\n",
    "             \"true positive\":[tp], \"precision\":[prec], \"recall\":[rec], \"f1\":[f1]\n",
    "        })\n",
    "    agmt_scores = pd.concat([agmt_scores, label_agmt])\n",
    "agmt_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_performance/\").mkdir(parents=True, exist_ok=True)\n",
    "agmt_scores.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_{c}_baseline_fastText{d}_strict_agmt.csv\".format(a=a,d=d, c=category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "***\n",
    "*Note: models below use tags as targets instead of labels*\n",
    "\n",
    "## 2. Person Name Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multilabel models with the `PassiveAggressiveClassifier` for the Person Name category of labels in order to compare their performance to the sequence classifier's performance.\n",
    "\n",
    "Then, try the `RandomForestClassifier` since this yeilded high performance in the optimization experiments.\n",
    "\n",
    "#### Hypothesis\n",
    "* The baseline sequence classifiers will outperform (F1 score >0.1 higher) the baseline multilabel token classifiers for labels in the Person Name category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ClassifierChain(classifier=PassiveAggressiveClassifier(loss='squared_hinge',\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"pa\"\n",
    "pn_clf = ClassifierChain(classifier=PassiveAggressiveClassifier(\n",
    "        max_iter=100, \n",
    "        loss=\"squared_hinge\",  # equivalent to pa_type=2 (PA-II)\n",
    "        random_state=22,\n",
    "    )\n",
    ")\n",
    "# a = \"rf\"\n",
    "# pn_clf = ClassifierChain(classifier=RandomForestClassifier(random_state=22))\n",
    "pn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pn_clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - weighted: 0.9040107294821986\n",
      "Precision - macro: 0.2342654641746455\n",
      "\n",
      "Recall - weighted: 0.8862445780186117\n",
      "Recall - macro: 0.3036530273250453\n",
      "\n",
      "F1 Score - weighted: 0.8930894528962644\n",
      "F1 Score - macro: 0.2420080946447515\n",
      "\n",
      "Accuracy - normalized: 0.820845495392083\n",
      "Accuracy - unnormalized: 125142\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - weighted:\", metrics.precision_score(y_dev, predictions, average=\"weighted\", zero_division=0))\n",
    "print(\"Precision - macro:\", metrics.precision_score(y_dev, predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "print()\n",
    "print(\"Recall - weighted:\", metrics.recall_score(y_dev, predictions, average=\"weighted\", zero_division=0))\n",
    "print(\"Recall - macro:\", metrics.recall_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - weighted:\", metrics.f1_score(y_dev, predictions, average=\"weighted\", zero_division=0))\n",
    "print(\"F1 Score - macro:\", metrics.f1_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"Accuracy - normalized:\", metrics.accuracy_score(y_dev, predictions, normalize=True))  # fraction of correctly classified samples\n",
    "print(\"Accuracy - unnormalized:\", metrics.accuracy_score(y_dev, predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[O]</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos label predicted_tag\n",
       "0            5       154       After    IN   [O]             O\n",
       "1            5       155         his  PRP$   [O]             O\n",
       "2            5       156  ordination    NN   [O]             O\n",
       "3            5       157          he   PRP   [O]     Masculine\n",
       "4            5       158       spent   VBD   [O]             O"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = utils.makePredictionDF(predictions, dev_data, \"tag\", \"predicted_tag\", \"O\", mlb)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = dev_data.explode([\"tag\"])\n",
    "exp_df = exp_df.rename(columns={\"tag\":\"expected_tag\"})\n",
    "# exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_tag</th>\n",
       "      <th>predicted_tag</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos expected_tag predicted_tag  \\\n",
       "0            5       154       After    IN            O             O   \n",
       "1            5       155         his  PRP$            O             O   \n",
       "2            5       156  ordination    NN            O             O   \n",
       "3            5       157          he   PRP            O             O   \n",
       "4            5       158       spent   VBD            O             O   \n",
       "\n",
       "          _merge  \n",
       "0  true negative  \n",
       "1  true negative  \n",
       "2  true negative  \n",
       "3      left_only  \n",
       "4  true negative  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = utils.makeEvaluationDataFrame(\n",
    "    exp_df, \n",
    "    pred_df, \n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_tag\"],   # left on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"predicted_tag\"],  # right on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_tag\", \"predicted_tag\", \"_merge\"],  # final column list\n",
    "    \"expected_tag\",\n",
    "    \"predicted_tag\", \n",
    "    \"token_id\",  # ID column\n",
    "    \"O\"          # No tag value\n",
    ")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_output/\").mkdir(parents=True, exist_ok=True)\n",
    "eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_baseline_fastText{d}_pn_predictions.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strict Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_stats = utils.getAgreementStatsForAllTags(eval_df, \"_merge\", \"token_id\", \"tag(s)\", y_dev, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tags = [ \n",
    "    'B-Unknown', 'I-Unknown', 'B-Feminine', 'I-Feminine', 'B-Masculine',  'I-Masculine',\n",
    "#     'B-Gendered-Pronoun', 'I-Gendered-Pronoun','B-Gendered-Role', 'I-Gendered-Role', \n",
    "#     'B-Generalization', 'I-Generalization', \n",
    "#     'B-Stereotype', 'I-Stereotype', 'B-Omission', 'I-Omission', 'B-Occupation', 'I-Occupation'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>left_only</th>\n",
       "      <th>right_only</th>\n",
       "      <th>true negative</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>true positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>691</td>\n",
       "      <td>680</td>\n",
       "      <td>5280.0</td>\n",
       "      <td>5971.0</td>\n",
       "      <td>141319</td>\n",
       "      <td>0.234265</td>\n",
       "      <td>0.303653</td>\n",
       "      <td>0.242008</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tag(s)  false negative  false positive  left_only  right_only  \\\n",
       "0          all             691             680     5280.0      5971.0   \n",
       "0    B-Unknown             221               0        NaN         NaN   \n",
       "0    I-Unknown             216               0        NaN         NaN   \n",
       "0   B-Feminine              30               0        NaN         NaN   \n",
       "0   I-Feminine              22               0        NaN         NaN   \n",
       "0  B-Masculine              75               0        NaN         NaN   \n",
       "0  I-Masculine              85               0        NaN         NaN   \n",
       "\n",
       "   true negative  precision    recall        f1  true positive  \n",
       "0         141319   0.234265  0.303653  0.242008            NaN  \n",
       "0              0   0.000000  0.000000  0.000000            0.0  \n",
       "0              0   0.000000  0.000000  0.000000            0.0  \n",
       "0              0   0.000000  0.000000  0.000000            0.0  \n",
       "0              0   0.000000  0.000000  0.000000            0.0  \n",
       "0              0   0.000000  0.000000  0.000000            0.0  \n",
       "0              0   0.000000  0.000000  0.000000            0.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label_tag in label_tags:\n",
    "    label_agmt_stats = utils.getScoresByTags(eval_df, \"_merge\", [label_tag])\n",
    "    agmt_stats = pd.concat([agmt_stats, label_agmt_stats])\n",
    "agmt_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_performance/\").mkdir(parents=True, exist_ok=True)\n",
    "agmt_stats.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_baseline_fastText{d}_pn_strict_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loose Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each label, where a correct prediction is a prediction with the correct annotation label (not necessarily the correct IOB tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the evaluation DataFrame where tags are replaced by label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tags = {\n",
    "    \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "#     \"Gendered Pronoun\": [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"], \"Gendered Role\": [\"B-Gendered-Role\", \"I-Gendered-Role\"],\n",
    "#     \"Generalization\": [\"B-Generalization\", \"I-Generalization\"], \n",
    "#     \"Stereotype\": [\"B-Stereotype\", \"I-Stereotype\"], \"Omission\": [\"B-Omission\", \"I-Omission\"], \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df = eval_df.copy()\n",
    "for label,tags in label_tags.items():\n",
    "    for tag in tags:\n",
    "        loose_eval_df[\"expected_tag\"] = loose_eval_df[\"expected_tag\"].replace(to_replace=tag, value=label)\n",
    "        loose_eval_df[\"predicted_tag\"] = loose_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df = loose_eval_df.fillna(\"O\")\n",
    "loose_eval_df = loose_eval_df.drop(columns=[\"_merge\"])\n",
    "loose_eval_df = utils.compareExpectedPredicted(loose_eval_df, \"_merge\", \"O\")\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt = pd.DataFrame.from_dict({\n",
    "        \"tag(s)\":[], \"false negative\":[], \"false positive\":[], \"true negative\":[], \n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>437.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tag(s)  false negative  false positive  true negative  true positive  \\\n",
       "0    Unknown           437.0             0.0            0.0            0.0   \n",
       "0   Feminine            52.0             0.0            0.0            0.0   \n",
       "0  Masculine           160.0             0.0            0.0            0.0   \n",
       "\n",
       "   precision  recall   f1  \n",
       "0        0.0     0.0  0.0  \n",
       "0        0.0     0.0  0.0  \n",
       "0        0.0     0.0  0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label,tags in label_tags.items():\n",
    "    labels_agmt_stats = utils.getScoresByTags(loose_eval_df, \"_merge\", [label])\n",
    "    loose_agmt = pd.concat([loose_agmt, labels_agmt_stats])\n",
    "loose_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Classifier Chain, the Random Forest estimator performs better than the Passive Aggressive estimator.\n",
    "\n",
    "Compared to the Baseline Sequence Classifier for Person Names:\n",
    "* Unknown F1: 0.597172\n",
    "* Feminine F1: 0.767750\n",
    "* Masculine F1: 0.599679\n",
    "\n",
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_baseline_fastText{d}_pn_loose_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Linguistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multilabel models with the `PassiveAggressiveClassifier` for the Linguistic category of labels in order to compare their performance to the sequence classifier's performance (the Passive Aggressive algorithm was top-performing for the baseline Linguistic sequence classification model).\n",
    "\n",
    "#### Hypothesis\n",
    "* The baseline sequence classifiers will have worse performance (F1 score >=0.1 lower) than the baseline multilabel token classifiers for labels in the Linguistic category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ClassifierChain</label><div class=\"sk-toggleable__content\"><pre>ClassifierChain(classifier=PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;,\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">classifier: PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PassiveAggressiveClassifier</label><div class=\"sk-toggleable__content\"><pre>PassiveAggressiveClassifier(loss=&#x27;squared_hinge&#x27;, max_iter=100, random_state=22)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ClassifierChain(classifier=PassiveAggressiveClassifier(loss='squared_hinge',\n",
       "                                                       max_iter=100,\n",
       "                                                       random_state=22),\n",
       "                require_dense=[True, True])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"pa\"\n",
    "l_clf = ClassifierChain(classifier=PassiveAggressiveClassifier(\n",
    "        max_iter=100, \n",
    "        loss=\"squared_hinge\",  # equivalent to pa_type=2 (PA-II)\n",
    "        random_state=22,\n",
    "    )\n",
    ")\n",
    "# a = \"rf\"\n",
    "# l_clf = ClassifierChain(classifier = RandomForestClassifier(random_state=22))\n",
    "l_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = l_clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - micro: 0.8578084802880332\n",
      "Precision - macro: 0.2342654641746455\n",
      "\n",
      "Recall - micro: 0.8862445780186117\n",
      "Recall - macro: 0.3036530273250453\n",
      "\n",
      "F1 Score - micro: 0.8717947094703455\n",
      "F1 Score - macro: 0.2420080946447515\n",
      "\n",
      "Accuracy - normalized: 0.820845495392083\n",
      "Accuracy - unnormalized: 125142\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - micro:\", metrics.precision_score(y_dev, predictions, average=\"micro\", zero_division=0))  # micro = calculated from TP, FP, FN sums across all labels\n",
    "print(\"Precision - macro:\", metrics.precision_score(y_dev, predictions, average=\"macro\", zero_division=0))  # macro = mean of all labels' score\n",
    "print()\n",
    "print(\"Recall - micro:\", metrics.recall_score(y_dev, predictions, average=\"micro\", zero_division=0))\n",
    "print(\"Recall - macro:\", metrics.recall_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - micro:\", metrics.f1_score(y_dev, predictions, average=\"micro\", zero_division=0))\n",
    "print(\"F1 Score - macro:\", metrics.f1_score(y_dev, predictions, average=\"macro\", zero_division=0))\n",
    "print()\n",
    "print(\"Accuracy - normalized:\", metrics.accuracy_score(y_dev, predictions, normalize=True))  # fraction of correctly classified samples\n",
    "print(\"Accuracy - unnormalized:\", metrics.accuracy_score(y_dev, predictions, normalize=False))  # number of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision - per label: [0.04012211 0.08160769 0.95865737 0.01582278 0.07511737]\n",
      "\n",
      "Recall - per label: [0.09745763 0.42551253 0.93535673 0.02090592 0.03903232]\n",
      "\n",
      "F1 Score - per label: [0.05684276 0.13695015 0.94686372 0.01801261 0.05137124]\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision - per label:\", metrics.precision_score(y_dev, predictions, average=None, zero_division=0))\n",
    "print()\n",
    "print(\"Recall - per label:\", metrics.recall_score(y_dev, predictions, average=None, zero_division=0))\n",
    "print()\n",
    "print(\"F1 Score - per label:\", metrics.f1_score(y_dev, predictions, average=None, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the scores above are calculated with the `\"O\"` label*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Each Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[O]</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos  tag predicted_label\n",
       "0            5       154       After    IN  [O]               O\n",
       "1            5       155         his  PRP$  [O]               O\n",
       "2            5       156  ordination    NN  [O]               O\n",
       "3            5       157          he   PRP  [O]       Masculine\n",
       "4            5       158       spent   VBD  [O]               O"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = utils.makePredictionDF(predictions, dev_data, \"label\", \"predicted_label\", \"O\", mlb)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = dev_data.explode([\"label\"])\n",
    "exp_df = exp_df.rename(columns={\"label\":\"expected_label\"})\n",
    "# exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>expected_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id       token   pos expected_label predicted_label  \\\n",
       "0            5       154       After    IN              O               O   \n",
       "1            5       155         his  PRP$              O               O   \n",
       "2            5       156  ordination    NN              O               O   \n",
       "3            5       157          he   PRP              O               O   \n",
       "4            5       158       spent   VBD              O               O   \n",
       "\n",
       "          _merge  \n",
       "0  true negative  \n",
       "1  true negative  \n",
       "2  true negative  \n",
       "3      left_only  \n",
       "4  true negative  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = utils.makeEvaluationDataFrame(\n",
    "    exp_df, \n",
    "    pred_df, \n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_label\"],   # left on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"predicted_label\"],  # right on\n",
    "    [\"sentence_id\", \"token_id\", \"token\", \"pos\", \"expected_label\", \"predicted_label\", \"_merge\"],  # final column list\n",
    "    \"expected_label\",\n",
    "    \"predicted_label\", \n",
    "    \"token_id\",  # ID column\n",
    "    \"O\"          # No tag value\n",
    ")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_output/\").mkdir(parents=True, exist_ok=True)\n",
    "# eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_baseline_fastText{d}_predictions.csv\".format(a=a,d=d))\n",
    "eval_df.to_csv(config.tokc_path+\"multilabel_model_output/cc-{a}_linguistic_baseline_fastText{d}_predictions.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strict Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_stats = utils.getAgreementStatsForAllTags(eval_df, \"_merge\", \"token_id\", \"label(s)\", y_dev, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ \n",
    "#     'Feminine', 'Masculine', 'Unknown',                      # Person Name category of labels\n",
    "    'Gendered-Pronoun','Gendered-Role', 'Generalization',   # Linguistic category of labels\n",
    "#     'Occupation', 'Omission', 'Stereotype'                  # Contextual category of labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>left_only</th>\n",
       "      <th>right_only</th>\n",
       "      <th>true negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>393</td>\n",
       "      <td>388</td>\n",
       "      <td>5155.0</td>\n",
       "      <td>5971.0</td>\n",
       "      <td>140738</td>\n",
       "      <td>292</td>\n",
       "      <td>0.234265</td>\n",
       "      <td>0.303653</td>\n",
       "      <td>0.242008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label(s)  false negative  false positive  left_only  right_only  \\\n",
       "0               all             393             388     5155.0      5971.0   \n",
       "0  Gendered-Pronoun               0               0        NaN         NaN   \n",
       "0     Gendered-Role               0               0        NaN         NaN   \n",
       "0    Generalization               0               0        NaN         NaN   \n",
       "\n",
       "   true negative  true positive  precision    recall        f1  \n",
       "0         140738            292   0.234265  0.303653  0.242008  \n",
       "0              0              0   0.000000  0.000000  0.000000  \n",
       "0              0              0   0.000000  0.000000  0.000000  \n",
       "0              0              0   0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in labels:\n",
    "    label_agmt_stats = utils.getScoresByTags(eval_df, \"_merge\", [label], exp_col=\"expected_label\", pred_col=\"predicted_label\")\n",
    "    label_agmt_stats = label_agmt_stats.rename(columns={\"tag(s)\":\"label(s)\"})\n",
    "    agmt_stats = pd.concat([agmt_stats, label_agmt_stats])\n",
    "agmt_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(config.tokc_path+\"multilabel_model_performance/\").mkdir(parents=True, exist_ok=True)\n",
    "# agmt_stats.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_baseline_fastText{d}_strict_agmt.csv\".format(a=a,d=d))\n",
    "agmt_stats.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_linguistic_baseline_fastText{d}_strict_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Note: code below is for tags, not labels*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loose Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 score at the token level for each label, where a correct prediction is a prediction with the correct annotation label (not necessarily the correct IOB tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the evaluation DataFrame where tags are replaced by label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tags = {\n",
    "#     \"Unknown\": [\"B-Unknown\", \"I-Unknown\"], \"Feminine\": [\"B-Feminine\", \"I-Feminine\"], \"Masculine\": [\"B-Masculine\", \"I-Masculine\"],\n",
    "    \"Gendered Pronoun\": [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\"], \"Gendered Role\": [\"B-Gendered-Role\", \"I-Gendered-Role\"],\n",
    "    \"Generalization\": [\"B-Generalization\", \"I-Generalization\"], \n",
    "#     \"Stereotype\": [\"B-Stereotype\", \"I-Stereotype\"], \"Omission\": [\"B-Omission\", \"I-Omission\"], \"Occupation\": [\"B-Occupation\", \"I-Occupation\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'expected_tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gender-bias/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/gender-bias/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/gender-bias/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'expected_tag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label,tags \u001b[38;5;129;01min\u001b[39;00m label_tags\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags:\n\u001b[0;32m----> 4\u001b[0m         loose_eval_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mloose_eval_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpected_tag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(to_replace\u001b[38;5;241m=\u001b[39mtag, value\u001b[38;5;241m=\u001b[39mlabel)\n\u001b[1;32m      5\u001b[0m         loose_eval_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loose_eval_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(to_replace\u001b[38;5;241m=\u001b[39mtag, value\u001b[38;5;241m=\u001b[39mlabel)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# loose_eval_df.head()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gender-bias/lib/python3.9/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/gender-bias/lib/python3.9/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'expected_tag'"
     ]
    }
   ],
   "source": [
    "loose_eval_df = eval_df.copy()\n",
    "for label,tags in label_tags.items():\n",
    "    for tag in tags:\n",
    "        loose_eval_df[\"expected_tag\"] = loose_eval_df[\"expected_tag\"].replace(to_replace=tag, value=label)\n",
    "        loose_eval_df[\"predicted_tag\"] = loose_eval_df[\"predicted_tag\"].replace(to_replace=tag, value=label)\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_eval_df = loose_eval_df.fillna(\"O\")\n",
    "loose_eval_df = loose_eval_df.drop(columns=[\"_merge\"])\n",
    "loose_eval_df = utils.compareExpectedPredicted(loose_eval_df, \"_merge\", \"O\")\n",
    "# loose_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt = pd.DataFrame.from_dict({\n",
    "        \"tag(s)\":[], \"false negative\":[], \"false positive\":[], \"true negative\":[], \n",
    "         \"true positive\":[], \"precision\":[], \"recall\":[], \"f1\":[]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label,tags in label_tags.items():\n",
    "    labels_agmt_stats = utils.getScoresByTags(loose_eval_df, \"_merge\", [label])\n",
    "    loose_agmt = pd.concat([loose_agmt, labels_agmt_stats])\n",
    "loose_agmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Classifier Chain, the Random Forest estimator yields better results than the Passive Aggressive estimator.\n",
    "\n",
    "Compared to the Baseline Sequence Classifier:\n",
    "* Gendered-Pronoun F1: 0.872418\n",
    "* Gendered-Role F1: 0.659875\n",
    "* Generalization F1: 0.319392\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_agmt.to_csv(config.tokc_path+\"multilabel_model_performance/cc-{a}_baseline_fastText{d}_ling_loose_agmt.csv\".format(a=a,d=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

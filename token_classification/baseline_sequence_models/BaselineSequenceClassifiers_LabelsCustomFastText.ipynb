{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Gender Biased Sequence Classifiers\n",
    "\n",
    "### Target: Labels\n",
    "\n",
    "### Features: Word Embeddings\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/model_input/`\n",
    "    * Prediction Data: Data: under directory `../data/token_clf_data/model_output/crf_l2sgd_baseline/`\n",
    "* Sequence classification\n",
    "    * 3 categories, 9 lables (2 from original annotation taxonomy weren't applied during manual annotation):\n",
    "        1. Person Name: Unknown, Feminine, Masculine (Non-binary not applied during annotation)\n",
    "        2. Linguistic: Generalization, Gendered Pronoun, Gendered Role\n",
    "        3. Contextual: Occupation, Omission, Stereotype (Empowering only applied by one annotator and too few times for training)\n",
    "    * 1 model per category\n",
    "* Word embeddings\n",
    "    * Custom fastText (word2vec with subwords, trained on Archives' descriptive metadata extracted in October 2020)  \n",
    "\n",
    "***\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[0.](#0) Preprocessing\n",
    "\n",
    "[1.](#1) Models\n",
    "\n",
    "[2.](#2) Performance Evaluation\n",
    "\n",
    "[Appendix A](#A) Person Name Model Optimization\n",
    "\n",
    "[Appendix B](#B) Linguistic Model Optimization\n",
    "\n",
    "[Appendix C](#C) Contextual Model Optimization\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For preprocessing\n",
    "import scipy.stats\n",
    "from gensim.models import FastText\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "# For classification\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# For evaluation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, ConfusionMatrixDisplay#, plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from intervaltree import Interval, IntervalTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## 0. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and validation (dev) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicate rows with all but the same annotation ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463441, 9) (156146, 9)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop(columns=[\"ann_id\"])\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev.drop(columns=[\"ann_id\"])\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Non-binary labels as these were mistaken labels identified early on that were meant to be excluded, and because only one token has this label, it prevents the data from being input into the models with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.tag != \"B-Nonbinary\"]\n",
    "df_train = df_train.loc[df_train.tag != \"I-Nonbinary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463439, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that won't be used as features for the classifiers and remove any duplicate rows that remain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"sentence_id\", \"token_id\", \"pos\", \"token\", \"tag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[cols_to_keep]\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev[cols_to_keep]\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "# df_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create separate subsets of data for each category so they can be used with three separate models, replacing `NaN` tag values with `'O'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Feminine' 'B-Gendered-Pronoun' 'B-Gendered-Role' 'B-Generalization'\n",
      " 'B-Masculine' 'B-Occupation' 'B-Omission' 'B-Stereotype' 'B-Unknown'\n",
      " 'I-Feminine' 'I-Gendered-Pronoun' 'I-Gendered-Role' 'I-Generalization'\n",
      " 'I-Masculine' 'I-Occupation' 'I-Omission' 'I-Stereotype' 'I-Unknown' 'O']\n"
     ]
    }
   ],
   "source": [
    "tags = (df_train.tag.unique())\n",
    "tags.sort()\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_cat_tags = ['B-Gendered-Pronoun', 'B-Gendered-Role', 'B-Generalization', 'I-Gendered-Pronoun', 'I-Gendered-Role', 'I-Generalization']\n",
    "df_train_ling = df_train.loc[df_train.tag.isin(ling_cat_tags)]\n",
    "df_dev_ling = df_dev.loc[df_dev.tag.isin(ling_cat_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_cat_tags = ['B-Feminine', 'B-Masculine', 'B-Unknown', 'I-Feminine', 'I-Masculine', 'I-Unknown']\n",
    "df_train_pers = df_train.loc[df_train.tag.isin(pers_cat_tags)]\n",
    "df_dev_pers = df_dev.loc[df_dev.tag.isin(pers_cat_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cat_tags = ['B-Occupation', 'B-Omission', 'B-Stereotype', 'I-Occupation', 'I-Omission', 'I-Stereotype']\n",
    "df_train_cont = df_train.loc[df_train.tag.isin(cont_cat_tags)]\n",
    "df_dev_cont = df_dev.loc[df_dev.tag.isin(cont_cat_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = (df_train.drop(columns=[\"tag\"])).drop_duplicates()\n",
    "df_dev = (df_dev.drop(columns=[\"tag\"])).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_cols = [\"sentence_id\", \"token_id\", \"pos\", \"token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ling = df_train.join(df_train_ling.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "df_train_ling = df_train_ling.rename(columns={\"tag\":\"tag_linguistic\"})\n",
    "df_train_ling = df_train_ling.fillna('O')\n",
    "# df_train_ling.head()\n",
    "df_dev_ling = df_dev.join(df_dev_ling.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "df_dev_ling = df_dev_ling.rename(columns={\"tag\":\"tag_linguistic\"})\n",
    "df_dev_ling = df_dev_ling.fillna('O')\n",
    "# df_dev_ling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pers = df_train.join(df_train_pers.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "df_train_pers = df_train_pers.rename(columns={\"tag\":\"tag_personname\"})\n",
    "df_train_pers = df_train_pers.fillna('O')\n",
    "df_dev_pers = df_dev.join(df_dev_pers.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "df_dev_pers = df_dev_pers.rename(columns={\"tag\":\"tag_personname\"})\n",
    "df_dev_pers = df_dev_pers.fillna('O')\n",
    "# df_dev_pers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_contextual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NN</td>\n",
       "      <td>Title</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NNS</td>\n",
       "      <td>Papers</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>DT</td>\n",
       "      <td>The</td>\n",
       "      <td>B-Stereotype</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id  pos   token tag_contextual\n",
       "3            1         3   NN   Title              O\n",
       "4            1         4    :       :              O\n",
       "5            1         5  NNS  Papers              O\n",
       "6            1         6   IN      of              O\n",
       "7            1         7   DT     The   B-Stereotype"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cont = df_train.join(df_train_cont.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "df_train_cont = df_train_cont.rename(columns={\"tag\":\"tag_contextual\"})\n",
    "df_train_cont = df_train_cont.fillna('O')\n",
    "df_dev_cont = df_dev.join(df_dev_cont.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "df_dev_cont = df_dev_cont.rename(columns={\"tag\":\"tag_contextual\"})\n",
    "df_dev_cont = df_dev_cont.fillna('O')\n",
    "df_train_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ling = df_train_ling.drop_duplicates()\n",
    "df_dev_ling = df_dev_ling.drop_duplicates()\n",
    "df_train_pers = df_train_pers.drop_duplicates()\n",
    "df_dev_pers = df_dev_pers.drop_duplicates()\n",
    "df_train_cont = df_train_cont.drop_duplicates()\n",
    "df_dev_cont = df_dev_cont.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452222 452086\n",
      "455327 452086\n",
      "453119 452086\n",
      "\n",
      "152494 152455\n",
      "153568 152455\n",
      "152768 152455\n"
     ]
    }
   ],
   "source": [
    "train_dfs = [df_train_ling, df_train_pers, df_train_cont]\n",
    "dev_dfs = [df_dev_ling, df_dev_pers, df_dev_cont]\n",
    "for df in train_dfs:\n",
    "    print(df.shape[0], len(df.token_id.unique()))\n",
    "print()\n",
    "for df in dev_dfs:\n",
    "    print(df.shape[0], len(df.token_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens can have multiple tags, so there are more rows than unique token IDs.  In order to pass the data into a CRF model, we need to have one tag per token, so we'll simply **take the first tag** when we extract features for each token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "Use the custom fastText word embeddings, trained on the entire dataset of descriptive metadata from the Archives (harvested in October 2020) using the Continuous Bag-of-Words (CBOW) algorithm.  Subword embeddings (for subwords from 2 to 6 characters long, inclusive) are used to infer the embeddings for out-of-vocabulary (OOV) words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the word embedding model trained on lowercased text to 100 dimensions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]\n",
    "d = dimensions[1]\n",
    "file_name = config.fasttext_path+\"fasttext{}_lowercased.model\".format(d)  #get_tmpfile()\n",
    "embedding_model = FastText.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 35968\n",
      "Lowercased vocabulary size: 31335\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(df_train.token.unique())\n",
    "vocabulary_lowercased = [token.lower() for token in vocabulary]\n",
    "vocabulary_lowercased = list(set(vocabulary_lowercased))\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"Lowercased vocabulary size:\", len(vocabulary_lowercased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define feature dictionaries for baseline models, using only the word embeddings and token as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vector representation of a token from a fastText word embedding model\n",
    "def extractEmbedding(token, fasttext_model=embedding_model):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    embedding = fasttext_model.wv[token]\n",
    "    return embedding\n",
    "\n",
    "def extractTokenFeatures(sentence, i):\n",
    "    token = sentence[i][0]\n",
    "    pos = sentence[i][1]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'token': token\n",
    "    }\n",
    "    \n",
    "    # Add each value in a token's word embedding as a separate feature\n",
    "    embedding = extractEmbedding(token)\n",
    "    for i,n in enumerate(embedding):\n",
    "        features['e{}'.format(i)] = n\n",
    "    \n",
    "    # Record whether a token is the first or last token of a sentence\n",
    "    if i == 0:\n",
    "        features['START'] = True\n",
    "    elif i == (len(sentence) - 1):\n",
    "        features['END'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extractSentenceFeatures(sentence):\n",
    "    return [extractTokenFeatures(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "def extractSentenceTargets(sentence):\n",
    "    return [tag_list[0] for token, pos, tag_list in sentence]\n",
    "\n",
    "def extractSentenceTokens(sentence):\n",
    "    return [token for token, pos, tag_list in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References:*\n",
    "* *https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html*\n",
    "* *https://stackoverflow.com/questions/58736548/how-to-use-word-embedding-as-features-for-crf-sklearn-crfsuite-model-training*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Models\n",
    "\n",
    "### Linguistic\n",
    "\n",
    "* **Features:** custom fastText embeddings\n",
    "* **Target:** Linguistic label category IOB tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_ling\n",
    "df_dev = df_dev_ling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by token, so the all the tags for one token are recorded in a list for that token's row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_token_groups = utils.implodeDataFrame(df_train, ['token_id', 'sentence_id', 'pos', 'token'])\n",
    "df_dev_token_groups = utils.implodeDataFrame(df_dev, ['token_id', 'sentence_id', 'pos', 'token'])\n",
    "df_train_token_groups = df_train_token_groups.reset_index()\n",
    "df_dev_token_groups = df_dev_token_groups.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by sentence, where each sentence is a list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_grouped = utils.implodeDataFrame(df_train_token_groups, ['sentence_id'])\n",
    "df_dev_grouped = utils.implodeDataFrame(df_dev_token_groups, ['sentence_id'])\n",
    "df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "# df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the POS and category tags together with the tokens so each sentence item is a tuple: `(TOKEN, POS-TAG, TAG_LIST)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Title', 'NN', ['O']), (':', ':', ['O']), ('Papers', 'NNS', ['O'])]\n",
      "[('After', 'IN', ['O']), ('his', 'PRP$', ['B-Gendered-Pronoun']), ('ordination', 'NN', ['O'])]\n"
     ]
    }
   ],
   "source": [
    "df_train_grouped = df_train_grouped.reset_index()\n",
    "df_dev_grouped = df_dev_grouped.reset_index()\n",
    "train_sentences_ling = utils.zipFeaturesAndTarget(df_train_grouped, \"tag_linguistic\")\n",
    "print(train_sentences_ling[0][:3])\n",
    "dev_sentences_ling = utils.zipFeaturesAndTarget(df_dev_grouped, \"tag_linguistic\")\n",
    "print(dev_sentences_ling[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_sentences_ling\n",
    "dev_sentences = dev_sentences_ling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X_train = [extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# Target\n",
    "y_train = [extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Linguistic** category of tags.  We'll increase the max_iterations to 100 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow']\n",
    "a = algorithms[3]\n",
    "# clf_ling = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "clf_ling = sklearn_crfsuite.CRF(algorithm=a, pa_type=0, max_iterations=100, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf_ling.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Gendered-Pronoun', 'B-Generalization', 'B-Gendered-Role', 'I-Generalization', 'I-Gendered-Role', 'I-Gendered-Pronoun']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf_ling.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_ling.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "##### Strict Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.6727129243627292\n",
      "  - Prec: 0.7616050258203558\n",
      "  - Rec 0.6635150166852057\n"
     ]
    }
   ],
   "source": [
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data in a directory specific to this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_grouped = df_dev_grouped.rename(columns={\"tag_linguistic\":\"tag_linguistic_expected\"})\n",
    "df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_linguistic_predicted\", y_pred)\n",
    "# df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_linguistic_expected</th>\n",
       "      <th>tag_linguistic_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id token_id   pos    sentence tag_linguistic_expected  \\\n",
       "0            5      154    IN       After                     [O]   \n",
       "0            5      155  PRP$         his    [B-Gendered-Pronoun]   \n",
       "0            5      156    NN  ordination                     [O]   \n",
       "0            5      157   PRP          he    [B-Gendered-Pronoun]   \n",
       "0            5      158   VBD       spent                     [O]   \n",
       "\n",
       "  tag_linguistic_predicted  \n",
       "0                        O  \n",
       "0       B-Gendered-Pronoun  \n",
       "0                        O  \n",
       "0       B-Gendered-Pronoun  \n",
       "0                        O  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns)[1:])\n",
    "df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"sequence_model_output/crf_{a}_baseline/\".format(a=a)\n",
    "Path(config.tokc_path+output_path).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_linguistic_labels_baseline_fastText{d}.csv\".format(a=a, d=d)\n",
    "df_dev_exploded.to_csv(config.tokc_path+output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person Name\n",
    "\n",
    "* **Features:** custom fastText embeddings\n",
    "* **Target:** Person-Name label category IOB tags\n",
    "\n",
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_pers\n",
    "df_dev = df_dev_pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_token_groups = utils.implodeDataFrame(df_train, ['token_id', 'sentence_id', 'pos', 'token'])\n",
    "df_dev_token_groups = utils.implodeDataFrame(df_dev, ['token_id', 'sentence_id', 'pos', 'token'])\n",
    "df_train_token_groups = df_train_token_groups.reset_index()\n",
    "df_dev_token_groups = df_dev_token_groups.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_personname</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[154, 155, 156, 157, 158, 159, 160, 161, 162, ...</td>\n",
       "      <td>[IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...</td>\n",
       "      <td>[After, his, ordination, he, spent, three, yea...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[NN, :, NN]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[[O], [O], [O]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[321, 322, 323, 324, 325, 326, 327, 328, 329, ...</td>\n",
       "      <td>[NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[498, 499, 500, 501, 502, 503, 504, 505, 506, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...</td>\n",
       "      <td>[In, 1941, Tom, Allan, married, Jane, Moore, a...</td>\n",
       "      <td>[[O], [O], [B-Masculine], [I-Masculine], [O], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[649, 650, 651, 652, 653, 654, 655, 656, 657, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...</td>\n",
       "      <td>[In, 1955, Rev, Tom, Allan, accepted, a, call,...</td>\n",
       "      <td>[[O], [O], [B-Masculine], [I-Masculine], [I-Ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      token_id  \\\n",
       "sentence_id                                                      \n",
       "5            [154, 155, 156, 157, 158, 159, 160, 161, 162, ...   \n",
       "11                                             [308, 309, 310]   \n",
       "13           [321, 322, 323, 324, 325, 326, 327, 328, 329, ...   \n",
       "18           [498, 499, 500, 501, 502, 503, 504, 505, 506, ...   \n",
       "24           [649, 650, 651, 652, 653, 654, 655, 656, 657, ...   \n",
       "\n",
       "                                                           pos  \\\n",
       "sentence_id                                                      \n",
       "5            [IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...   \n",
       "11                                                 [NN, :, NN]   \n",
       "13           [NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...   \n",
       "18           [IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...   \n",
       "24           [IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...   \n",
       "\n",
       "                                                      sentence  \\\n",
       "sentence_id                                                      \n",
       "5            [After, his, ordination, he, spent, three, yea...   \n",
       "11                                        [Identifier, :, AA6]   \n",
       "13           [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "18           [In, 1941, Tom, Allan, married, Jane, Moore, a...   \n",
       "24           [In, 1955, Rev, Tom, Allan, accepted, a, call,...   \n",
       "\n",
       "                                                tag_personname  \n",
       "sentence_id                                                     \n",
       "5            [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  \n",
       "11                                             [[O], [O], [O]]  \n",
       "13           [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  \n",
       "18           [[O], [O], [B-Masculine], [I-Masculine], [O], ...  \n",
       "24           [[O], [O], [B-Masculine], [I-Masculine], [I-Ma...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_grouped = utils.implodeDataFrame(df_train_token_groups, ['sentence_id'])\n",
    "df_dev_grouped = utils.implodeDataFrame(df_dev_token_groups, ['sentence_id'])\n",
    "df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the POS and category tags together with the tokens so each sentence item is a tuple: `(TOKEN, POS-TAG, TAG_LIST)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Title', 'NN', ['O']), (':', ':', ['O']), ('Papers', 'NNS', ['O'])]\n",
      "[('After', 'IN', ['O']), ('his', 'PRP$', ['O']), ('ordination', 'NN', ['O'])]\n"
     ]
    }
   ],
   "source": [
    "df_train_grouped = df_train_grouped.reset_index()\n",
    "df_dev_grouped = df_dev_grouped.reset_index()\n",
    "train_sentences_pers = utils.zipFeaturesAndTarget(df_train_grouped, \"tag_personname\")\n",
    "print(train_sentences_pers[0][:3])\n",
    "dev_sentences_pers = utils.zipFeaturesAndTarget(df_dev_grouped, \"tag_personname\")\n",
    "print(dev_sentences_pers[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_sentences_pers\n",
    "dev_sentences = dev_sentences_pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X_train = [extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# Target\n",
    "y_train = [extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Person Name** category of tags.  We'll increase the max iterations to 100 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = algorithms[3]\n",
    "# clf_pers = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "clf_pers = sklearn_crfsuite.CRF(algorithm=a, pa_type=0, max_iterations=100, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf_pers.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Unknown', 'I-Unknown', 'I-Masculine', 'B-Masculine', 'B-Feminine', 'I-Feminine']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf_pers.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_pers.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "##### Strict Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.48188573242789934\n",
      "  - Prec: 0.6440677421567675\n",
      "  - Rec 0.39005572224603513\n"
     ]
    }
   ],
   "source": [
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_personname_expected</th>\n",
       "      <th>tag_personname_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[154, 155, 156, 157, 158, 159, 160, 161, 162, ...</td>\n",
       "      <td>[IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...</td>\n",
       "      <td>[After, his, ordination, he, spent, three, yea...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[NN, :, NN]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[[O], [O], [O]]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[321, 322, 323, 324, 325, 326, 327, 328, 329, ...</td>\n",
       "      <td>[NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>[498, 499, 500, 501, 502, 503, 504, 505, 506, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...</td>\n",
       "      <td>[In, 1941, Tom, Allan, married, Jane, Moore, a...</td>\n",
       "      <td>[[O], [O], [B-Masculine], [I-Masculine], [O], ...</td>\n",
       "      <td>[O, O, B-Masculine, I-Masculine, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>[649, 650, 651, 652, 653, 654, 655, 656, 657, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...</td>\n",
       "      <td>[In, 1955, Rev, Tom, Allan, accepted, a, call,...</td>\n",
       "      <td>[[O], [O], [B-Masculine], [I-Masculine], [I-Ma...</td>\n",
       "      <td>[O, O, B-Masculine, I-Masculine, I-Masculine, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id                                           token_id  \\\n",
       "0            5  [154, 155, 156, 157, 158, 159, 160, 161, 162, ...   \n",
       "1           11                                    [308, 309, 310]   \n",
       "2           13  [321, 322, 323, 324, 325, 326, 327, 328, 329, ...   \n",
       "3           18  [498, 499, 500, 501, 502, 503, 504, 505, 506, ...   \n",
       "4           24  [649, 650, 651, 652, 653, 654, 655, 656, 657, ...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...   \n",
       "1                                        [NN, :, NN]   \n",
       "2  [NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...   \n",
       "3  [IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...   \n",
       "4  [IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  [After, his, ordination, he, spent, three, yea...   \n",
       "1                               [Identifier, :, AA6]   \n",
       "2  [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "3  [In, 1941, Tom, Allan, married, Jane, Moore, a...   \n",
       "4  [In, 1955, Rev, Tom, Allan, accepted, a, call,...   \n",
       "\n",
       "                             tag_personname_expected  \\\n",
       "0  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...   \n",
       "1                                    [[O], [O], [O]]   \n",
       "2  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...   \n",
       "3  [[O], [O], [B-Masculine], [I-Masculine], [O], ...   \n",
       "4  [[O], [O], [B-Masculine], [I-Masculine], [I-Ma...   \n",
       "\n",
       "                            tag_personname_predicted  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1                                          [O, O, O]  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, B-Masculine, I-Masculine, O, O, O, O, O...  \n",
       "4  [O, O, B-Masculine, I-Masculine, I-Masculine, ...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_grouped = df_dev_grouped.rename(columns={\"tag_personname\":\"tag_personname_expected\"})\n",
    "df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_personname_predicted\", y_pred)\n",
    "df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_personname_expected</th>\n",
       "      <th>tag_personname_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token_id   pos    sentence tag_personname_expected  \\\n",
       "sentence_id                                                      \n",
       "5                154    IN       After                     [O]   \n",
       "5                155  PRP$         his                     [O]   \n",
       "5                156    NN  ordination                     [O]   \n",
       "5                157   PRP          he                     [O]   \n",
       "5                158   VBD       spent                     [O]   \n",
       "\n",
       "            tag_personname_predicted  \n",
       "sentence_id                           \n",
       "5                                  O  \n",
       "5                                  O  \n",
       "5                                  O  \n",
       "5                                  O  \n",
       "5                                  O  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_grouped = df_dev_grouped.set_index(\"sentence_id\")\n",
    "df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns))\n",
    "df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_personname_labels_baseline_fastText{d}.csv\".format(a=a, d=d)\n",
    "df_dev_exploded.to_csv(config.tokc_path+output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "### Contextual\n",
    "\n",
    "* **Features:** custom fastText embeddings\n",
    "* **Target:** Contextual label category IOB tags\n",
    "\n",
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_cont\n",
    "df_dev = df_dev_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_token_groups = utils.implodeDataFrame(df_train, ['token_id', 'sentence_id', 'pos', 'token'])\n",
    "df_dev_token_groups = utils.implodeDataFrame(df_dev, ['token_id', 'sentence_id', 'pos', 'token'])\n",
    "df_train_token_groups = df_train_token_groups.reset_index()\n",
    "df_dev_token_groups = df_dev_token_groups.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_contextual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[154, 155, 156, 157, 158, 159, 160, 161, 162, ...</td>\n",
       "      <td>[IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...</td>\n",
       "      <td>[After, his, ordination, he, spent, three, yea...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[NN, :, NN]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[[O], [O], [O]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[321, 322, 323, 324, 325, 326, 327, 328, 329, ...</td>\n",
       "      <td>[NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[498, 499, 500, 501, 502, 503, 504, 505, 506, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...</td>\n",
       "      <td>[In, 1941, Tom, Allan, married, Jane, Moore, a...</td>\n",
       "      <td>[[O], [O], [B-Stereotype], [I-Stereotype], [I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[649, 650, 651, 652, 653, 654, 655, 656, 657, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...</td>\n",
       "      <td>[In, 1955, Rev, Tom, Allan, accepted, a, call,...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      token_id  \\\n",
       "sentence_id                                                      \n",
       "5            [154, 155, 156, 157, 158, 159, 160, 161, 162, ...   \n",
       "11                                             [308, 309, 310]   \n",
       "13           [321, 322, 323, 324, 325, 326, 327, 328, 329, ...   \n",
       "18           [498, 499, 500, 501, 502, 503, 504, 505, 506, ...   \n",
       "24           [649, 650, 651, 652, 653, 654, 655, 656, 657, ...   \n",
       "\n",
       "                                                           pos  \\\n",
       "sentence_id                                                      \n",
       "5            [IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...   \n",
       "11                                                 [NN, :, NN]   \n",
       "13           [NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...   \n",
       "18           [IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...   \n",
       "24           [IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...   \n",
       "\n",
       "                                                      sentence  \\\n",
       "sentence_id                                                      \n",
       "5            [After, his, ordination, he, spent, three, yea...   \n",
       "11                                        [Identifier, :, AA6]   \n",
       "13           [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "18           [In, 1941, Tom, Allan, married, Jane, Moore, a...   \n",
       "24           [In, 1955, Rev, Tom, Allan, accepted, a, call,...   \n",
       "\n",
       "                                                tag_contextual  \n",
       "sentence_id                                                     \n",
       "5            [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  \n",
       "11                                             [[O], [O], [O]]  \n",
       "13           [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  \n",
       "18           [[O], [O], [B-Stereotype], [I-Stereotype], [I-...  \n",
       "24           [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_grouped = utils.implodeDataFrame(df_train_token_groups, ['sentence_id'])\n",
    "df_dev_grouped = utils.implodeDataFrame(df_dev_token_groups, ['sentence_id'])\n",
    "df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the POS and category tags together with the tokens so each sentence item is a tuple: `(TOKEN, POS-TAG, TAG_LIST)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Title', 'NN', ['O']), (':', ':', ['O']), ('Papers', 'NNS', ['O'])]\n",
      "[('After', 'IN', ['O']), ('his', 'PRP$', ['O']), ('ordination', 'NN', ['O'])]\n"
     ]
    }
   ],
   "source": [
    "df_train_grouped = df_train_grouped.reset_index()\n",
    "df_dev_grouped = df_dev_grouped.reset_index()\n",
    "train_sentences_cont = utils.zipFeaturesAndTarget(df_train_grouped, \"tag_contextual\")\n",
    "print(train_sentences_cont[0][:3])\n",
    "dev_sentences_cont = utils.zipFeaturesAndTarget(df_dev_grouped, \"tag_contextual\")\n",
    "print(dev_sentences_cont[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_sentences_cont\n",
    "dev_sentences = dev_sentences_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X_train = [extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# Target\n",
    "y_train = [extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Contextual** category of tags.  We'll increase the maximum iterations to 100 for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = algorithms[3]\n",
    "# clf_cont = sklearn_crfsuite.CRF(algorithm=a, variance=0.5, max_iterations=100, all_possible_transitions=True)\n",
    "clf_cont = sklearn_crfsuite.CRF(algorithm=a, pa_type=0, max_iterations=100, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf_cont.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Stereotype', 'I-Stereotype', 'B-Occupation', 'I-Occupation', 'B-Omission', 'I-Omission']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf_cont.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_cont.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "##### Strict Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.3351113309637379\n",
      "  - Prec: 0.7623135913364345\n",
      "  - Rec 0.23452768729641693\n"
     ]
    }
   ],
   "source": [
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_contextual_expected</th>\n",
       "      <th>tag_contextual_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[154, 155, 156, 157, 158, 159, 160, 161, 162, ...</td>\n",
       "      <td>[IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...</td>\n",
       "      <td>[After, his, ordination, he, spent, three, yea...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[NN, :, NN]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[[O], [O], [O]]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[321, 322, 323, 324, 325, 326, 327, 328, 329, ...</td>\n",
       "      <td>[NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>[498, 499, 500, 501, 502, 503, 504, 505, 506, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...</td>\n",
       "      <td>[In, 1941, Tom, Allan, married, Jane, Moore, a...</td>\n",
       "      <td>[[O], [O], [B-Stereotype], [I-Stereotype], [I-...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>[649, 650, 651, 652, 653, 654, 655, 656, 657, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...</td>\n",
       "      <td>[In, 1955, Rev, Tom, Allan, accepted, a, call,...</td>\n",
       "      <td>[[O], [O], [O], [O], [O], [O], [O], [O], [O], ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-Occupation, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id                                           token_id  \\\n",
       "0            5  [154, 155, 156, 157, 158, 159, 160, 161, 162, ...   \n",
       "1           11                                    [308, 309, 310]   \n",
       "2           13  [321, 322, 323, 324, 325, 326, 327, 328, 329, ...   \n",
       "3           18  [498, 499, 500, 501, 502, 503, 504, 505, 506, ...   \n",
       "4           24  [649, 650, 651, 652, 653, 654, 655, 656, 657, ...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...   \n",
       "1                                        [NN, :, NN]   \n",
       "2  [NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...   \n",
       "3  [IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...   \n",
       "4  [IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  [After, his, ordination, he, spent, three, yea...   \n",
       "1                               [Identifier, :, AA6]   \n",
       "2  [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "3  [In, 1941, Tom, Allan, married, Jane, Moore, a...   \n",
       "4  [In, 1955, Rev, Tom, Allan, accepted, a, call,...   \n",
       "\n",
       "                             tag_contextual_expected  \\\n",
       "0  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...   \n",
       "1                                    [[O], [O], [O]]   \n",
       "2  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...   \n",
       "3  [[O], [O], [B-Stereotype], [I-Stereotype], [I-...   \n",
       "4  [[O], [O], [O], [O], [O], [O], [O], [O], [O], ...   \n",
       "\n",
       "                            tag_contextual_predicted  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1                                          [O, O, O]  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, B-Occupation, O...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_grouped = df_dev_grouped.rename(columns={\"tag_contextual\":\"tag_contextual_expected\"})\n",
    "df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_contextual_predicted\", y_pred)\n",
    "df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_contextual_expected</th>\n",
       "      <th>tag_contextual_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token_id   pos    sentence tag_contextual_expected  \\\n",
       "sentence_id                                                      \n",
       "5                154    IN       After                     [O]   \n",
       "5                155  PRP$         his                     [O]   \n",
       "5                156    NN  ordination                     [O]   \n",
       "5                157   PRP          he                     [O]   \n",
       "5                158   VBD       spent                     [O]   \n",
       "\n",
       "            tag_contextual_predicted  \n",
       "sentence_id                           \n",
       "5                                  O  \n",
       "5                                  O  \n",
       "5                                  O  \n",
       "5                                  O  \n",
       "5                                  O  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_grouped = df_dev_grouped.set_index(\"sentence_id\")\n",
    "df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns))\n",
    "df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"crf_{a}_contextual_labels_baseline_fastText{d}.csv\".format(a=a, d=d)\n",
    "df_dev_exploded.to_csv(config.tokc_path+output_path+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Performance Evaluation\n",
    "\n",
    "### Strict Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"sequence_model_output/crf_{a}_baseline/\".format(a=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"contextual\"\n",
    "filename = \"crf_{a}_{c}_labels_baseline_fastText{d}.csv\".format(a=a, c=category, d=d)\n",
    "pred_cont = pd.read_csv(config.tokc_path+output_path+filename, index_col=0)\n",
    "pred_cont = utils.getColumnValuesAsLists(pred_cont, \"tag_{}_expected\".format(category))\n",
    "# pred_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"personname\"\n",
    "filename = \"crf_{a}_{c}_labels_baseline_fastText{d}.csv\".format(a=a, c=category, d=d)\n",
    "pred_pers = pd.read_csv(config.tokc_path+output_path+filename, index_col=0)\n",
    "pred_pers = utils.getColumnValuesAsLists(pred_pers, \"tag_{}_expected\".format(category))\n",
    "# pred_pers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"linguistic\"\n",
    "filename = \"crf_{a}_{c}_labels_baseline_fastText{d}.csv\".format(a=a, c=category, d=d)\n",
    "pred_ling = pd.read_csv(config.tokc_path+output_path+filename, index_col=0)\n",
    "pred_ling = utils.getColumnValuesAsLists(pred_ling, \"tag_{}_expected\".format(category))\n",
    "pred_ling = pred_ling.set_index(\"sentence_id\")\n",
    "# pred_ling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics for each category of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"contextual\"\n",
    "pred_cont = utils.isPredictedInExpected(pred_cont, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "# pred_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"personname\"\n",
    "pred_pers = utils.isPredictedInExpected(pred_pers, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "# pred_pers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"linguistic\"\n",
    "pred_ling = utils.isPredictedInExpected(pred_ling, \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), '_merge', 'O')\n",
    "# pred_ling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"contextual\"\n",
    "tags = ['B-Occupation', 'I-Occupation', 'B-Omission', 'I-Omission', 'B-Stereotype', 'I-Stereotype']\n",
    "pred_cont_stats = utils.getScoresByCatTags(\n",
    "    pred_cont, \"_merge\", tags[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_cont, \"_merge\", tags[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_cont_stats = pd.concat([pred_cont_stats, tag_stats])\n",
    "# pred_cont_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"personname\"\n",
    "tags = ['B-Feminine', 'I-Feminine', 'B-Masculine', 'I-Masculine', 'B-Unknown', 'I-Unknown', \"B-Nonbinary\", \"I-Nonbinary\"]\n",
    "pred_pers_stats = utils.getScoresByCatTags(\n",
    "    pred_pers, \"_merge\", tags[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_pers, \"_merge\", tags[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_pers_stats = pd.concat([pred_pers_stats, tag_stats])\n",
    "# pred_pers_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"linguistic\"\n",
    "tags = [\"B-Gendered-Pronoun\", \"I-Gendered-Pronoun\", \"B-Gendered-Role\", \"I-Gendered-Role\", \"B-Generalization\", \"I-Generalization\"]\n",
    "pred_ling_stats = utils.getScoresByCatTags(\n",
    "    pred_ling, \"_merge\", tags[0], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_ling, \"_merge\", tags[i], \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_ling_stats = pd.concat([pred_ling_stats, tag_stats])\n",
    "# pred_ling_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Occupation</td>\n",
       "      <td>395</td>\n",
       "      <td>82</td>\n",
       "      <td>255</td>\n",
       "      <td>0.756677</td>\n",
       "      <td>0.392308</td>\n",
       "      <td>0.516717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Occupation</td>\n",
       "      <td>570</td>\n",
       "      <td>55</td>\n",
       "      <td>189</td>\n",
       "      <td>0.774590</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>0.376869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Omission</td>\n",
       "      <td>593</td>\n",
       "      <td>103</td>\n",
       "      <td>408</td>\n",
       "      <td>0.798434</td>\n",
       "      <td>0.407592</td>\n",
       "      <td>0.539683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Omission</td>\n",
       "      <td>1281</td>\n",
       "      <td>70</td>\n",
       "      <td>204</td>\n",
       "      <td>0.744526</td>\n",
       "      <td>0.137374</td>\n",
       "      <td>0.231950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Stereotype</td>\n",
       "      <td>238</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.061303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Stereotype</td>\n",
       "      <td>680</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.065484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Feminine</td>\n",
       "      <td>69</td>\n",
       "      <td>29</td>\n",
       "      <td>166</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.706383</td>\n",
       "      <td>0.772093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Feminine</td>\n",
       "      <td>219</td>\n",
       "      <td>39</td>\n",
       "      <td>335</td>\n",
       "      <td>0.895722</td>\n",
       "      <td>0.604693</td>\n",
       "      <td>0.721983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Masculine</td>\n",
       "      <td>300</td>\n",
       "      <td>127</td>\n",
       "      <td>352</td>\n",
       "      <td>0.734864</td>\n",
       "      <td>0.539877</td>\n",
       "      <td>0.622458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Masculine</td>\n",
       "      <td>515</td>\n",
       "      <td>99</td>\n",
       "      <td>242</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.319683</td>\n",
       "      <td>0.440801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>871</td>\n",
       "      <td>301</td>\n",
       "      <td>742</td>\n",
       "      <td>0.711409</td>\n",
       "      <td>0.460012</td>\n",
       "      <td>0.558735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Unknown</td>\n",
       "      <td>1508</td>\n",
       "      <td>502</td>\n",
       "      <td>1282</td>\n",
       "      <td>0.718610</td>\n",
       "      <td>0.459498</td>\n",
       "      <td>0.560560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Nonbinary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Nonbinary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>22</td>\n",
       "      <td>170</td>\n",
       "      <td>722</td>\n",
       "      <td>0.809417</td>\n",
       "      <td>0.970430</td>\n",
       "      <td>0.882641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Gendered-Pronoun</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Gendered-Role</td>\n",
       "      <td>214</td>\n",
       "      <td>116</td>\n",
       "      <td>361</td>\n",
       "      <td>0.756813</td>\n",
       "      <td>0.627826</td>\n",
       "      <td>0.686312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Gendered-Role</td>\n",
       "      <td>78</td>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.437158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-Generalization</td>\n",
       "      <td>162</td>\n",
       "      <td>14</td>\n",
       "      <td>49</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.232227</td>\n",
       "      <td>0.357664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Generalization</td>\n",
       "      <td>122</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>0.258427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag(s)  false negative  false positive  true positive  \\\n",
       "0        B-Occupation             395              82            255   \n",
       "0        I-Occupation             570              55            189   \n",
       "0          B-Omission             593             103            408   \n",
       "0          I-Omission            1281              70            204   \n",
       "0        B-Stereotype             238               7              8   \n",
       "0        I-Stereotype             680               5             24   \n",
       "0          B-Feminine              69              29            166   \n",
       "0          I-Feminine             219              39            335   \n",
       "0         B-Masculine             300             127            352   \n",
       "0         I-Masculine             515              99            242   \n",
       "0           B-Unknown             871             301            742   \n",
       "0           I-Unknown            1508             502           1282   \n",
       "0         B-Nonbinary               0               0              0   \n",
       "0         I-Nonbinary               0               0              0   \n",
       "0  B-Gendered-Pronoun              22             170            722   \n",
       "0  I-Gendered-Pronoun               6               2              9   \n",
       "0     B-Gendered-Role             214             116            361   \n",
       "0     I-Gendered-Role              78              25             40   \n",
       "0    B-Generalization             162              14             49   \n",
       "0    I-Generalization             122              10             23   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.756677  0.392308  0.516717  \n",
       "0   0.774590  0.249012  0.376869  \n",
       "0   0.798434  0.407592  0.539683  \n",
       "0   0.744526  0.137374  0.231950  \n",
       "0   0.533333  0.032520  0.061303  \n",
       "0   0.827586  0.034091  0.065484  \n",
       "0   0.851282  0.706383  0.772093  \n",
       "0   0.895722  0.604693  0.721983  \n",
       "0   0.734864  0.539877  0.622458  \n",
       "0   0.709677  0.319683  0.440801  \n",
       "0   0.711409  0.460012  0.558735  \n",
       "0   0.718610  0.459498  0.560560  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.000000  0.000000  0.000000  \n",
       "0   0.809417  0.970430  0.882641  \n",
       "0   0.818182  0.600000  0.692308  \n",
       "0   0.756813  0.627826  0.686312  \n",
       "0   0.615385  0.338983  0.437158  \n",
       "0   0.777778  0.232227  0.357664  \n",
       "0   0.696970  0.158621  0.258427  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = pd.concat([pred_cont_stats, pred_pers_stats, pred_ling_stats])\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"sequence_model_performance/crf_{a}_baseline/\".format(a=a)\n",
    "Path(config.tokc_path+output_path).mkdir(exist_ok=True, parents=True)\n",
    "stats.to_csv(\n",
    "    config.tokc_path+output_path+\"crf_{a}_baseline_fastText{d}_performance_strict_alltags.csv\".format(a=a, d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotation Agreement\n",
    "\n",
    "Calculate agreement at the annotation level, so if the model labels any word correctly from a manually annotated text span, that annotation is recorded as being correctly labeled (`true positive`).  Note whether the models' labels are an `exact_match`, `label_match`, `category_match` or `mismatch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the annotation data:\n",
    "\n",
    "*Note: `ann_id` of `9999` indicates no annotation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>154</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>14379</td>\n",
       "      <td>155</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>156</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>14380</td>\n",
       "      <td>157</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>158</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id  token_id                 tag\n",
       "0            5   99999       154                   O\n",
       "1            5   14379       155  B-Gendered-Pronoun\n",
       "2            5   99999       156                   O\n",
       "3            5   14380       157  B-Gendered-Pronoun\n",
       "4            5   99999       158                   O"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", usecols=[\"sentence_id\", \"ann_id\", \"token_id\", \"tag\"])\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the annotation data by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157663, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>14379</td>\n",
       "      <td>155</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>14380</td>\n",
       "      <td>157</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>41262</td>\n",
       "      <td>163</td>\n",
       "      <td>[B-Occupation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>41262</td>\n",
       "      <td>164</td>\n",
       "      <td>[I-Occupation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>154</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id  token_id                   tag\n",
       "0            5   14379       155  [B-Gendered-Pronoun]\n",
       "1            5   14380       157  [B-Gendered-Pronoun]\n",
       "2            5   41262       163        [B-Occupation]\n",
       "3            5   41262       164        [I-Occupation]\n",
       "4            5   99999       154                   [O]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ann = utils.implodeDataFrame(df_dev, [\"sentence_id\", \"ann_id\", \"token_id\"])\n",
    "df_ann = df_ann.reset_index()\n",
    "print(df_ann.shape)\n",
    "df_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the columns of the dev and prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_linguistic_expected</th>\n",
       "      <th>tag_linguistic_predicted</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id   pos       token tag_linguistic_expected  \\\n",
       "0            5       154    IN       After                     [O]   \n",
       "1            5       155  PRP$         his    [B-Gendered-Pronoun]   \n",
       "2            5       156    NN  ordination                     [O]   \n",
       "3            5       157   PRP          he    [B-Gendered-Pronoun]   \n",
       "4            5       158   VBD       spent                     [O]   \n",
       "\n",
       "  tag_linguistic_predicted         _merge  \n",
       "0                        O  true negative  \n",
       "1       B-Gendered-Pronoun  true positive  \n",
       "2                        O  true negative  \n",
       "3       B-Gendered-Pronoun  true positive  \n",
       "4                        O  true negative  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sentence_id a column instead of the index\n",
    "pred_cont = pred_cont.reset_index()\n",
    "pred_pers = pred_pers.reset_index()\n",
    "pred_ling = pred_ling.reset_index()\n",
    "# Rename `sentence` column `token`\n",
    "pred_cont = pred_cont.rename(columns={\"sentence\":\"token\"})\n",
    "pred_pers = pred_pers.rename(columns={\"sentence\":\"token\"})\n",
    "pred_ling = pred_ling.rename(columns={\"sentence\":\"token\"})\n",
    "pred_ling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data, adding the annotation IDs (`ann_id` column) to the prediction DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [\"sentence_id\", \"token_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152455, 7)\n",
      "(157663, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_contextual_expected</th>\n",
       "      <th>tag_contextual_predicted</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>14379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>14380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id   pos       token tag_contextual_expected  \\\n",
       "0            5       154    IN       After                     [O]   \n",
       "1            5       155  PRP$         his                     [O]   \n",
       "2            5       156    NN  ordination                     [O]   \n",
       "3            5       157   PRP          he                     [O]   \n",
       "4            5       158   VBD       spent                     [O]   \n",
       "\n",
       "  tag_contextual_predicted         _merge  ann_id  \n",
       "0                        O  true negative   99999  \n",
       "1                        O  true negative   14379  \n",
       "2                        O  true negative   99999  \n",
       "3                        O  true negative   14380  \n",
       "4                        O  true negative   99999  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pred_cont.shape)\n",
    "pred_cont_ann = pred_cont.join(df_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "print(pred_cont_ann.shape)\n",
    "pred_cont_ann = pred_cont_ann.drop(columns=[\"tag\"])  # duplicate of tag_expected\n",
    "assert pred_cont_ann.loc[pred_cont_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_cont_ann.loc[pred_cont_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_cont_ann.loc[pred_cont_ann[\"tag_contextual_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_cont_ann.loc[pred_cont_ann[\"tag_contextual_expected\"].isna()].shape[0] == 0\n",
    "pred_cont_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_personname_expected</th>\n",
       "      <th>tag_personname_predicted</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>14379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>14380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id   pos       token tag_personname_expected  \\\n",
       "0            5       154    IN       After                     [O]   \n",
       "1            5       155  PRP$         his                     [O]   \n",
       "2            5       156    NN  ordination                     [O]   \n",
       "3            5       157   PRP          he                     [O]   \n",
       "4            5       158   VBD       spent                     [O]   \n",
       "\n",
       "  tag_personname_predicted         _merge  ann_id  \n",
       "0                        O  true negative   99999  \n",
       "1                        O  true negative   14379  \n",
       "2                        O  true negative   99999  \n",
       "3                        O  true negative   14380  \n",
       "4                        O  true negative   99999  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pers_ann = pred_pers.join(df_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_pers_ann = pred_pers_ann.drop(columns=[\"tag\"])  # duplicate of tag_expected\n",
    "assert pred_pers_ann.loc[pred_pers_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_pers_ann.loc[pred_pers_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_pers_ann.loc[pred_pers_ann[\"tag_personname_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_pers_ann.loc[pred_pers_ann[\"tag_personname_expected\"].isna()].shape[0] == 0\n",
    "pred_pers_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_linguistic_expected</th>\n",
       "      <th>tag_linguistic_predicted</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>[B-Gendered-Pronoun]</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id   pos       token tag_linguistic_expected  \\\n",
       "0            5       154    IN       After                     [O]   \n",
       "1            5       155  PRP$         his    [B-Gendered-Pronoun]   \n",
       "2            5       156    NN  ordination                     [O]   \n",
       "3            5       157   PRP          he    [B-Gendered-Pronoun]   \n",
       "4            5       158   VBD       spent                     [O]   \n",
       "\n",
       "  tag_linguistic_predicted         _merge  ann_id  \n",
       "0                        O  true negative   99999  \n",
       "1       B-Gendered-Pronoun  true positive   14379  \n",
       "2                        O  true negative   99999  \n",
       "3       B-Gendered-Pronoun  true positive   14380  \n",
       "4                        O  true negative   99999  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ling_ann = pred_ling.join(df_ann.set_index(index_list), on=index_list, how=\"left\")\n",
    "pred_ling_ann = pred_ling_ann.drop(columns=[\"tag\"])  # duplicate of tag_expected\n",
    "assert pred_ling_ann.loc[pred_ling_ann[\"token_id\"].isna()].shape[0] == 0\n",
    "assert pred_ling_ann.loc[pred_ling_ann[\"ann_id\"].isna()].shape[0] == 0\n",
    "assert pred_ling_ann.loc[pred_ling_ann[\"tag_linguistic_predicted\"].isna()].shape[0] == 0\n",
    "assert pred_ling_ann.loc[pred_ling_ann[\"tag_linguistic_expected\"].isna()].shape[0] == 0\n",
    "pred_ling_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode the DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ling_ann = pred_ling_ann.explode([\"tag_linguistic_expected\"])\n",
    "# pred_ling_ann.head() # Looks good\n",
    "pred_cont_ann = pred_cont_ann.explode([\"tag_contextual_expected\"])\n",
    "pred_pers_ann = pred_pers_ann.explode([\"tag_personname_expected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the BIO tags to label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pred_cont_ann, pred_pers_ann, pred_ling_ann]\n",
    "categories = [\"contextual\", \"personname\", \"linguistic\"]\n",
    "maxI = len(categories)\n",
    "for i in range(maxI):\n",
    "    df, category = dfs[i], categories[i]\n",
    "    # Get the predicted labels\n",
    "    pred_labels = list(df[\"tag_{}_predicted\".format(category)])\n",
    "    pred_labels = [label if label == \"O\" else label[2:] for label in pred_labels]\n",
    "    df.insert(len(df.columns), \"label_{}_predicted\".format(category), pred_labels)\n",
    "    # Get the lists of expected labels\n",
    "    exp_labels = list(df[\"tag_{}_expected\".format(category)])\n",
    "    exp_labels = [label if label == \"O\" else label[2:] for label in exp_labels]\n",
    "    df.insert(len(df.columns), \"label_{}_expected\".format(category), exp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_linguistic_expected</th>\n",
       "      <th>tag_linguistic_predicted</th>\n",
       "      <th>_merge</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>label_linguistic_predicted</th>\n",
       "      <th>label_linguistic_expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14379</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>true positive</td>\n",
       "      <td>14380</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>true negative</td>\n",
       "      <td>99999</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token_id   pos       token tag_linguistic_expected  \\\n",
       "0            5       154    IN       After                       O   \n",
       "1            5       155  PRP$         his      B-Gendered-Pronoun   \n",
       "2            5       156    NN  ordination                       O   \n",
       "3            5       157   PRP          he      B-Gendered-Pronoun   \n",
       "4            5       158   VBD       spent                       O   \n",
       "\n",
       "  tag_linguistic_predicted         _merge  ann_id label_linguistic_predicted  \\\n",
       "0                        O  true negative   99999                          O   \n",
       "1       B-Gendered-Pronoun  true positive   14379           Gendered-Pronoun   \n",
       "2                        O  true negative   99999                          O   \n",
       "3       B-Gendered-Pronoun  true positive   14380           Gendered-Pronoun   \n",
       "4                        O  true negative   99999                          O   \n",
       "\n",
       "  label_linguistic_expected  \n",
       "0                         O  \n",
       "1          Gendered-Pronoun  \n",
       "2                         O  \n",
       "3          Gendered-Pronoun  \n",
       "4                         O  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_cont_ann.head()\n",
    "# pred_pers_ann.head()\n",
    "pred_ling_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>label_contextual_predicted</th>\n",
       "      <th>label_contextual_expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>14379</td>\n",
       "      <td>[155]</td>\n",
       "      <td>[his]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>14380</td>\n",
       "      <td>[157]</td>\n",
       "      <td>[he]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>41262</td>\n",
       "      <td>[163, 164]</td>\n",
       "      <td>[army, Chaplain]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[Occupation, Occupation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>[154, 156, 158, 159, 160, 161, 162, 165, 166, ...</td>\n",
       "      <td>[After, ordination, spent, three, years, as, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>99999</td>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id                                           token_id  \\\n",
       "0            5   14379                                              [155]   \n",
       "1            5   14380                                              [157]   \n",
       "2            5   41262                                         [163, 164]   \n",
       "3            5   99999  [154, 156, 158, 159, 160, 161, 162, 165, 166, ...   \n",
       "4           11   99999                                    [308, 309, 310]   \n",
       "\n",
       "                                               token  \\\n",
       "0                                              [his]   \n",
       "1                                               [he]   \n",
       "2                                   [army, Chaplain]   \n",
       "3  [After, ordination, spent, three, years, as, a...   \n",
       "4                               [Identifier, :, AA6]   \n",
       "\n",
       "                          label_contextual_predicted  \\\n",
       "0                                                [O]   \n",
       "1                                                [O]   \n",
       "2                                             [O, O]   \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4                                          [O, O, O]   \n",
       "\n",
       "                           label_contextual_expected  \n",
       "0                                                [O]  \n",
       "1                                                [O]  \n",
       "2                           [Occupation, Occupation]  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4                                          [O, O, O]  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"contextual\"\n",
    "pred_cont_ann = pred_cont_ann.drop(columns=[\"pos\", \"_merge\", \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_cont_ann = utils.implodeDataFrame(pred_cont_ann, [\"sentence_id\", \"ann_id\"])\n",
    "pred_cont_ann = pred_cont_ann.reset_index()\n",
    "pred_cont_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>label_personname_predicted</th>\n",
       "      <th>label_personname_expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>14379</td>\n",
       "      <td>[155]</td>\n",
       "      <td>[his]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>14380</td>\n",
       "      <td>[157]</td>\n",
       "      <td>[he]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>41262</td>\n",
       "      <td>[163, 164]</td>\n",
       "      <td>[army, Chaplain]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>[154, 156, 158, 159, 160, 161, 162, 165, 166, ...</td>\n",
       "      <td>[After, ordination, spent, three, years, as, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>99999</td>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id                                           token_id  \\\n",
       "0            5   14379                                              [155]   \n",
       "1            5   14380                                              [157]   \n",
       "2            5   41262                                         [163, 164]   \n",
       "3            5   99999  [154, 156, 158, 159, 160, 161, 162, 165, 166, ...   \n",
       "4           11   99999                                    [308, 309, 310]   \n",
       "\n",
       "                                               token  \\\n",
       "0                                              [his]   \n",
       "1                                               [he]   \n",
       "2                                   [army, Chaplain]   \n",
       "3  [After, ordination, spent, three, years, as, a...   \n",
       "4                               [Identifier, :, AA6]   \n",
       "\n",
       "                          label_personname_predicted  \\\n",
       "0                                                [O]   \n",
       "1                                                [O]   \n",
       "2                                             [O, O]   \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4                                          [O, O, O]   \n",
       "\n",
       "                           label_personname_expected  \n",
       "0                                                [O]  \n",
       "1                                                [O]  \n",
       "2                                             [O, O]  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4                                          [O, O, O]  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"personname\"\n",
    "pred_pers_ann = pred_pers_ann.drop(columns=[\"pos\", \"_merge\", \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_pers_ann = utils.implodeDataFrame(pred_pers_ann, [\"sentence_id\", \"ann_id\"])\n",
    "pred_pers_ann = pred_pers_ann.reset_index()\n",
    "pred_pers_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>label_linguistic_predicted</th>\n",
       "      <th>label_linguistic_expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>14379</td>\n",
       "      <td>[155]</td>\n",
       "      <td>[his]</td>\n",
       "      <td>[Gendered-Pronoun]</td>\n",
       "      <td>[Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>14380</td>\n",
       "      <td>[157]</td>\n",
       "      <td>[he]</td>\n",
       "      <td>[Gendered-Pronoun]</td>\n",
       "      <td>[Gendered-Pronoun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>41262</td>\n",
       "      <td>[163, 164]</td>\n",
       "      <td>[army, Chaplain]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>[154, 156, 158, 159, 160, 161, 162, 165, 166, ...</td>\n",
       "      <td>[After, ordination, spent, three, years, as, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>99999</td>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id                                           token_id  \\\n",
       "0            5   14379                                              [155]   \n",
       "1            5   14380                                              [157]   \n",
       "2            5   41262                                         [163, 164]   \n",
       "3            5   99999  [154, 156, 158, 159, 160, 161, 162, 165, 166, ...   \n",
       "4           11   99999                                    [308, 309, 310]   \n",
       "\n",
       "                                               token  \\\n",
       "0                                              [his]   \n",
       "1                                               [he]   \n",
       "2                                   [army, Chaplain]   \n",
       "3  [After, ordination, spent, three, years, as, a...   \n",
       "4                               [Identifier, :, AA6]   \n",
       "\n",
       "                          label_linguistic_predicted  \\\n",
       "0                                 [Gendered-Pronoun]   \n",
       "1                                 [Gendered-Pronoun]   \n",
       "2                                             [O, O]   \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4                                          [O, O, O]   \n",
       "\n",
       "                           label_linguistic_expected  \n",
       "0                                 [Gendered-Pronoun]  \n",
       "1                                 [Gendered-Pronoun]  \n",
       "2                                             [O, O]  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4                                          [O, O, O]  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"linguistic\"\n",
    "pred_ling_ann = pred_ling_ann.drop(columns=[\"pos\", \"_merge\", \"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_ling_ann = utils.implodeDataFrame(pred_ling_ann, [\"sentence_id\", \"ann_id\"])\n",
    "pred_ling_ann = pred_ling_ann.reset_index()\n",
    "pred_ling_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the agreements and disagreements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "agmt_types_cont = utils.getAnnotationAgreement(pred_cont_ann, \"label_contextual_predicted\", \"label_contextual_expected\")\n",
    "agmt_types_pers = utils.getAnnotationAgreement(pred_pers_ann, \"label_personname_predicted\", \"label_personname_expected\")\n",
    "agmt_types_ling = utils.getAnnotationAgreement(pred_ling_ann, \"label_linguistic_predicted\", \"label_linguistic_expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cont_ann.insert(len(pred_cont_ann.columns), \"annotation_agreement\", agmt_types_cont)\n",
    "pred_pers_ann.insert(len(pred_pers_ann.columns), \"annotation_agreement\", agmt_types_pers)\n",
    "pred_ling_ann.insert(len(pred_ling_ann.columns), \"annotation_agreement\", agmt_types_ling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>label_contextual_predicted</th>\n",
       "      <th>label_contextual_expected</th>\n",
       "      <th>annotation_agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>14379</td>\n",
       "      <td>[155]</td>\n",
       "      <td>[his]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>14380</td>\n",
       "      <td>[157]</td>\n",
       "      <td>[he]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>41262</td>\n",
       "      <td>[163, 164]</td>\n",
       "      <td>[army, Chaplain]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[Occupation, Occupation]</td>\n",
       "      <td>false negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>[154, 156, 158, 159, 160, 161, 162, 165, 166, ...</td>\n",
       "      <td>[After, ordination, spent, three, years, as, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>99999</td>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>true negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  ann_id                                           token_id  \\\n",
       "0            5   14379                                              [155]   \n",
       "1            5   14380                                              [157]   \n",
       "2            5   41262                                         [163, 164]   \n",
       "3            5   99999  [154, 156, 158, 159, 160, 161, 162, 165, 166, ...   \n",
       "4           11   99999                                    [308, 309, 310]   \n",
       "\n",
       "                                               token  \\\n",
       "0                                              [his]   \n",
       "1                                               [he]   \n",
       "2                                   [army, Chaplain]   \n",
       "3  [After, ordination, spent, three, years, as, a...   \n",
       "4                               [Identifier, :, AA6]   \n",
       "\n",
       "                          label_contextual_predicted  \\\n",
       "0                                                [O]   \n",
       "1                                                [O]   \n",
       "2                                             [O, O]   \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4                                          [O, O, O]   \n",
       "\n",
       "                           label_contextual_expected annotation_agreement  \n",
       "0                                                [O]        true negative  \n",
       "1                                                [O]        true negative  \n",
       "2                           [Occupation, Occupation]       false negative  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...        true negative  \n",
       "4                                          [O, O, O]        true negative  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cont_ann.head()  # Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>true negative</th>\n",
       "      <th>false negative</th>\n",
       "      <th>true positive</th>\n",
       "      <th>false positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linguistic</td>\n",
       "      <td>12841</td>\n",
       "      <td>632</td>\n",
       "      <td>1673</td>\n",
       "      <td>500</td>\n",
       "      <td>0.769903</td>\n",
       "      <td>0.725813</td>\n",
       "      <td>0.747209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Person Name</td>\n",
       "      <td>10607</td>\n",
       "      <td>1568</td>\n",
       "      <td>2701</td>\n",
       "      <td>770</td>\n",
       "      <td>0.778162</td>\n",
       "      <td>0.632701</td>\n",
       "      <td>0.697933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contextual</td>\n",
       "      <td>11878</td>\n",
       "      <td>1600</td>\n",
       "      <td>1442</td>\n",
       "      <td>726</td>\n",
       "      <td>0.665129</td>\n",
       "      <td>0.474030</td>\n",
       "      <td>0.553551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        labels  true negative  false negative  true positive  false positive  \\\n",
       "0   Linguistic          12841             632           1673             500   \n",
       "0  Person Name          10607            1568           2701             770   \n",
       "0   Contextual          11878            1600           1442             726   \n",
       "\n",
       "   precision    recall       f_1  \n",
       "0   0.769903  0.725813  0.747209  \n",
       "0   0.778162  0.632701  0.697933  \n",
       "0   0.665129  0.474030  0.553551  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_cont = utils.getAnnotationAgreementMetrics(pred_cont_ann, \"Contextual\")\n",
    "metrics_pers = utils.getAnnotationAgreementMetrics(pred_pers_ann, \"Person Name\")\n",
    "metrics_ling = utils.getAnnotationAgreementMetrics(pred_ling_ann, \"Linguistic\")\n",
    "metrics = pd.concat([metrics_ling, metrics_pers, metrics_cont])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"sequence_model_performance/crf_{a}_baseline/\".format(a=a)\n",
    "metrics.to_csv(\n",
    "    config.tokc_path+output_path+\"crf_{a}_baseline_fastText{d}_performance_annotagmt_alltags_cats.csv\".format(a=a, d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the metrics per label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loose Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the manual annotation evaluation, we want to evaluate the predictions more loosely, considering overlapping text spans in addition to exactly matching text spans.\n",
    "\n",
    "#### Token Agreement\n",
    "\n",
    "First, generalize the tokens' IOB tags to the label, and calculate agreement scores for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"contextual\"\n",
    "pred_cont_labels = pred_cont.copy()\n",
    "tag_exp = list(pred_cont_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_cont_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "# print(label_exp[:20])  # Looks good\n",
    "# print(label_pred[:20]) # Looks good\n",
    "pred_cont_labels = pred_cont_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_cont_labels.insert(len(pred_cont_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_cont_labels.insert(len(pred_cont_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_cont_labels.head(20)  # Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"personname\"\n",
    "pred_pers_labels = pred_pers.copy()\n",
    "tag_exp = list(pred_pers_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_pers_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_pers_labels = pred_pers_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_pers_labels.insert(len(pred_pers_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_pers_labels.insert(len(pred_pers_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_pers_labels.loc[pred_pers_labels.label_personname_predicted == \"Feminine\"].head()  # Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"linguistic\"\n",
    "pred_ling_labels = pred_ling.copy()\n",
    "tag_exp = list(pred_ling_labels[\"tag_{}_expected\".format(category)])\n",
    "tag_pred = list(pred_ling_labels[\"tag_{}_predicted\".format(category)])\n",
    "label_exp = [[tag if tag == \"O\" else tag[2:] for tag in tag_exp_list] for tag_exp_list in tag_exp]\n",
    "label_pred = [tag if tag == \"O\" else tag[2:] for tag in tag_pred]\n",
    "pred_ling_labels = pred_ling_labels.drop(columns=[\"tag_{}_expected\".format(category), \"tag_{}_predicted\".format(category)])\n",
    "pred_ling_labels.insert(len(pred_ling_labels.columns), \"label_{}_expected\".format(category), label_exp)\n",
    "pred_ling_labels.insert(len(pred_ling_labels.columns), \"label_{}_predicted\".format(category), label_pred)\n",
    "# pred_ling_labels.head()  # Looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the agreement metrics at the label level for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Occupation</td>\n",
       "      <td>965</td>\n",
       "      <td>111</td>\n",
       "      <td>470</td>\n",
       "      <td>0.808950</td>\n",
       "      <td>0.327526</td>\n",
       "      <td>0.466270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Omission</td>\n",
       "      <td>1822</td>\n",
       "      <td>163</td>\n",
       "      <td>622</td>\n",
       "      <td>0.792357</td>\n",
       "      <td>0.254501</td>\n",
       "      <td>0.385259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stereotype</td>\n",
       "      <td>907</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.038176</td>\n",
       "      <td>0.072948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0  Occupation             965             111            470   0.808950   \n",
       "0    Omission            1822             163            622   0.792357   \n",
       "0  Stereotype             907               8             36   0.818182   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.327526  0.466270  \n",
       "0  0.254501  0.385259  \n",
       "0  0.038176  0.072948  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"contextual\"\n",
    "tags = ['Occupation', 'Omission', 'Stereotype']\n",
    "pred_cont_labels = pred_cont_labels.drop(columns=[\"_merge\"])\n",
    "pred_cont_labels = utils.isPredictedInExpected(pred_cont_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "pred_cont_stats = utils.getScoresByCatTags(\n",
    "    pred_cont_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_cont_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_cont_stats = pd.concat([pred_cont_stats, tag_stats])\n",
    "pred_cont_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feminine</td>\n",
       "      <td>284</td>\n",
       "      <td>51</td>\n",
       "      <td>518</td>\n",
       "      <td>0.910369</td>\n",
       "      <td>0.645885</td>\n",
       "      <td>0.755653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masculine</td>\n",
       "      <td>809</td>\n",
       "      <td>207</td>\n",
       "      <td>613</td>\n",
       "      <td>0.747561</td>\n",
       "      <td>0.431083</td>\n",
       "      <td>0.546833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>2365</td>\n",
       "      <td>707</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.749912</td>\n",
       "      <td>0.472687</td>\n",
       "      <td>0.579869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0   Feminine             284              51            518   0.910369   \n",
       "0  Masculine             809             207            613   0.747561   \n",
       "0    Unknown            2365             707           2120   0.749912   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.645885  0.755653  \n",
       "0  0.431083  0.546833  \n",
       "0  0.472687  0.579869  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"personname\"\n",
    "tags = ['Feminine', 'Masculine', 'Unknown']\n",
    "pred_pers_labels = pred_pers_labels.drop(columns=[\"_merge\"])\n",
    "pred_pers_labels = utils.isPredictedInExpected(pred_pers_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "\n",
    "pred_pers_stats = utils.getScoresByCatTags(\n",
    "    pred_pers_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_pers_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_pers_stats = pd.concat([pred_pers_stats, tag_stats])\n",
    "pred_pers_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the Baseline Multilabel Classifier:\n",
    "* Feminine F1: 0.556837\n",
    "* Masculine F1: 0.322305\n",
    "* Unknown F1: 0.461489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag(s)</th>\n",
       "      <th>false negative</th>\n",
       "      <th>false positive</th>\n",
       "      <th>true positive</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Pronoun</td>\n",
       "      <td>28</td>\n",
       "      <td>172</td>\n",
       "      <td>731</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.963109</td>\n",
       "      <td>0.879663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gendered-Role</td>\n",
       "      <td>292</td>\n",
       "      <td>141</td>\n",
       "      <td>401</td>\n",
       "      <td>0.739852</td>\n",
       "      <td>0.578644</td>\n",
       "      <td>0.649393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalization</td>\n",
       "      <td>284</td>\n",
       "      <td>24</td>\n",
       "      <td>72</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.318584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tag(s)  false negative  false positive  true positive  precision  \\\n",
       "0  Gendered-Pronoun              28             172            731   0.809524   \n",
       "0     Gendered-Role             292             141            401   0.739852   \n",
       "0    Generalization             284              24             72   0.750000   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.963109  0.879663  \n",
       "0  0.578644  0.649393  \n",
       "0  0.202247  0.318584  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"linguistic\"\n",
    "tags = [\"Gendered-Pronoun\", \"Gendered-Role\", \"Generalization\"]\n",
    "pred_ling_labels = pred_ling_labels.drop(columns=[\"_merge\"])\n",
    "pred_ling_labels = utils.isPredictedInExpected(pred_ling_labels, \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), '_merge', 'O')\n",
    "\n",
    "\n",
    "pred_ling_stats = utils.getScoresByCatTags(\n",
    "    pred_ling_labels, \"_merge\", tags[0], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    ")\n",
    "for i in range(1, len(tags)):\n",
    "    tag_stats = utils.getScoresByCatTags(\n",
    "        pred_ling_labels, \"_merge\", tags[i], \"label_{}_expected\".format(category), \"label_{}_predicted\".format(category), \"token_id\"\n",
    "    )\n",
    "    pred_ling_stats = pd.concat([pred_ling_stats, tag_stats])\n",
    "pred_ling_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the Baseline Multilabel Classifier:\n",
    "* Gendered Pronoun F1: 0.908933\n",
    "* Gendered Role F1: 0.564103\n",
    "* Generalization F1: 0.594937"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine and save the performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_stats = pd.concat([pred_cont_stats, pred_pers_stats, pred_ling_stats])\n",
    "# loose_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"sequence_model_performance/crf_{a}_baseline/\".format(a=a)\n",
    "loose_stats.to_csv(\n",
    "    config.tokc_path+output_path+\"crf_{a}_baseline_fastText{d}_performance_loose_alltags.csv\".format(a=a, d=d)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"A\"></a>\n",
    "## Appendix A: Person Name Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "Look for the highest-performing (based on F1 score) models by trying different algorithms and parameters.  Algorithms vailable with sklearn_crfsuite are:\n",
    " * 'lbfgs' - Gradient descent using the L-BFGS method\n",
    " * 'l2sgd' - Stochastic Gradient Descent with L2 regularization term\n",
    " * 'ap' - Averaged Perceptron\n",
    " * 'pa' - Passive Aggressive (PA)\n",
    " * 'arow' - Adaptive Regularization Of Weight Vector (AROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow']\n",
    "max_iters=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[\"tag_personname\"].unique()\n",
    "targets = [\n",
    "        'B-Unknown', 'I-Unknown', 'B-Feminine',\n",
    "        'I-Feminine', 'B-Masculine', 'I-Masculine'\n",
    "]\n",
    "f1_scorer = make_scorer(\n",
    "    metrics.flat_f1_score, average='None', \n",
    "    labels=targets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf0A = sklearn_crfsuite.CRF(algorithm=algorithms[0], c1=0, max_iterations=max_iters, all_possible_transitions=True) # unlimited iterations\n",
    "crf0B = sklearn_crfsuite.CRF(algorithm=algorithms[0], c1=0.1, max_iterations=max_iters, all_possible_transitions=True)\n",
    "crf0C = sklearn_crfsuite.CRF(algorithm=algorithms[0], c1=0.1, c2=0.2, max_iterations=max_iters, all_possible_transitions=True)\n",
    "\n",
    "crf1A = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=1.0, max_iterations=max_iters, all_possible_transitions=True) # max iters: 1000\n",
    "crf1B = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=0.2, max_iterations=max_iters, all_possible_transitions=True)\n",
    "\n",
    "crf2 = sklearn_crfsuite.CRF(algorithm=algorithms[2], max_iterations=max_iters, all_possible_transitions=True) # max iters: 100\n",
    "\n",
    "crf3A = sklearn_crfsuite.CRF(algorithm=algorithms[3], pa_type=0, max_iterations=max_iters, all_possible_transitions=True) # max iters: 100\n",
    "crf3B = sklearn_crfsuite.CRF(algorithm=algorithms[3], pa_type=1, max_iterations=max_iters, all_possible_transitions=True)\n",
    "crf3C = sklearn_crfsuite.CRF(algorithm=algorithms[3], pa_type=2, max_iterations=max_iters, all_possible_transitions=True)\n",
    "\n",
    "crf4A = sklearn_crfsuite.CRF(algorithm=algorithms[4], variance=1, max_iterations=max_iters, all_possible_transitions=True) # max iters: 100\n",
    "crf4B = sklearn_crfsuite.CRF(algorithm=algorithms[4], variance=0.5, max_iterations=max_iters, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0 None None None\n",
      "  Weighted:\n",
      "  - F1: 0.25563333936374655\n",
      "  - Prec: 0.48241170869152883\n",
      "  - Rec 0.17545363623374768\n",
      "  Unweighted:\n",
      "  - F1: [0.21609604 0.24892487 0.40752351 0.28761651 0.32380952 0.23603462]\n",
      "  - Prec: [0.42631579 0.42087254 0.77380952 0.72       0.51987768 0.51020408]\n",
      "  - Rec [0.14472901 0.17672414 0.27659574 0.1797005  0.2351314  0.15353122]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf0A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf0A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf0A.algorithm, crf0A.c1, crf0A.c2, crf0A.pa_type, crf0A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0.1 None None None\n",
      "  Weighted:\n",
      "  - F1: 0.3952936125163706\n",
      "  - Prec: 0.6164456508900145\n",
      "  - Rec 0.2961851693099014\n",
      "  Unweighted:\n",
      "  - F1: [0.34542314 0.37176232 0.62222222 0.53304904 0.43134087 0.38205128]\n",
      "  - Prec: [0.62794349 0.63431542 0.74117647 0.74183976 0.5184466  0.51114923]\n",
      "  - Rec [0.23823705 0.26293103 0.53617021 0.41597338 0.36929461 0.30501535]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf0B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf0B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf0B.algorithm, crf0B.c1, crf0B.c2, crf0B.pa_type, crf0B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0.1 0.2 None None\n",
      "  Weighted:\n",
      "  - F1: 0.4505459769939627\n",
      "  - Prec: 0.6028142561062638\n",
      "  - Rec 0.36133733390484357\n",
      "  Unweighted:\n",
      "  - F1: [0.45994065 0.48070953 0.63333333 0.5320911  0.38327526 0.30410184]\n",
      "  - Prec: [0.60963618 0.62804171 0.71891892 0.70410959 0.51764706 0.49199085]\n",
      "  - Rec [0.36926742 0.38936782 0.56595745 0.42762063 0.30428769 0.22006141]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf0C.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf0C.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf0C.algorithm, crf0C.c1, crf0C.c2, crf0C.pa_type, crf0C.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2sgd None 1.0 None None\n",
      "  Weighted:\n",
      "  - F1: 0.39947172781326473\n",
      "  - Prec: 0.5390631041498564\n",
      "  - Rec 0.31975996570938703\n",
      "  Unweighted:\n",
      "  - F1: [0.37020484 0.37389855 0.5990566  0.5520728  0.44057052 0.35034657]\n",
      "  - Prec: [0.49403579 0.55477032 0.67195767 0.70360825 0.51576994 0.4557377 ]\n",
      "  - Rec [0.29600953 0.28196839 0.54042553 0.45424293 0.38450899 0.28454452]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf1A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf1A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf1A.algorithm, crf1A.c1, crf1A.c2, crf1A.pa_type, crf1A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2sgd None 0.2 None None\n",
      "  Weighted:\n",
      "  - F1: 0.28237156184801626\n",
      "  - Prec: 0.6289538389122998\n",
      "  - Rec 0.19402771824546364\n",
      "  Unweighted:\n",
      "  - F1: [0.26691042 0.26218487 0.63341646 0.54115226 0.31621349 0.09779482]\n",
      "  - Prec: [0.57367387 0.59541985 0.76506024 0.70889488 0.58148148 0.77272727]\n",
      "  - Rec [0.17391304 0.16810345 0.54042553 0.43760399 0.21715076 0.05220061]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf1B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf1B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf1B.algorithm, crf1B.c1, crf1B.c2, crf1B.pa_type, crf1B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap None None None None\n",
      "  Weighted:\n",
      "  - F1: 0.43418543421107464\n",
      "  - Prec: 0.6506056343043833\n",
      "  - Rec 0.33190455779397054\n",
      "  Unweighted:\n",
      "  - F1: [0.41818182 0.42871094 0.6367713  0.60142712 0.46962233 0.29945694]\n",
      "  - Prec: [0.62162162 0.66920732 0.67298578 0.77631579 0.57777778 0.61858974]\n",
      "  - Rec [0.31506849 0.31537356 0.60425532 0.49084859 0.395574   0.1975435 ]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf2.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf2.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf2.algorithm, crf2.c1, crf2.c2, crf2.pa_type, crf2.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa None None 0 None\n",
      "  Weighted:\n",
      "  - F1: 0.46544025425232854\n",
      "  - Prec: 0.6526455542987322\n",
      "  - Rec 0.36676668095442205\n",
      "  Unweighted:\n",
      "  - F1: [0.46613697 0.48264984 0.66350711 0.59205021 0.45128205 0.30015552]\n",
      "  - Prec: [0.63900415 0.64752116 0.7486631  0.7971831  0.59060403 0.62459547]\n",
      "  - Rec [0.36688505 0.38469828 0.59574468 0.47088186 0.36514523 0.1975435 ]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf3A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf3A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf3A.algorithm, crf3A.c1, crf3A.c2, crf3A.pa_type, crf3A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa None None 1 None\n",
      "  Weighted:\n",
      "  - F1: 0.46124270643488524\n",
      "  - Prec: 0.6487355256491798\n",
      "  - Rec 0.36233747678239747\n",
      "  Unweighted:\n",
      "  - F1: [0.46369138 0.47971145 0.66019417 0.58029979 0.44633731 0.29434547]\n",
      "  - Prec: [0.63523316 0.6440678  0.76836158 0.81381381 0.58093126 0.60509554]\n",
      "  - Rec [0.36509827 0.38218391 0.5787234  0.45091514 0.36237898 0.19447288]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf3B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf3B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf3B.algorithm, crf3B.c1, crf3B.c2, crf3B.pa_type, crf3B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa None None 2 None\n",
      "  Weighted:\n",
      "  - F1: 0.4648015025167207\n",
      "  - Prec: 0.6483430407678211\n",
      "  - Rec 0.36748106872410347\n",
      "  Unweighted:\n",
      "  - F1: [0.46373544 0.48826291 0.65876777 0.58201058 0.4467354  0.29439252]\n",
      "  - Prec: [0.62830957 0.64653641 0.74331551 0.7994186  0.58956916 0.61563518]\n",
      "  - Rec [0.36748064 0.39224138 0.59148936 0.45757072 0.35961272 0.19344933]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf3C.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf3C.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf3C.algorithm, crf3C.c1, crf3C.c2, crf3C.pa_type, crf3C.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arow None None None 1\n",
      "  Weighted:\n",
      "  - F1: 0.4815200566676591\n",
      "  - Prec: 0.5175284691788858\n",
      "  - Rec 0.46506643806258036\n",
      "  Unweighted:\n",
      "  - F1: [0.45074415 0.48698438 0.6437247  0.59171598 0.51690294 0.38585209]\n",
      "  - Prec: [0.55643045 0.55022624 0.61389961 0.60137457 0.42664266 0.35      ]\n",
      "  - Rec [0.3787969  0.43678161 0.67659574 0.58236273 0.65560166 0.42988741]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf4A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf4A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf4A.algorithm, crf4A.c1, crf4A.c2, crf4A.pa_type, crf4A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arow None None None 0.5\n",
      "  Weighted:\n",
      "  - F1: 0.4947955715164867\n",
      "  - Prec: 0.5537045694622348\n",
      "  - Rec 0.44920702957565367\n",
      "  Unweighted:\n",
      "  - F1: [0.50312809 0.51784329 0.65154639 0.62031107 0.41166937 0.36140135]\n",
      "  - Prec: [0.56259205 0.56281619 0.632      0.68902439 0.49706458 0.45230769]\n",
      "  - Rec [0.45503276 0.47952586 0.67234043 0.5640599  0.35131397 0.30092119]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf4B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf4B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf4B.algorithm, crf4B.c1, crf4B.c2, crf4B.pa_type, crf4B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best model:** arow with variance=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"B\"></a>\n",
    "## Appendix B: Linguistic Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "Look for the highest-performing (based on F1 score) models by trying different algorithms and parameters.  Algorithms vailable with sklearn_crfsuite are:\n",
    " * 'lbfgs' - Gradient descent using the L-BFGS method\n",
    " * 'l2sgd' - Stochastic Gradient Descent with L2 regularization term\n",
    " * 'ap' - Averaged Perceptron\n",
    " * 'pa' - Passive Aggressive (PA)\n",
    " * 'arow' - Adaptive Regularization Of Weight Vector (AROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow']\n",
    "max_iters=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[\"tag_linguistic\"].unique()\n",
    "targets = [\n",
    "        'B-Gendered-Pronoun', 'I-Gendered-Pronoun', 'B-Generalization',\n",
    "        'I-Generalization', 'B-Gendered-Role', 'I-Gendered-Role'\n",
    "]\n",
    "f1_scorer = make_scorer(\n",
    "    metrics.flat_f1_score, average='None', \n",
    "    labels=targets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf0A = sklearn_crfsuite.CRF(algorithm=algorithms[0], c1=0, max_iterations=max_iters, all_possible_transitions=True) # unlimited iterations\n",
    "crf0B = sklearn_crfsuite.CRF(algorithm=algorithms[0], c1=0.1, max_iterations=max_iters, all_possible_transitions=True)\n",
    "crf0C = sklearn_crfsuite.CRF(algorithm=algorithms[0], c1=0.1, c2=0.2, max_iterations=max_iters, all_possible_transitions=True)\n",
    "\n",
    "crf1A = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=1.0, max_iterations=max_iters, all_possible_transitions=True) # max iters: 1000\n",
    "crf1B = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=0.2, max_iterations=max_iters, all_possible_transitions=True)\n",
    "\n",
    "crf2 = sklearn_crfsuite.CRF(algorithm=algorithms[2], max_iterations=max_iters, all_possible_transitions=True) # max iters: 100\n",
    "\n",
    "crf3A = sklearn_crfsuite.CRF(algorithm=algorithms[3], pa_type=0, max_iterations=max_iters, all_possible_transitions=True) # max iters: 100\n",
    "crf3B = sklearn_crfsuite.CRF(algorithm=algorithms[3], pa_type=1, max_iterations=max_iters, all_possible_transitions=True)\n",
    "crf3C = sklearn_crfsuite.CRF(algorithm=algorithms[3], pa_type=2, max_iterations=max_iters, all_possible_transitions=True)\n",
    "\n",
    "crf4A = sklearn_crfsuite.CRF(algorithm=algorithms[4], variance=1, max_iterations=max_iters, all_possible_transitions=True) # max iters: 100\n",
    "crf4B = sklearn_crfsuite.CRF(algorithm=algorithms[4], variance=0.5, max_iterations=max_iters, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0 None None None\n",
      "  Weighted:\n",
      "  - F1: 0.5220463286505957\n",
      "  - Prec: 0.6424500624544645\n",
      "  - Rec 0.4922135706340378\n",
      "  Unweighted:\n",
      "  - F1: [0.85307443 0.         0.00892857 0.         0.48390942 0.29530201]\n",
      "  - Prec: [0.81559406 0.         0.25       0.         0.74087591 0.6875    ]\n",
      "  - Rec [0.89416554 0.         0.00454545 0.         0.35929204 0.18803419]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf0A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf0A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf0A.algorithm, crf0A.c1, crf0A.c2, crf0A.pa_type, crf0A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0.1 None None None\n",
      "  Weighted:\n",
      "  - F1: 0.6083674858352446\n",
      "  - Prec: 0.7689434393136277\n",
      "  - Rec 0.60734149054505\n",
      "  Unweighted:\n",
      "  - F1: [0.87484511 0.         0.05286344 0.05298013 0.67586207 0.40993789]\n",
      "  - Prec: [0.8050171  0.         0.85714286 0.57142857 0.76222222 0.75      ]\n",
      "  - Rec [0.95793758 0.         0.02727273 0.02777778 0.60707965 0.28205128]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf0B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf0B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf0B.algorithm, crf0B.c1, crf0B.c2, crf0B.pa_type, crf0B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0.1 0.2 None None\n",
      "  Weighted:\n",
      "  - F1: 0.6427339974408373\n",
      "  - Prec: 0.7651587413557269\n",
      "  - Rec 0.6218020022246941\n",
      "  Unweighted:\n",
      "  - F1: [0.85677912 0.         0.28679245 0.15662651 0.6903164  0.41463415]\n",
      "  - Prec: [0.80695444 0.         0.84444444 0.59090909 0.75313808 0.72340426]\n",
      "  - Rec [0.91316147 0.         0.17272727 0.09027778 0.63716814 0.29059829]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf0C.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf0C.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf0C.algorithm, crf0C.c1, crf0C.c2, crf0C.pa_type, crf0C.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2sgd None 1.0 None None\n",
      "  Weighted:\n",
      "  - F1: 0.5811224023583894\n",
      "  - Prec: 0.6272431558647711\n",
      "  - Rec 0.5817575083426029\n",
      "  Unweighted:\n",
      "  - F1: [0.88389058 0.         0.         0.         0.62406816 0.34899329]\n",
      "  - Prec: [0.80066079 0.         0.         0.         0.78342246 0.8125    ]\n",
      "  - Rec [0.98643148 0.         0.         0.         0.51858407 0.22222222]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf1A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf1A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf1A.algorithm, crf1A.c1, crf1A.c2, crf1A.pa_type, crf1A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2sgd None 0.2 None None\n",
      "  Weighted:\n",
      "  - F1: 0.517290080102963\n",
      "  - Prec: 0.7148552332736035\n",
      "  - Rec 0.514460511679644\n",
      "  Unweighted:\n",
      "  - F1: [0.88242424 0.         0.00904977 0.         0.41344956 0.37735849]\n",
      "  - Prec: [0.7973713  0.         1.         0.         0.69747899 0.71428571]\n",
      "  - Rec [0.98778833 0.         0.00454545 0.         0.29380531 0.25641026]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf1B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf1B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf1B.algorithm, crf1B.c1, crf1B.c2, crf1B.pa_type, crf1B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap None None None None\n",
      "  Weighted:\n",
      "  - F1: 0.6014970716276341\n",
      "  - Prec: 0.8224996619695852\n",
      "  - Rec 0.5912124582869855\n",
      "  Unweighted:\n",
      "  - F1: [0.87876923 0.64       0.07017544 0.10457516 0.63731656 0.28767123]\n",
      "  - Prec: [0.80405405 0.8        1.         0.88888889 0.781491   0.72413793]\n",
      "  - Rec [0.9687924  0.53333333 0.03636364 0.05555556 0.5380531  0.17948718]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf2.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf2.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf2.algorithm, crf2.c1, crf2.c2, crf2.pa_type, crf2.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa None None 0 None\n",
      "  Weighted:\n",
      "  - F1: 0.6608095292949409\n",
      "  - Prec: 0.7676344251172861\n",
      "  - Rec 0.6551724137931034\n",
      "  Unweighted:\n",
      "  - F1: [0.88456865 0.69230769 0.29104478 0.23255814 0.67378641 0.40697674]\n",
      "  - Prec: [0.80088009 0.81818182 0.8125     0.71428571 0.74623656 0.63636364]\n",
      "  - Rec [0.98778833 0.6        0.17727273 0.13888889 0.61415929 0.2991453 ]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf3A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf3A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf3A.algorithm, crf3A.c1, crf3A.c2, crf3A.pa_type, crf3A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa None None 1 None\n",
      "  Weighted:\n",
      "  - F1: 0.6599815730795738\n",
      "  - Prec: 0.762389078290318\n",
      "  - Rec 0.6490545050055617\n",
      "  Unweighted:\n",
      "  - F1: [0.87945879 0.69230769 0.29927007 0.24277457 0.67249757 0.40462428]\n",
      "  - Prec: [0.80427447 0.81818182 0.75925926 0.72413793 0.74568966 0.625     ]\n",
      "  - Rec [0.97014925 0.6        0.18636364 0.14583333 0.61238938 0.2991453 ]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf3B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf3B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf3B.algorithm, crf3B.c1, crf3B.c2, crf3B.pa_type, crf3B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa None None 2 None\n",
      "  Weighted:\n",
      "  - F1: 0.6597046279568559\n",
      "  - Prec: 0.7595230166063811\n",
      "  - Rec 0.6490545050055617\n",
      "  Unweighted:\n",
      "  - F1: [0.87730061 0.69230769 0.30434783 0.25142857 0.67120623 0.4       ]\n",
      "  - Prec: [0.80067189 0.81818182 0.75       0.70967742 0.74514039 0.64150943]\n",
      "  - Rec [0.97014925 0.6        0.19090909 0.15277778 0.61061947 0.29059829]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf3C.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf3C.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf3C.algorithm, crf3C.c1, crf3C.c2, crf3C.pa_type, crf3C.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arow None None None 1\n",
      "  Weighted:\n",
      "  - F1: 0.6586210838588668\n",
      "  - Prec: 0.6726122299605511\n",
      "  - Rec 0.664071190211346\n",
      "  Unweighted:\n",
      "  - F1: [0.87015385 0.54545455 0.28490028 0.27237354 0.66475645 0.48913043]\n",
      "  - Prec: [0.79617117 0.5        0.38167939 0.30973451 0.7219917  0.67164179]\n",
      "  - Rec [0.95929444 0.6        0.22727273 0.24305556 0.6159292  0.38461538]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf4A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf4A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf4A.algorithm, crf4A.c1, crf4A.c2, crf4A.pa_type, crf4A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arow None None None 0.5\n",
      "  Weighted:\n",
      "  - F1: 0.6488288150410274\n",
      "  - Prec: 0.6222010099532319\n",
      "  - Rec 0.6846496106785317\n",
      "  Unweighted:\n",
      "  - F1: [0.86294416 0.69230769 0.29045643 0.21761658 0.67236955 0.38541667]\n",
      "  - Prec: [0.81048868 0.81818182 0.26717557 0.17355372 0.65066225 0.49333333]\n",
      "  - Rec [0.92265943 0.6        0.31818182 0.29166667 0.69557522 0.31623932]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf4B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf4B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf4B.algorithm, crf4B.c1, crf4B.c2, crf4B.pa_type, crf4B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best model:** pa with pa_type=0; arow with variance=0.5 also has strong performance, and is strongest with other categories, Person Name and Contextual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"C\"></a>\n",
    "## Appendix C: Contextual Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "Look for the highest-performing (based on F1 score) models by trying different algorithms and parameters.  Algorithms vailable with sklearn_crfsuite are:\n",
    " * 'lbfgs' - Gradient descent using the L-BFGS method\n",
    " * 'l2sgd' - Stochastic Gradient Descent with L2 regularization term\n",
    " * 'ap' - Averaged Perceptron\n",
    " * 'pa' - Passive Aggressive (PA)\n",
    " * 'arow' - Adaptive Regularization Of Weight Vector (AROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow']\n",
    "max_iters=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[\"tag_contextual\"].unique()\n",
    "targets = [\n",
    "        'B-Occupation', 'I-Occupation', 'B-Stereotype',\n",
    "        'I-Stereotype', 'B-Omission', 'I-Omission'\n",
    "]\n",
    "f1_scorer = make_scorer(\n",
    "    metrics.flat_f1_score, average='None', \n",
    "    labels=targets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf0A = sklearn_crfsuite.CRF(algorithm=algorithms[0], c1=0, max_iterations=max_iters, all_possible_transitions=True) # unlimited iterations\n",
    "crf0B = sklearn_crfsuite.CRF(algorithm=algorithms[0], c1=0.1, max_iterations=max_iters, all_possible_transitions=True)\n",
    "crf0C = sklearn_crfsuite.CRF(algorithm=algorithms[0], c1=0.1, c2=0.2, max_iterations=max_iters, all_possible_transitions=True)\n",
    "\n",
    "crf1A = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=1.0, max_iterations=max_iters, all_possible_transitions=True) # max iters: 1000\n",
    "crf1B = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=0.2, max_iterations=max_iters, all_possible_transitions=True)\n",
    "\n",
    "crf2 = sklearn_crfsuite.CRF(algorithm=algorithms[2], max_iterations=max_iters, all_possible_transitions=True) # max iters: 100\n",
    "\n",
    "crf3A = sklearn_crfsuite.CRF(algorithm=algorithms[3], pa_type=0, max_iterations=max_iters, all_possible_transitions=True) # max iters: 100\n",
    "crf3B = sklearn_crfsuite.CRF(algorithm=algorithms[3], pa_type=1, max_iterations=max_iters, all_possible_transitions=True)\n",
    "crf3C = sklearn_crfsuite.CRF(algorithm=algorithms[3], pa_type=2, max_iterations=max_iters, all_possible_transitions=True)\n",
    "\n",
    "crf4A = sklearn_crfsuite.CRF(algorithm=algorithms[4], variance=1, max_iterations=max_iters, all_possible_transitions=True) # max iters: 100\n",
    "crf4B = sklearn_crfsuite.CRF(algorithm=algorithms[4], variance=0.5, max_iterations=max_iters, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0 None None None\n",
      "  Weighted:\n",
      "  - F1: 0.07526574269976748\n",
      "  - Prec: 0.3419825203723303\n",
      "  - Rec 0.04234527687296417\n",
      "  Unweighted:\n",
      "  - F1: [0.         0.         0.         0.         0.18710263 0.11749681]\n",
      "  - Prec: [0.         0.         0.         0.         0.76865672 0.58974359]\n",
      "  - Rec [0.         0.         0.         0.         0.10651499 0.06524823]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf0A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf0A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf0A.algorithm, crf0A.c1, crf0A.c2, crf0A.pa_type, crf0A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0.1 None None None\n",
      "  Weighted:\n",
      "  - F1: 0.27203523227916193\n",
      "  - Prec: 0.6341356146479743\n",
      "  - Rec 0.1780673181324647\n",
      "  Unweighted:\n",
      "  - F1: [0.27612903 0.25559105 0.11299435 0.16603774 0.48888889 0.1993205 ]\n",
      "  - Prec: [0.75352113 0.65934066 0.58823529 0.56410256 0.79672897 0.49438202]\n",
      "  - Rec [0.16903633 0.15852048 0.0625     0.09734513 0.35263702 0.1248227 ]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf0B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf0B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf0B.algorithm, crf0B.c1, crf0B.c2, crf0B.pa_type, crf0B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs 0.1 0.2 None None\n",
      "  Weighted:\n",
      "  - F1: 0.36974453222807907\n",
      "  - Prec: 0.6650245193879263\n",
      "  - Rec 0.2625407166123778\n",
      "  Unweighted:\n",
      "  - F1: [0.47764449 0.42293907 0.16304348 0.22738386 0.53013699 0.27465536]\n",
      "  - Prec: [0.77112676 0.65738162 0.625      0.66428571 0.78498986 0.54411765]\n",
      "  - Rec [0.34597156 0.31175694 0.09375    0.13716814 0.40020683 0.18368794]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf0C.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf0C.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf0C.algorithm, crf0C.c1, crf0C.c2, crf0C.pa_type, crf0C.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2sgd None 1.0 None None\n",
      "  Weighted:\n",
      "  - F1: 0.14238250038533273\n",
      "  - Prec: 0.6949935082632666\n",
      "  - Rec 0.08534201954397394\n",
      "  Unweighted:\n",
      "  - F1: [0.01253918 0.01308901 0.02325581 0.05890603 0.33637117 0.1907061 ]\n",
      "  - Prec: [0.8        0.71428571 0.16666667 0.6        0.84583333 0.63967611]\n",
      "  - Rec [0.00631912 0.00660502 0.0125     0.03097345 0.20992761 0.11205674]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf1A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf1A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf1A.algorithm, crf1A.c1, crf1A.c2, crf1A.pa_type, crf1A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2sgd None 0.2 None None\n",
      "  Weighted:\n",
      "  - F1: 0.23290156335890247\n",
      "  - Prec: 0.587957004746522\n",
      "  - Rec 0.16547231270358306\n",
      "  Unweighted:\n",
      "  - F1: [0.03703704 0.06532663 0.17894737 0.16252822 0.51733333 0.25569358]\n",
      "  - Prec: [0.8        0.66666667 0.56666667 0.34615385 0.72795497 0.47318008]\n",
      "  - Rec [0.01895735 0.0343461  0.10625    0.10619469 0.40124095 0.1751773 ]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf1B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf1B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf1B.algorithm, crf1B.c1, crf1B.c2, crf1B.pa_type, crf1B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap None None None None\n",
      "  Weighted:\n",
      "  - F1: 0.22686387330210564\n",
      "  - Prec: 0.8443018920394695\n",
      "  - Rec 0.14505971769815418\n",
      "  Unweighted:\n",
      "  - F1: [0.26168224 0.20303384 0.02453988 0.03478261 0.49893086 0.15275995]\n",
      "  - Prec: [0.84482759 0.87       0.66666667 1.         0.80275229 0.80405405]\n",
      "  - Rec [0.15481833 0.11492734 0.0125     0.01769912 0.36194416 0.08439716]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf2.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf2.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf2.algorithm, crf2.c1, crf2.c2, crf2.pa_type, crf2.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa None None 0 None\n",
      "  Weighted:\n",
      "  - F1: 0.306310049934223\n",
      "  - Prec: 0.80194162836387\n",
      "  - Rec 0.20781758957654722\n",
      "  Unweighted:\n",
      "  - F1: [0.45810056 0.32640333 0.08284024 0.04329004 0.5375603  0.22061483]\n",
      "  - Prec: [0.78244275 0.76585366 0.77777778 1.         0.80578512 0.73493976]\n",
      "  - Rec [0.32385466 0.20739762 0.04375    0.02212389 0.4033092  0.12978723]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf3A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf3A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf3A.algorithm, crf3A.c1, crf3A.c2, crf3A.pa_type, crf3A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa None None 1 None\n",
      "  Weighted:\n",
      "  - F1: 0.30799201369715534\n",
      "  - Prec: 0.8058167663636654\n",
      "  - Rec 0.20868621064060802\n",
      "  Unweighted:\n",
      "  - F1: [0.45240761 0.32432432 0.08284024 0.04892086 0.5399449  0.22543701]\n",
      "  - Prec: [0.77692308 0.76097561 0.77777778 1.         0.80824742 0.75100402]\n",
      "  - Rec [0.31911532 0.20607662 0.04375    0.02507375 0.40537746 0.13262411]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf3B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf3B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf3B.algorithm, crf3B.c1, crf3B.c2, crf3B.pa_type, crf3B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa None None 2 None\n",
      "  Weighted:\n",
      "  - F1: 0.30804522569488546\n",
      "  - Prec: 0.809491042440604\n",
      "  - Rec 0.20912052117263843\n",
      "  Unweighted:\n",
      "  - F1: [0.46784922 0.32398754 0.08333333 0.04610951 0.53830228 0.22128174]\n",
      "  - Prec: [0.78438662 0.75728155 0.875      1.         0.80912863 0.75      ]\n",
      "  - Rec [0.33333333 0.20607662 0.04375    0.02359882 0.4033092  0.12978723]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf3C.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf3C.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf3C.algorithm, crf3C.c1, crf3C.c2, crf3C.pa_type, crf3C.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arow None None None 1\n",
      "  Weighted:\n",
      "  - F1: 0.36669929570944\n",
      "  - Prec: 0.40929601777426833\n",
      "  - Rec 0.3355048859934853\n",
      "  Unweighted:\n",
      "  - F1: [0.6036036  0.5095057  0.24512535 0.24287653 0.37733645 0.24971537]\n",
      "  - Prec: [0.70230608 0.60035842 0.22110553 0.22487437 0.43355705 0.26857143]\n",
      "  - Rec [0.52922591 0.44253633 0.275      0.2640118  0.33402275 0.23333333]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf4A.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf4A.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf4A.algorithm, crf4A.c1, crf4A.c2, crf4A.pa_type, crf4A.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arow None None None 0.5\n",
      "  Weighted:\n",
      "  - F1: 0.3925447072732632\n",
      "  - Prec: 0.43471962523819374\n",
      "  - Rec 0.36503800217155263\n",
      "  Unweighted:\n",
      "  - F1: [0.57769653 0.50433526 0.21782178 0.22643746 0.42546064 0.32653061]\n",
      "  - Prec: [0.68546638 0.55661882 0.18032787 0.18929633 0.46237864 0.38461538]\n",
      "  - Rec [0.49921011 0.46103038 0.275      0.28171091 0.39400207 0.28368794]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    crf4B.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Predict\n",
    "y_pred = crf4B.predict(X_dev)\n",
    "\n",
    "# Evaluate\n",
    "print(crf4B.algorithm, crf4B.c1, crf4B.c2, crf4B.pa_type, crf4B.variance)\n",
    "print(\"  Weighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", zero_division=0, labels=targets))\n",
    "print(\"  Unweighted:\")\n",
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=None, zero_division=0, labels=targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best model:** arow with variance=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

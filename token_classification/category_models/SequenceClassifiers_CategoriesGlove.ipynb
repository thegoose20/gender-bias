{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Gender Biased Token Classifiers\n",
    "\n",
    "### Target: Label Categories\n",
    "\n",
    "### Word Embeddings: GloVe\n",
    "\n",
    "* Supervised learning\n",
    "    * Train, Validate, and (Blind) Test Data: under directory `../data/token_clf_data/model_input/`\n",
    "    * Prediction Data: under directory `../data/token_clf_data/model_output`\n",
    "* Multilabel classification\n",
    "    * 3 categories of labels: Linguistic, Person Name, Contextual\n",
    "* Word Embeddings\n",
    "    * GloVe (trained on Common Crawl + English Wikipedia dump)\n",
    "\n",
    "***\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "**[0.](#0) Preprocessing**\n",
    "\n",
    "**[1.](#1) Models**\n",
    "\n",
    "**[2.](#2) Performance Evaluation**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom functions and variables\n",
    "import utils, config\n",
    "\n",
    "# For data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "# For creating directories\n",
    "from pathlib import Path\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import scipy.stats\n",
    "\n",
    "# For classification\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# For evaluation\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, ConfusionMatrixDisplay #, plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from intervaltree import Interval, IntervalTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## 0. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the train and validation (dev) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(467564, 10) (157740, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>3</td>\n",
       "      <td>Title</td>\n",
       "      <td>(17, 22)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>4</td>\n",
       "      <td>:</td>\n",
       "      <td>(22, 23)</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>5</td>\n",
       "      <td>Papers</td>\n",
       "      <td>(24, 30)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>6</td>\n",
       "      <td>of</td>\n",
       "      <td>(31, 33)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14384</td>\n",
       "      <td>7</td>\n",
       "      <td>The</td>\n",
       "      <td>(34, 37)</td>\n",
       "      <td>DT</td>\n",
       "      <td>B-Unknown</td>\n",
       "      <td>Title</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description_id  sentence_id  ann_id  token_id   token token_offsets  pos  \\\n",
       "3               1            1   99999         3   Title      (17, 22)   NN   \n",
       "4               1            1   99999         4       :      (22, 23)    :   \n",
       "5               1            1   99999         5  Papers      (24, 30)  NNS   \n",
       "6               1            1   99999         6      of      (31, 33)   IN   \n",
       "7               1            1   14384         7     The      (34, 37)   DT   \n",
       "\n",
       "         tag  field subset  \n",
       "3          O  Title  train  \n",
       "4          O  Title  train  \n",
       "5          O  Title  train  \n",
       "6          O  Title  train  \n",
       "7  B-Unknown  Title  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(config.tokc_path+\"model_input/token_train.csv\", index_col=0)\n",
    "df_dev = pd.read_csv(config.tokc_path+\"model_input/token_validate.csv\", index_col=0)\n",
    "print(df_train.shape, df_dev.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicate rows with all but the same annotation ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463441, 9) (156146, 9)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop(columns=[\"ann_id\"])\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev.drop(columns=[\"ann_id\"])\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "print(df_train.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Non-binary labels as these were mistaken labels identified early on that were meant to be excluded, and because only one token has this label, it prevents the data from being input into the models with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[df_train.tag != \"B-Nonbinary\"]\n",
    "df_train = df_train.loc[df_train.tag != \"I-Nonbinary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(463439, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Label Categories\n",
    "\n",
    "Add the annotation label categories as a column of higher-level Inside-Outside-Beginning (IOB) tags so they can be used as targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = utils.addCategoryTagColumn(df_train)\n",
    "# df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>field</th>\n",
       "      <th>subset</th>\n",
       "      <th>tag_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>(907, 912)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>(913, 916)</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>B-Linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>(917, 927)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>(928, 930)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Gendered-Pronoun</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>B-Linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>(931, 936)</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>Biographical / Historical</td>\n",
       "      <td>dev</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     description_id  sentence_id  token_id       token token_offsets   pos  \\\n",
       "172               3            5       154       After    (907, 912)    IN   \n",
       "173               3            5       155         his    (913, 916)  PRP$   \n",
       "174               3            5       156  ordination    (917, 927)    NN   \n",
       "175               3            5       157          he    (928, 930)   PRP   \n",
       "176               3            5       158       spent    (931, 936)   VBD   \n",
       "\n",
       "                    tag                      field subset       tag_cat  \n",
       "172                   O  Biographical / Historical    dev             O  \n",
       "173  B-Gendered-Pronoun  Biographical / Historical    dev  B-Linguistic  \n",
       "174                   O  Biographical / Historical    dev             O  \n",
       "175  B-Gendered-Pronoun  Biographical / Historical    dev  B-Linguistic  \n",
       "176                   O  Biographical / Historical    dev             O  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev = utils.addCategoryTagColumn(df_dev)\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that won't be used as features for the classifiers and remove any duplicate rows that remain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"sentence_id\", \"token_id\", \"pos\", \"token\", \"tag_cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[cols_to_keep]\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_dev = df_dev[cols_to_keep]\n",
    "df_dev = df_dev.drop_duplicates()\n",
    "# df_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create columns for each category so they can be passed into the models as individual features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_cat_tags = [\"B-Linguistic\", \"I-Linguistic\"]\n",
    "df_train_ling = df_train.loc[df_train.tag_cat.isin(ling_cat_tags)]\n",
    "df_dev_ling = df_dev.loc[df_dev.tag_cat.isin(ling_cat_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pers_cat_tags = [\"B-Person-Name\", \"I-Person-Name\"]\n",
    "df_train_pers = df_train.loc[df_train.tag_cat.isin(pers_cat_tags)]\n",
    "df_dev_pers = df_dev.loc[df_dev.tag_cat.isin(pers_cat_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cat_tags = [\"B-Contextual\", \"I-Contextual\"]\n",
    "df_train_cont = df_train.loc[df_train.tag_cat.isin(cont_cat_tags)]\n",
    "df_dev_cont = df_dev.loc[df_dev.tag_cat.isin(cont_cat_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = (df_train.drop(columns=[\"tag_cat\"])).drop_duplicates()\n",
    "df_dev = (df_dev.drop(columns=[\"tag_cat\"])).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_cols = [\"sentence_id\", \"token_id\", \"pos\", \"token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.join(df_train_ling.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "df_train = df_train.join(df_train_pers.set_index(join_cols), on=join_cols, how=\"outer\", lsuffix=\"\", rsuffix=\"_personname\")\n",
    "df_train = df_train.join(df_train_cont.set_index(join_cols), on=join_cols, how=\"outer\", lsuffix=\"\", rsuffix=\"_contextual\")\n",
    "df_train = df_train.rename(columns={\"tag_cat\":\"tag_cat_linguistic\"})\n",
    "# df_train.head(30)  # Should have one row per token!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = df_dev.join(df_dev_ling.set_index(join_cols), on=join_cols, how=\"outer\")\n",
    "df_dev = df_dev.join(df_dev_pers.set_index(join_cols), on=join_cols, how=\"outer\", lsuffix=\"\", rsuffix=\"_personname\")\n",
    "df_dev = df_dev.join(df_dev_cont.set_index(join_cols), on=join_cols, how=\"outer\", lsuffix=\"\", rsuffix=\"_contextual\")\n",
    "df_dev = df_dev.rename(columns={\"tag_cat\":\"tag_cat_linguistic\"})\n",
    "# df_dev.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the `tag_cat_` columns' `nan` values with `'O'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_cat_linguistic</th>\n",
       "      <th>tag_cat_personname</th>\n",
       "      <th>tag_cat_contextual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id  token_id   pos       token tag_cat_linguistic  \\\n",
       "172            5       154    IN       After                  O   \n",
       "173            5       155  PRP$         his       B-Linguistic   \n",
       "174            5       156    NN  ordination                  O   \n",
       "175            5       157   PRP          he       B-Linguistic   \n",
       "176            5       158   VBD       spent                  O   \n",
       "\n",
       "    tag_cat_personname tag_cat_contextual  \n",
       "172                  O                  O  \n",
       "173                  O                  O  \n",
       "174                  O                  O  \n",
       "175                  O                  O  \n",
       "176                  O                  O  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_cat_cols = [\"tag_cat_linguistic\", \"tag_cat_personname\", \"tag_cat_contextual\"]\n",
    "df_train[tag_cat_cols] = df_train[tag_cat_cols].fillna('O')\n",
    "df_dev[tag_cat_cols] = df_dev[tag_cat_cols].fillna('O')\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by sentence, so the token column becomes a list of tokens for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_cat_linguistic</th>\n",
       "      <th>tag_cat_personname</th>\n",
       "      <th>tag_cat_contextual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[154, 155, 156, 157, 158, 159, 160, 161, 162, ...</td>\n",
       "      <td>[IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...</td>\n",
       "      <td>[After, his, ordination, he, spent, three, yea...</td>\n",
       "      <td>[O, B-Linguistic, O, B-Linguistic, O, O, O, O,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-Contextual, I-Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[NN, :, NN]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[321, 322, 323, 324, 325, 326, 327, 328, 329, ...</td>\n",
       "      <td>[NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[498, 499, 500, 501, 502, 503, 504, 505, 506, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...</td>\n",
       "      <td>[In, 1941, Tom, Allan, married, Jane, Moore, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, B-Person-Name, I-Person-Name, O, B-Pers...</td>\n",
       "      <td>[O, O, B-Contextual, I-Contextual, I-Contextua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[649, 650, 651, 652, 653, 654, 655, 656, 657, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...</td>\n",
       "      <td>[In, 1955, Rev, Tom, Allan, accepted, a, call,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, B-Person-Name, I-Person-Name, I-Person-...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-Contextual, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      token_id  \\\n",
       "sentence_id                                                      \n",
       "5            [154, 155, 156, 157, 158, 159, 160, 161, 162, ...   \n",
       "11                                             [308, 309, 310]   \n",
       "13           [321, 322, 323, 324, 325, 326, 327, 328, 329, ...   \n",
       "18           [498, 499, 500, 501, 502, 503, 504, 505, 506, ...   \n",
       "24           [649, 650, 651, 652, 653, 654, 655, 656, 657, ...   \n",
       "\n",
       "                                                           pos  \\\n",
       "sentence_id                                                      \n",
       "5            [IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...   \n",
       "11                                                 [NN, :, NN]   \n",
       "13           [NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...   \n",
       "18           [IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...   \n",
       "24           [IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...   \n",
       "\n",
       "                                                      sentence  \\\n",
       "sentence_id                                                      \n",
       "5            [After, his, ordination, he, spent, three, yea...   \n",
       "11                                        [Identifier, :, AA6]   \n",
       "13           [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "18           [In, 1941, Tom, Allan, married, Jane, Moore, a...   \n",
       "24           [In, 1955, Rev, Tom, Allan, accepted, a, call,...   \n",
       "\n",
       "                                            tag_cat_linguistic  \\\n",
       "sentence_id                                                      \n",
       "5            [O, B-Linguistic, O, B-Linguistic, O, O, O, O,...   \n",
       "11                                                   [O, O, O]   \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "18           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "24           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                            tag_cat_personname  \\\n",
       "sentence_id                                                      \n",
       "5            [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "11                                                   [O, O, O]   \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "18           [O, O, B-Person-Name, I-Person-Name, O, B-Pers...   \n",
       "24           [O, O, B-Person-Name, I-Person-Name, I-Person-...   \n",
       "\n",
       "                                            tag_cat_contextual  \n",
       "sentence_id                                                     \n",
       "5            [O, O, O, O, O, O, O, O, O, B-Contextual, I-Co...  \n",
       "11                                                   [O, O, O]  \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "18           [O, O, B-Contextual, I-Contextual, I-Contextua...  \n",
       "24           [O, O, O, O, O, O, O, O, O, O, B-Contextual, O...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_grouped = utils.implodeDataFrame(df_train, [\"sentence_id\"])\n",
    "df_dev_grouped = utils.implodeDataFrame(df_dev, [\"sentence_id\"])\n",
    "df_train_grouped = df_train_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped = df_dev_grouped.rename(columns={\"token\":\"sentence\"})\n",
    "df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the POS and category tags together with the tokens so each sentence item is a tuple: `(TOKEN, POS-TAG, CATEGORY-TAG)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Title', 'NN', 'O'), (':', ':', 'O'), ('Papers', 'NNS', 'O')]\n",
      "[('After', 'IN', 'O'), ('his', 'PRP$', 'B-Linguistic'), ('ordination', 'NN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_ling = utils.zipFeaturesAndTarget(df_train_grouped, \"tag_cat_linguistic\")\n",
    "print(train_sentences_ling[0][:3])\n",
    "dev_sentences_ling = utils.zipFeaturesAndTarget(df_dev_grouped, \"tag_cat_linguistic\")\n",
    "print(dev_sentences_ling[0][:3])\n",
    "train_sentences_pers = utils.zipFeaturesAndTarget(df_train_grouped, \"tag_cat_personname\")\n",
    "dev_sentences_pers = utils.zipFeaturesAndTarget(df_dev_grouped, \"tag_cat_personname\")\n",
    "train_sentences_cont = utils.zipFeaturesAndTarget(df_train_grouped, \"tag_cat_contextual\")\n",
    "dev_sentences_cont = utils.zipFeaturesAndTarget(df_dev_grouped, \"tag_cat_contextual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "Get GloVe word embeddings (which were trained on English Wikipedia entries) for the vocabulary of the dataset (the unique tokens in the training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\"50\", \"100\", \"200\", \"300\"]\n",
    "d = dimensions[0]  \n",
    "# dimensonality of custom fastText embeddings = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = utils.getGloveEmbeddings(d)\n",
    "# print(glove[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 35968\n",
      "Lowercased vocabulary size: 31335\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(df_train.token.unique())\n",
    "vocabulary_lowercased = [token.lower() for token in vocabulary]\n",
    "vocabulary_lowercased = list(set(vocabulary_lowercased))\n",
    "print(\"Vocabulary size:\", len(vocabulary))\n",
    "print(\"Lowercased vocabulary size:\", len(vocabulary_lowercased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = utils.getEmbeddingsForTokens(glove, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(word_embeddings[0], glove[vocabulary[0].lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = dict(zip(vocabulary, word_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict_keys = list(embedding_dict.keys())\n",
    "for token in vocabulary:\n",
    "    assert token in embedding_dict_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature dictionaries:\n",
    "\n",
    "*References:*\n",
    "* *https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html*\n",
    "* *https://stackoverflow.com/questions/58736548/how-to-use-word-embedding-as-features-for-crf-sklearn-crfsuite-model-training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vector representation of a token from a dictionray of word embeddings\n",
    "def extractEmbedding(token, embedding_dict=glove, dimensions=int(d)):\n",
    "    if token.isalpha():\n",
    "        token = token.lower()\n",
    "    try:\n",
    "        embedding = embedding_dict[token]\n",
    "    except KeyError:\n",
    "        embedding = np.zeros((dimensions,))\n",
    "    return embedding.reshape(-1,1)\n",
    "\n",
    "def extractTokenFeatures(sentence, i):\n",
    "    token = sentence[i][0]\n",
    "    pos = sentence[i][1]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'pos': pos,\n",
    "        'pos[:2]': pos[:2],\n",
    "        'token': token\n",
    "    }\n",
    "    \n",
    "    # Add each value in a token's word embedding as a separate feature\n",
    "    embedding = extractEmbedding(token)\n",
    "    for i,n in enumerate(embedding):\n",
    "        features['e{}'.format(i)] = n\n",
    "    \n",
    "    # Record whether a token is the first or last token of a sentence\n",
    "    if i == 0:\n",
    "        features['START'] = True\n",
    "    elif i == (len(sentence) - 1):\n",
    "        features['END'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extractSentenceFeatures(sentence):\n",
    "    return [extractTokenFeatures(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "def extractSentenceTargets(sentence):\n",
    "    return [tag for token, pos, tag in sentence]\n",
    "\n",
    "def extractSentenceTokens(sentence):\n",
    "    return [token for token, pos, tag in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractSentenceFeatures(train_sentences[0])[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Models\n",
    "\n",
    "### Linguistic\n",
    "\n",
    "* **Features:** part-of-speech tag, first 2 letters of part-of-speech tag abbreviation, GloVe embeddings\n",
    "* **Target:** Linguistic label category IOB tags\n",
    "* **Algorithm:** L2SGD\n",
    "\n",
    "#### Train\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Linguistic** category of tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_sentences_ling\n",
    "dev_sentences = dev_sentences_ling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X_train = [extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# Target\n",
    "y_train = [extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow']\n",
    "# Available algorithms with sklearn_crfsuite are:\n",
    "#     'lbfgs' - Gradient descent using the L-BFGS method\n",
    "#     'l2sgd' - Stochastic Gradient Descent with L2 regularization term\n",
    "#     'ap' - Averaged Perceptron\n",
    "#     'pa' - Passive Aggressive (PA)\n",
    "#     'arow' - Adaptive Regularization Of Weight Vector (AROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100) #iterations unlimited\n",
    "clf = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=0.1, max_iterations=100, all_possible_transitions=True)     # up to 1000 iterations allowed\n",
    "# clf = sklearn_crfsuite.CRF(algorithm=algorithms[2], max_iterations=100)           # max iterations allowed\n",
    "# clf = sklearn_crfsuite.CRF(algorithm=algorithms[3], max_iterations=100)           # max iterations allowed\n",
    "# clf = sklearn_crfsuite.CRF(algorithm=algorithms[4], max_iterations=100)           # max iterations allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Linguistic', 'I-Linguistic']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "##### Strict Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.7486667314705173\n",
      "  - Prec: 0.795437655165426\n",
      "  - Rec 0.735140771637122\n"
     ]
    }
   ],
   "source": [
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments: comparison of all available algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # LBFGS - third-best\n",
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # L2SGD - tied for first (about the same as the fourth)\n",
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # AP - second-best\n",
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # PA - tied for first (about the same as the second)\n",
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # AROW - worst-performing\n",
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_grouped = df_dev_grouped.rename(columns={\"tag_cat_linguistic\":\"tag_cat_linguistic_expected\"})\n",
    "df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_cat_linguistic_predicted\", y_pred)\n",
    "# df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect.\n",
    "\n",
    "As with the manual annotation evaluation, we want to also evaluate the predictions more loosely, considering overlapping text spans in addition to exactly matching text spans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person Name\n",
    "\n",
    "* **Features:** part-of-speech tag, first 2 letters of part-of-speech tag abbreviation, custom fastText embeddings\n",
    "* **Target:** Person-Name label category IOB tags\n",
    "* **Algorithm:** L2SGD\n",
    "\n",
    "#### Train\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Person Name** category of tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_sentences_pers\n",
    "dev_sentences = dev_sentences_pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X_train = [extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# Target\n",
    "y_train = [extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=0.1, max_iterations=100, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Person-Name', 'I-Person-Name']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "##### Strict Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.6836995226710204\n",
      "  - Prec: 0.7664732820029956\n",
      "  - Rec 0.6174724342663274\n"
     ]
    }
   ],
   "source": [
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_grouped = df_dev_grouped.rename(columns={\"tag_cat_personname\":\"tag_cat_personname_expected\"})\n",
    "df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_cat_personname_predicted\", y_pred)\n",
    "# df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "### Contextual\n",
    "\n",
    "* **Features:** part-of-speech tag, first 2 letters of part-of-speech tag abbreviation, custom fastText embeddings\n",
    "* **Target:** Contextual label category IOB tags\n",
    "* **Algorithm:** L2SGD\n",
    "\n",
    "#### Train\n",
    "\n",
    "Train a Conditional Random Field (CRF) model with the default parameters on the **Contextual** category of tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_sentences_cont\n",
    "dev_sentences = dev_sentences_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X_train = [extractSentenceFeatures(sentence) for sentence in train_sentences]\n",
    "X_dev = [extractSentenceFeatures(sentence) for sentence in dev_sentences]\n",
    "# Target\n",
    "y_train = [extractSentenceTargets(sentence) for sentence in train_sentences]\n",
    "y_dev = [extractSentenceTargets(sentence) for sentence in dev_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['lbfgs', 'l2sgd', 'ap', 'pa', 'arow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn_crfsuite.CRF(algorithm=algorithms[1], c2=0.1, max_iterations=100, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66059532/attributeerror-crf-object-has-no-attribute-keep-tempfiles\n",
    "try:\n",
    "    clf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `'O'` tags from the targets list since we are interested in the ability to apply the gendered and gender biased language related tags, and the `'O'` tags far outnumber the tags for gendered and gender biased language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Contextual', 'I-Contextual']\n"
     ]
    }
   ],
   "source": [
    "targets = list(clf.classes_)\n",
    "targets.remove('O')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "##### Strict Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - F1: 0.32005165248636236\n",
      "  - Prec: 0.6812817844789257\n",
      "  - Rec 0.20969419596421884\n"
     ]
    }
   ],
   "source": [
    "print(\"  - F1:\", metrics.flat_f1_score(y_dev, y_pred, average=\"weighted\", labels=targets))\n",
    "print(\"  - Prec:\", metrics.flat_precision_score(y_dev, y_pred, average=\"weighted\", labels=targets))\n",
    "print(\"  - Rec\", metrics.flat_recall_score(y_dev, y_pred, average=\"weighted\", labels=targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets_sorted = sorted(targets, key=lambda name: (name[1:], name[0]))\n",
    "# print(metrics.flat_classification_report(y_dev, y_pred, labels=targets_sorted, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag_cat_linguistic_expected</th>\n",
       "      <th>tag_cat_personname_expected</th>\n",
       "      <th>tag_cat_contextual_expected</th>\n",
       "      <th>tag_cat_linguistic_predicted</th>\n",
       "      <th>tag_cat_personname_predicted</th>\n",
       "      <th>tag_cat_contextual_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[154, 155, 156, 157, 158, 159, 160, 161, 162, ...</td>\n",
       "      <td>[IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...</td>\n",
       "      <td>[After, his, ordination, he, spent, three, yea...</td>\n",
       "      <td>[O, B-Linguistic, O, B-Linguistic, O, O, O, O,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-Contextual, I-Co...</td>\n",
       "      <td>[O, B-Linguistic, O, B-Linguistic, O, O, O, O,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[308, 309, 310]</td>\n",
       "      <td>[NN, :, NN]</td>\n",
       "      <td>[Identifier, :, AA6]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[321, 322, 323, 324, 325, 326, 327, 328, 329, ...</td>\n",
       "      <td>[NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...</td>\n",
       "      <td>[Scope, and, Contents, :, Sermons, and, addres...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[498, 499, 500, 501, 502, 503, 504, 505, 506, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...</td>\n",
       "      <td>[In, 1941, Tom, Allan, married, Jane, Moore, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, B-Person-Name, I-Person-Name, O, B-Pers...</td>\n",
       "      <td>[O, O, B-Contextual, I-Contextual, I-Contextua...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, B-Person-Name, I-Person-Name, O, B-Pers...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[649, 650, 651, 652, 653, 654, 655, 656, 657, ...</td>\n",
       "      <td>[IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...</td>\n",
       "      <td>[In, 1955, Rev, Tom, Allan, accepted, a, call,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, B-Person-Name, I-Person-Name, I-Person-...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-Contextual, O...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, B-Person-Name, I-Person-Name, I-Person-...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      token_id  \\\n",
       "sentence_id                                                      \n",
       "5            [154, 155, 156, 157, 158, 159, 160, 161, 162, ...   \n",
       "11                                             [308, 309, 310]   \n",
       "13           [321, 322, 323, 324, 325, 326, 327, 328, 329, ...   \n",
       "18           [498, 499, 500, 501, 502, 503, 504, 505, 506, ...   \n",
       "24           [649, 650, 651, 652, 653, 654, 655, 656, 657, ...   \n",
       "\n",
       "                                                           pos  \\\n",
       "sentence_id                                                      \n",
       "5            [IN, PRP$, NN, PRP, VBD, CD, NNS, IN, DT, NN, ...   \n",
       "11                                                 [NN, :, NN]   \n",
       "13           [NN, CC, NNS, :, NNS, CC, NNS, ,, JJ, ;, NNS, ...   \n",
       "18           [IN, CD, NNP, NNP, VBD, NNP, NNP, CC, PRP, VBD...   \n",
       "24           [IN, CD, NNP, NNP, NNP, VBD, DT, NN, TO, VB, N...   \n",
       "\n",
       "                                                      sentence  \\\n",
       "sentence_id                                                      \n",
       "5            [After, his, ordination, he, spent, three, yea...   \n",
       "11                                        [Identifier, :, AA6]   \n",
       "13           [Scope, and, Contents, :, Sermons, and, addres...   \n",
       "18           [In, 1941, Tom, Allan, married, Jane, Moore, a...   \n",
       "24           [In, 1955, Rev, Tom, Allan, accepted, a, call,...   \n",
       "\n",
       "                                   tag_cat_linguistic_expected  \\\n",
       "sentence_id                                                      \n",
       "5            [O, B-Linguistic, O, B-Linguistic, O, O, O, O,...   \n",
       "11                                                   [O, O, O]   \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "18           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "24           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                   tag_cat_personname_expected  \\\n",
       "sentence_id                                                      \n",
       "5            [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "11                                                   [O, O, O]   \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "18           [O, O, B-Person-Name, I-Person-Name, O, B-Pers...   \n",
       "24           [O, O, B-Person-Name, I-Person-Name, I-Person-...   \n",
       "\n",
       "                                   tag_cat_contextual_expected  \\\n",
       "sentence_id                                                      \n",
       "5            [O, O, O, O, O, O, O, O, O, B-Contextual, I-Co...   \n",
       "11                                                   [O, O, O]   \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "18           [O, O, B-Contextual, I-Contextual, I-Contextua...   \n",
       "24           [O, O, O, O, O, O, O, O, O, O, B-Contextual, O...   \n",
       "\n",
       "                                  tag_cat_linguistic_predicted  \\\n",
       "sentence_id                                                      \n",
       "5            [O, B-Linguistic, O, B-Linguistic, O, O, O, O,...   \n",
       "11                                                   [O, O, O]   \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "18           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "24           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                  tag_cat_personname_predicted  \\\n",
       "sentence_id                                                      \n",
       "5            [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "11                                                   [O, O, O]   \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "18           [O, O, B-Person-Name, I-Person-Name, O, B-Pers...   \n",
       "24           [O, O, B-Person-Name, I-Person-Name, I-Person-...   \n",
       "\n",
       "                                  tag_cat_contextual_predicted  \n",
       "sentence_id                                                     \n",
       "5            [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "11                                                   [O, O, O]  \n",
       "13           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "18           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "24           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_grouped = df_dev_grouped.rename(columns={\"tag_cat_contextual\":\"tag_cat_contextual_expected\"})\n",
    "df_dev_grouped.insert(len(df_dev_grouped.columns), \"tag_cat_contextual_predicted\", y_pred)\n",
    "df_dev_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Performance Evaluation\n",
    "\n",
    "### Strict Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in evaluation approach is strict, so unless the model predictions' labels are on text spans that exactly match the development data's test, the predicted labels will be deemed incorrect.\n",
    "\n",
    "As with the manual annotation evaluation, we want to evaluate the predictions more loosely, considering overlapping text spans in addition to exactly matching text spans.  Save the predictions for each token and then use IntervalTree to evaluate performance considering overlapping labels, rather than only exactly matching labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_cat_linguistic_expected</th>\n",
       "      <th>tag_cat_personname_expected</th>\n",
       "      <th>tag_cat_contextual_expected</th>\n",
       "      <th>tag_cat_linguistic_predicted</th>\n",
       "      <th>tag_cat_personname_predicted</th>\n",
       "      <th>tag_cat_contextual_predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>154</td>\n",
       "      <td>IN</td>\n",
       "      <td>After</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>155</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>his</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156</td>\n",
       "      <td>NN</td>\n",
       "      <td>ordination</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>157</td>\n",
       "      <td>PRP</td>\n",
       "      <td>he</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>158</td>\n",
       "      <td>VBD</td>\n",
       "      <td>spent</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token_id   pos       token tag_cat_linguistic_expected  \\\n",
       "sentence_id                                                          \n",
       "5                154    IN       After                           O   \n",
       "5                155  PRP$         his                B-Linguistic   \n",
       "5                156    NN  ordination                           O   \n",
       "5                157   PRP          he                B-Linguistic   \n",
       "5                158   VBD       spent                           O   \n",
       "\n",
       "            tag_cat_personname_expected tag_cat_contextual_expected  \\\n",
       "sentence_id                                                           \n",
       "5                                     O                           O   \n",
       "5                                     O                           O   \n",
       "5                                     O                           O   \n",
       "5                                     O                           O   \n",
       "5                                     O                           O   \n",
       "\n",
       "            tag_cat_linguistic_predicted tag_cat_personname_predicted  \\\n",
       "sentence_id                                                             \n",
       "5                                      O                            O   \n",
       "5                           B-Linguistic                            O   \n",
       "5                                      O                            O   \n",
       "5                           B-Linguistic                            O   \n",
       "5                                      O                            O   \n",
       "\n",
       "            tag_cat_contextual_predicted  \n",
       "sentence_id                               \n",
       "5                                      O  \n",
       "5                                      O  \n",
       "5                                      O  \n",
       "5                                      O  \n",
       "5                                      O  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_exploded = df_dev_grouped.explode(list(df_dev_grouped.columns))\n",
    "df_dev_exploded = df_dev_exploded.rename(columns={\"sentence\":\"token\"})\n",
    "df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152711, 9)\n",
      "(152711, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df_dev_exploded.shape)\n",
    "print(df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save grouped (one row per sentence) and exploded (one row per token) versions of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_grouped.to_csv(config.tokc_path+\"model_output/categoryTags_crfL2sgd_POS-GloVe50_bySentence.csv\")\n",
    "df_dev_exploded.to_csv(config.tokc_path+\"model_output/categoryTags_crfL2sgd_POS-GloVe50_byToken.csv\")\n",
    "# df_dev_exploded = pd.read_csv(config.tokc_path+\"model_output/categoryTags_crfL2sgd_POS-CustomFastText_byToken.csv\")\n",
    "# df_dev_exploded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly evaluate the **Linguistic** tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_exploded = df_dev_exploded.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_cat_linguistic_expected</th>\n",
       "      <th>tag_cat_linguistic_predicted</th>\n",
       "      <th>strict_agreement</th>\n",
       "      <th>match_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>TP</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>TP</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id token_id       token tag_cat_linguistic_expected  \\\n",
       "0            5      154       After                           O   \n",
       "1            5      155         his                B-Linguistic   \n",
       "2            5      156  ordination                           O   \n",
       "3            5      157          he                B-Linguistic   \n",
       "4            5      158       spent                           O   \n",
       "\n",
       "  tag_cat_linguistic_predicted strict_agreement   match_type  \n",
       "0                            O               TN  exact_match  \n",
       "1                 B-Linguistic               TP  exact_match  \n",
       "2                            O               TN  exact_match  \n",
       "3                 B-Linguistic               TP  exact_match  \n",
       "4                            O               TN  exact_match  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"linguistic\"\n",
    "tags = [\"B-Linguistic\", \"I-Linguistic\"]\n",
    "subdf_pred_ling = utils.addCatAgreementAndMatchTypeCols(df_dev_exploded, category, tags)\n",
    "subdf_pred_ling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linguistic_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strict_agreement</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>150444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>1410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  linguistic_count\n",
       "strict_agreement                  \n",
       "FN                             506\n",
       "FP                             351\n",
       "TN                          150444\n",
       "TP                            1410"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_pred_ling_stats = subdf_pred_ling.groupby(\"strict_agreement\").count()\n",
    "subdf_pred_ling_stats = subdf_pred_ling_stats[[\"token_id\"]]\n",
    "subdf_pred_ling_stats = subdf_pred_ling_stats.rename(columns={\"token_id\":\"linguistic_count\"})\n",
    "subdf_pred_ling_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linguistic_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>category_match</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exact_match</th>\n",
       "      <td>151854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mismatch</th>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                linguistic_count\n",
       "match_type                      \n",
       "category_match                 2\n",
       "exact_match               151854\n",
       "mismatch                     855"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_pred_ling_stats2 = subdf_pred_ling.groupby(\"match_type\").count()\n",
    "subdf_pred_ling_stats2 = subdf_pred_ling_stats2[[\"token_id\"]]\n",
    "subdf_pred_ling_stats2 = subdf_pred_ling_stats2.rename(columns={\"token_id\":\"linguistic_count\"})\n",
    "subdf_pred_ling_stats2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly evaluate the **Person Name** tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_cat_personname_expected</th>\n",
       "      <th>tag_cat_personname_predicted</th>\n",
       "      <th>strict_agreement</th>\n",
       "      <th>match_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id token_id       token tag_cat_personname_expected  \\\n",
       "0            5      154       After                           O   \n",
       "1            5      155         his                           O   \n",
       "2            5      156  ordination                           O   \n",
       "3            5      157          he                           O   \n",
       "4            5      158       spent                           O   \n",
       "\n",
       "  tag_cat_personname_predicted strict_agreement   match_type  \n",
       "0                            O               TN  exact_match  \n",
       "1                            O               TN  exact_match  \n",
       "2                            O               TN  exact_match  \n",
       "3                            O               TN  exact_match  \n",
       "4                            O               TN  exact_match  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"personname\"\n",
    "tags = [\"B-Person-Name\", \"I-Person-Name\"]\n",
    "subdf_pred_pers = utils.addCatAgreementAndMatchTypeCols(df_dev_exploded, category, tags)\n",
    "subdf_pred_pers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personname_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strict_agreement</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>2367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>144644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>4368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  personname_count\n",
       "strict_agreement                  \n",
       "FN                            2367\n",
       "FP                            1332\n",
       "TN                          144644\n",
       "TP                            4368"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_pred_pers_stats = subdf_pred_pers.groupby(\"strict_agreement\").count()\n",
    "subdf_pred_pers_stats = subdf_pred_pers_stats[[\"token_id\"]]\n",
    "subdf_pred_pers_stats = subdf_pred_pers_stats.rename(columns={\"token_id\":\"personname_count\"})\n",
    "subdf_pred_pers_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personname_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>category_match</th>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exact_match</th>\n",
       "      <td>149012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mismatch</th>\n",
       "      <td>3360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                personname_count\n",
       "match_type                      \n",
       "category_match               339\n",
       "exact_match               149012\n",
       "mismatch                    3360"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_pred_pers_stats2 = subdf_pred_pers.groupby(\"match_type\").count()\n",
    "subdf_pred_pers_stats2 = subdf_pred_pers_stats2[[\"token_id\"]]\n",
    "subdf_pred_pers_stats2 = subdf_pred_pers_stats2.rename(columns={\"token_id\":\"personname_count\"})\n",
    "subdf_pred_pers_stats2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly evaluate the **Contextual** tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>tag_cat_contextual_expected</th>\n",
       "      <th>tag_cat_contextual_predicted</th>\n",
       "      <th>strict_agreement</th>\n",
       "      <th>match_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>TN</td>\n",
       "      <td>exact_match</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id token_id       token tag_cat_contextual_expected  \\\n",
       "0            5      154       After                           O   \n",
       "1            5      155         his                           O   \n",
       "2            5      156  ordination                           O   \n",
       "3            5      157          he                           O   \n",
       "4            5      158       spent                           O   \n",
       "\n",
       "  tag_cat_contextual_predicted strict_agreement   match_type  \n",
       "0                            O               TN  exact_match  \n",
       "1                            O               TN  exact_match  \n",
       "2                            O               TN  exact_match  \n",
       "3                            O               TN  exact_match  \n",
       "4                            O               TN  exact_match  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = \"contextual\"\n",
    "tags = [\"B-Contextual\", \"I-Contextual\"]\n",
    "subdf_pred_cont = utils.addCatAgreementAndMatchTypeCols(df_dev_exploded, category, tags)\n",
    "subdf_pred_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextual_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strict_agreement</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FN</th>\n",
       "      <td>3667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FP</th>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TN</th>\n",
       "      <td>147588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TP</th>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  contextual_count\n",
       "strict_agreement                  \n",
       "FN                            3667\n",
       "FP                             448\n",
       "TN                          147588\n",
       "TP                            1008"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_pred_cont_stats = subdf_pred_cont.groupby(\"strict_agreement\").count()\n",
    "subdf_pred_cont_stats = subdf_pred_cont_stats[[\"token_id\"]]\n",
    "subdf_pred_cont_stats = subdf_pred_cont_stats.rename(columns={\"token_id\":\"contextual_count\"})\n",
    "subdf_pred_cont_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextual_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>category_match</th>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exact_match</th>\n",
       "      <td>148596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mismatch</th>\n",
       "      <td>3983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                contextual_count\n",
       "match_type                      \n",
       "category_match               132\n",
       "exact_match               148596\n",
       "mismatch                    3983"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_pred_cont_stats2 = subdf_pred_cont.groupby(\"match_type\").count()\n",
    "subdf_pred_cont_stats2 = subdf_pred_cont_stats2[[\"token_id\"]]\n",
    "subdf_pred_cont_stats2 = subdf_pred_cont_stats2.rename(columns={\"token_id\":\"contextual_count\"})\n",
    "subdf_pred_cont_stats2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the statistics and calculate precision, recall, and F1 scores for each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>strict_agreement</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linguistic_count</th>\n",
       "      <td>506</td>\n",
       "      <td>351</td>\n",
       "      <td>150444</td>\n",
       "      <td>1410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personname_count</th>\n",
       "      <td>2367</td>\n",
       "      <td>1332</td>\n",
       "      <td>144644</td>\n",
       "      <td>4368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contextual_count</th>\n",
       "      <td>3667</td>\n",
       "      <td>448</td>\n",
       "      <td>147588</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "strict_agreement    FN    FP      TN    TP\n",
       "linguistic_count   506   351  150444  1410\n",
       "personname_count  2367  1332  144644  4368\n",
       "contextual_count  3667   448  147588  1008"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_pred_stats = pd.concat([subdf_pred_ling_stats.T, subdf_pred_pers_stats.T, subdf_pred_cont_stats.T])\n",
    "subdf_pred_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>strict_agreement</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linguistic_count</th>\n",
       "      <td>506</td>\n",
       "      <td>351</td>\n",
       "      <td>150444</td>\n",
       "      <td>1410</td>\n",
       "      <td>0.800681</td>\n",
       "      <td>0.735908</td>\n",
       "      <td>0.766930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personname_count</th>\n",
       "      <td>2367</td>\n",
       "      <td>1332</td>\n",
       "      <td>144644</td>\n",
       "      <td>4368</td>\n",
       "      <td>0.766316</td>\n",
       "      <td>0.648552</td>\n",
       "      <td>0.702533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contextual_count</th>\n",
       "      <td>3667</td>\n",
       "      <td>448</td>\n",
       "      <td>147588</td>\n",
       "      <td>1008</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.215615</td>\n",
       "      <td>0.328821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "strict_agreement    FN    FP      TN    TP  precision    recall       f_1\n",
       "linguistic_count   506   351  150444  1410   0.800681  0.735908  0.766930\n",
       "personname_count  2367  1332  144644  4368   0.766316  0.648552  0.702533\n",
       "contextual_count  3667   448  147588  1008   0.692308  0.215615  0.328821"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprec, lrec, lf = utils.precisionRecallF1(subdf_pred_stats.TP.values[0], subdf_pred_stats.FP.values[0], subdf_pred_stats.FN.values[0])\n",
    "pprec, prec, pf = utils.precisionRecallF1(subdf_pred_stats.TP.values[1], subdf_pred_stats.FP.values[1], subdf_pred_stats.FN.values[1])\n",
    "cprec, crec, cf = utils.precisionRecallF1(subdf_pred_stats.TP.values[2], subdf_pred_stats.FP.values[2], subdf_pred_stats.FN.values[2])\n",
    "precision = [lprec, pprec, cprec]\n",
    "recall = [lrec, prec, crec]\n",
    "f_1 = [lf, pf, cf]\n",
    "subdf_pred_stats.insert(len(list(subdf_pred_stats.columns)), \"precision\", precision)\n",
    "subdf_pred_stats.insert(len(list(subdf_pred_stats.columns)), \"recall\", recall)\n",
    "subdf_pred_stats.insert(len(list(subdf_pred_stats.columns)), \"f_1\", f_1)\n",
    "subdf_pred_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>match_type</th>\n",
       "      <th>category_match</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>mismatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linguistic_count</th>\n",
       "      <td>2</td>\n",
       "      <td>151854</td>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personname_count</th>\n",
       "      <td>339</td>\n",
       "      <td>149012</td>\n",
       "      <td>3360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contextual_count</th>\n",
       "      <td>132</td>\n",
       "      <td>148596</td>\n",
       "      <td>3983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "match_type        category_match  exact_match  mismatch\n",
       "linguistic_count               2       151854       855\n",
       "personname_count             339       149012      3360\n",
       "contextual_count             132       148596      3983"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_pred_stats2 = pd.concat([subdf_pred_ling_stats2.T, subdf_pred_pers_stats2.T, subdf_pred_cont_stats2.T])\n",
    "subdf_pred_stats2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf_pred_stats.to_csv(config.tokc_path+\"model_output/strict_agreement_stats_glove50_crf.csv\")\n",
    "subdf_pred_stats2.to_csv(config.tokc_path+\"model_output/match_types_glove50_crf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loose Evaluation\n",
    "\n",
    "Conduct a loose evaluation of the model's performance (as the manual annotation were evaluated), considering any overlapping or envelopping expected and predicted annotations to be matches, in addition to exactly-matching expected and predicted annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_offsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(10, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>(12, 15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>(17, 22)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>(22, 23)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_id  sentence_id token_offsets\n",
       "0         0            0       (0, 10)\n",
       "1         1            0      (10, 11)\n",
       "2         2            0      (12, 15)\n",
       "3         3            1      (17, 22)\n",
       "4         4            1      (22, 23)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the start and end offsets of the tokens\n",
    "tok_df = pd.read_csv(config.tokc_path+\"desc_sent_ann_token_tag.csv\", usecols=[\"token_id\", \"sentence_id\", \"token_offsets\"])\n",
    "tok_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_offsets</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag_cat_linguistic_expected</th>\n",
       "      <th>tag_cat_linguistic_predicted</th>\n",
       "      <th>tag_cat_personname_expected</th>\n",
       "      <th>tag_cat_personname_predicted</th>\n",
       "      <th>tag_cat_contextual_expected</th>\n",
       "      <th>tag_cat_contextual_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>154</td>\n",
       "      <td>After</td>\n",
       "      <td>(907, 912)</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>his</td>\n",
       "      <td>(913, 916)</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>156</td>\n",
       "      <td>ordination</td>\n",
       "      <td>(917, 927)</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>he</td>\n",
       "      <td>(928, 930)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>B-Linguistic</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>158</td>\n",
       "      <td>spent</td>\n",
       "      <td>(931, 936)</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id token_id       token token_offsets   pos  \\\n",
       "0            5      154       After    (907, 912)    IN   \n",
       "1            5      155         his    (913, 916)  PRP$   \n",
       "2            5      156  ordination    (917, 927)    NN   \n",
       "3            5      157          he    (928, 930)   PRP   \n",
       "4            5      158       spent    (931, 936)   VBD   \n",
       "\n",
       "  tag_cat_linguistic_expected tag_cat_linguistic_predicted  \\\n",
       "0                           O                            O   \n",
       "1                B-Linguistic                 B-Linguistic   \n",
       "2                           O                            O   \n",
       "3                B-Linguistic                 B-Linguistic   \n",
       "4                           O                            O   \n",
       "\n",
       "  tag_cat_personname_expected tag_cat_personname_predicted  \\\n",
       "0                           O                            O   \n",
       "1                           O                            O   \n",
       "2                           O                            O   \n",
       "3                           O                            O   \n",
       "4                           O                            O   \n",
       "\n",
       "  tag_cat_contextual_expected tag_cat_contextual_predicted  \n",
       "0                           O                            O  \n",
       "1                           O                            O  \n",
       "2                           O                            O  \n",
       "3                           O                            O  \n",
       "4                           O                            O  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the offsets data to the dev data, keeping only the tokens included in the dev data\n",
    "df_dev_exploded = df_dev_exploded.reset_index()\n",
    "join_cols = [\"sentence_id\", \"token_id\"]\n",
    "df_pred = df_dev_exploded.join(tok_df.set_index(join_cols), on=join_cols, how=\"left\")\n",
    "df_pred = df_pred[[\"sentence_id\", \"token_id\", \"token\", \"token_offsets\", \"pos\", \n",
    "                   \"tag_cat_linguistic_expected\", \"tag_cat_linguistic_predicted\",\n",
    "                   \"tag_cat_personname_expected\", \"tag_cat_personname_predicted\",\n",
    "                   \"tag_cat_contextual_expected\", \"tag_cat_contextual_predicted\"\n",
    "                  ]]\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_pred.isna().values.any() == False, \"There should be no NaN values in any of the prediction DataFrame's columns.\"\n",
    "assert df_pred.shape[0] == df_dev_exploded.shape[0], \"There should be the same number of rows as the dev DataFrame prior to the join.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the offsets to tuples of ints\n",
    "offsets = list(df_pred.token_offsets)\n",
    "offsets = [tuple(((offset_pair[1:-1]).split(\", \"))) for offset_pair in offsets]\n",
    "offsets = [(int(start_offset), int(end_offset)) for start_offset,end_offset in offsets]\n",
    "# print(type(offsets[0]), type(offsets[0][0]), type(offsets[0][1]))\n",
    "col_i = list(df_pred.columns).index(\"token_offsets\")\n",
    "df_pred = df_pred.drop(columns=[\"token_offsets\"])\n",
    "df_pred.insert((col_i-1), \"token_offsets\", offsets)\n",
    "# df_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine how many expected **Linguistic** tags overlap, envelop/fall within, or exactly match predicted Linguistic tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Linguistic', 'I-Linguistic']\n",
      "---------------------------------------\n",
      "TP: 3235 | FP: 314 | FN: 482\n",
      "---------------------------------------\n",
      "Precision: 0.9115243730628346\n",
      "Recall: 0.8703255313424805\n",
      "F_1 Score: 0.8904486650151391\n"
     ]
    }
   ],
   "source": [
    "# LINGUISTIC\n",
    "ling_tags = [\"B-Linguistic\", \"I-Linguistic\"]\n",
    "b_ling_exp_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_linguistic_expected\", ling_tags)\n",
    "b_ling_pred_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_linguistic_predicted\", ling_tags)\n",
    "tp, fp, fn = utils.looseAgreement(b_ling_exp_tree, b_ling_pred_tree)\n",
    "print(ling_tags)\n",
    "print(\"---------------------------------------\")\n",
    "print(\"TP:\",tp, \"| FP:\",fp, \"| FN:\",fn)\n",
    "print(\"---------------------------------------\")\n",
    "prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F_1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-Linguistic\n",
      "---------------------------------------\n",
      "TP: 2854 | FP: 292 | FN: 287\n",
      "---------------------------------------\n",
      "Precision: 0.9071837253655436\n",
      "Recall: 0.9086278255332697\n",
      "F_1 Score: 0.9079052012088437\n"
     ]
    }
   ],
   "source": [
    "# B-LINGUISTIC\n",
    "b_ling_exp_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_linguistic_expected\", [ling_tags[0]])\n",
    "b_ling_pred_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_linguistic_predicted\", [ling_tags[0]])\n",
    "tp, fp, fn = utils.looseAgreement(b_ling_exp_tree, b_ling_pred_tree)\n",
    "print(ling_tags[0])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"TP:\",tp, \"| FP:\",fp, \"| FN:\",fn)\n",
    "print(\"---------------------------------------\")\n",
    "prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F_1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-Linguistic\n",
      "---------------------------------------\n",
      "TP: 76 | FP: 25 | FN: 201\n",
      "---------------------------------------\n",
      "Precision: 0.7524752475247525\n",
      "Recall: 0.2743682310469314\n",
      "F_1 Score: 0.4021164021164021\n"
     ]
    }
   ],
   "source": [
    "# I-LINGUISTIC\n",
    "b_ling_exp_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_linguistic_expected\", [ling_tags[1]])\n",
    "b_ling_pred_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_linguistic_predicted\", [ling_tags[1]])\n",
    "tp, fp, fn = utils.looseAgreement(b_ling_exp_tree, b_ling_pred_tree)\n",
    "print(ling_tags[1])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"TP:\",tp, \"| FP:\",fp, \"| FN:\",fn)\n",
    "print(\"---------------------------------------\")\n",
    "prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F_1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine how many expected **Person Name** tags overlap, envelop/fall within, or exactly match predicted Person Name tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Person-Name', 'I-Person-Name']\n",
      "---------------------------------------\n",
      "TP: 40830 | FP: 853 | FN: 2104\n",
      "---------------------------------------\n",
      "Precision: 0.9795360218794232\n",
      "Recall: 0.9509945497740718\n",
      "F_1 Score: 0.9650543035087512\n"
     ]
    }
   ],
   "source": [
    "# PERSON NAME\n",
    "tags = [\"B-Person-Name\", \"I-Person-Name\"]\n",
    "exp_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_personname_expected\", tags)\n",
    "pred_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_personname_predicted\", tags)\n",
    "tp, fp, fn = utils.looseAgreement(exp_tree, pred_tree)\n",
    "print(tags)\n",
    "print(\"---------------------------------------\")\n",
    "print(\"TP:\",tp, \"| FP:\",fp, \"| FN:\",fn)\n",
    "print(\"---------------------------------------\")\n",
    "prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F_1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-Person-Name\n",
      "---------------------------------------\n",
      "TP: 8864 | FP: 498 | FN: 896\n",
      "---------------------------------------\n",
      "Precision: 0.9468062379833369\n",
      "Recall: 0.9081967213114754\n",
      "F_1 Score: 0.9270996757661333\n"
     ]
    }
   ],
   "source": [
    "# B-PERSON-NAME\n",
    "exp_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_personname_expected\", [tags[0]])\n",
    "pred_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_personname_predicted\", [tags[0]])\n",
    "tp, fp, fn = utils.looseAgreement(exp_tree, pred_tree)\n",
    "print(tags[0])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"TP:\",tp, \"| FP:\",fp, \"| FN:\",fn)\n",
    "print(\"---------------------------------------\")\n",
    "prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F_1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-Person-Name\n",
      "---------------------------------------\n",
      "TP: 15733 | FP: 675 | FN: 1578\n",
      "---------------------------------------\n",
      "Precision: 0.9588615309605071\n",
      "Recall: 0.9088440875743746\n",
      "F_1 Score: 0.933183071858596\n"
     ]
    }
   ],
   "source": [
    "# I-PERSON-NAME\n",
    "exp_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_personname_expected\", [tags[1]])\n",
    "pred_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_personname_predicted\", [tags[1]])\n",
    "tp, fp, fn = utils.looseAgreement(exp_tree, pred_tree)\n",
    "print(tags[1])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"TP:\",tp, \"| FP:\",fp, \"| FN:\",fn)\n",
    "print(\"---------------------------------------\")\n",
    "prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F_1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine how many expected **Contextual** tags overlap, envelop/fall within, or exactly match predicted Person Name tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Contextual', 'I-Contextual']\n",
      "---------------------------------------\n",
      "TP: 6932 | FP: 300 | FN: 3397\n",
      "---------------------------------------\n",
      "Precision: 0.9585176991150443\n",
      "Recall: 0.6711201471584858\n",
      "F_1 Score: 0.789476681282387\n"
     ]
    }
   ],
   "source": [
    "# CONTEXTUAL\n",
    "tags = [\"B-Contextual\", \"I-Contextual\"]\n",
    "exp_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_contextual_expected\", tags)\n",
    "pred_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_contextual_predicted\", tags)\n",
    "tp, fp, fn = utils.looseAgreement(exp_tree, pred_tree)\n",
    "print(tags)\n",
    "print(\"---------------------------------------\")\n",
    "print(\"TP:\",tp, \"| FP:\",fp, \"| FN:\",fn)\n",
    "print(\"---------------------------------------\")\n",
    "prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F_1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-Contextual\n",
      "---------------------------------------\n",
      "TP: 2077 | FP: 95 | FN: 1293\n",
      "---------------------------------------\n",
      "Precision: 0.9562615101289135\n",
      "Recall: 0.6163204747774481\n",
      "F_1 Score: 0.749548899314327\n"
     ]
    }
   ],
   "source": [
    "# B-CONTEXTUAL\n",
    "exp_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_contextual_expected\", [tags[0]])\n",
    "pred_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_contextual_predicted\", [tags[0]])\n",
    "tp, fp, fn = utils.looseAgreement(exp_tree, pred_tree)\n",
    "print(tags[0])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"TP:\",tp, \"| FP:\",fp, \"| FN:\",fn)\n",
    "print(\"---------------------------------------\")\n",
    "prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F_1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-Contextual\n",
      "---------------------------------------\n",
      "TP: 2117 | FP: 248 | FN: 2347\n",
      "---------------------------------------\n",
      "Precision: 0.8951374207188161\n",
      "Recall: 0.4742383512544803\n",
      "F_1 Score: 0.6200029286864841\n"
     ]
    }
   ],
   "source": [
    "# I-CONTEXTUAL\n",
    "exp_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_contextual_expected\", [tags[1]])\n",
    "pred_tree = utils.createIntervalTree(df_pred, \"token_offsets\", \"tag_cat_contextual_predicted\", [tags[1]])\n",
    "tp, fp, fn = utils.looseAgreement(exp_tree, pred_tree)\n",
    "print(tags[1])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"TP:\",tp, \"| FP:\",fp, \"| FN:\",fn)\n",
    "print(\"---------------------------------------\")\n",
    "prec, rec, f1 = utils.precisionRecallF1(tp, fp, fn)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F_1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender-bias",
   "language": "python",
   "name": "gender-bias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
